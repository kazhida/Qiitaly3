[
  {
    "rendered_body": "\u003cp data-sourcepos=\"1:1-1:120\"\u003ePythonプログラムを業務として開発する場合、チームとしてのスキルを高めることが大事。\u003c/p\u003e\n\u003ch3 data-sourcepos=\"3:1-3:10\"\u003e\n\u003cspan id=\"前提\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E5%89%8D%E6%8F%90\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e前提\u003c/h3\u003e\n\u003cp data-sourcepos=\"4:1-5:32\"\u003egithub を使用していること\u003cbr\u003e\nLinuxを使用していること\u003c/p\u003e\n\u003ch2 data-sourcepos=\"7:1-7:51\"\u003e\n\u003cspan id=\"業務で活用するパッケージの作り方\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E6%A5%AD%E5%8B%99%E3%81%A7%E6%B4%BB%E7%94%A8%E3%81%99%E3%82%8B%E3%83%91%E3%83%83%E3%82%B1%E3%83%BC%E3%82%B8%E3%81%AE%E4%BD%9C%E3%82%8A%E6%96%B9\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e業務で活用するパッケージの作り方\u003c/h2\u003e\n\u003cul data-sourcepos=\"8:1-19:19\"\u003e\n\u003cli data-sourcepos=\"8:1-8:123\"\u003e\n\u003cp data-sourcepos=\"8:4-8:123\"\u003e\u003ca href=\"https://pep8-ja.readthedocs.io/ja/latest/\" rel=\"nofollow noopener\" target=\"_blank\"\u003eスタイルのガイドライン\u003c/a\u003eにそったコーディングをする。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"9:1-9:190\"\u003e\n\u003cp data-sourcepos=\"9:3-9:190\"\u003epythonスクリプトの書式を\u003ca href=\"https://pypi.org/project/black/\" rel=\"nofollow noopener\" target=\"_blank\"\u003eblack\u003c/a\u003e\u003ca href=\"https://book.st-hakky.com/hakky/application-python-black/\" rel=\"nofollow noopener\" target=\"_blank\"\u003eblackの解説記事\u003c/a\u003e などで書式をそろえる。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"10:1-10:72\"\u003e\n\u003cp data-sourcepos=\"10:3-10:72\"\u003e\u003ca href=\"https://docs.python.org/ja/3/library/typing.html\" rel=\"nofollow noopener\" target=\"_blank\"\u003etype hint\u003c/a\u003eを書く\u003c/p\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"11:1-12:0\"\u003e\n\u003cp data-sourcepos=\"11:3-11:103\"\u003edoc comment \u003ca href=\"https://numpydoc.readthedocs.io/en/latest/format.html\" rel=\"nofollow noopener\" target=\"_blank\"\u003enumpy style doc comment\u003c/a\u003eを書く\u003c/p\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"13:1-13:86\"\u003e\n\u003cp data-sourcepos=\"13:3-13:86\"\u003e自作コードを、モジュールとサンプルスクリプトに分離する。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"14:1-14:53\"\u003e\n\u003cp data-sourcepos=\"14:3-14:53\"\u003eモジュールのパッケージ名を決める。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"15:1-15:89\"\u003e\n\u003cp data-sourcepos=\"15:3-15:89\"\u003eモジュールのファイルはモジュールのディレクトリに移動する。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"16:1-16:72\"\u003e\n\u003cp data-sourcepos=\"16:3-16:72\"\u003e自作モジュールのインストール手順をsetup.cfg に書く\u003c/p\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"17:1-17:59\"\u003e\n\u003cp data-sourcepos=\"17:3-17:59\"\u003eモジュールのバージョン情報を追加する。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"18:1-19:19\"\u003e\n\u003cp data-sourcepos=\"18:3-19:19\"\u003eモジュールをlocal にインストールする。\u003cbr\u003e\n従来のやり方\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"20:1-22:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003epython setup.py install\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"23:3-23:26\"\u003epython3 -m pip install .\u003c/p\u003e\n\u003cul data-sourcepos=\"24:1-32:66\"\u003e\n\u003cli data-sourcepos=\"24:1-28:0\"\u003e\n\u003cp data-sourcepos=\"24:3-24:56\"\u003epytestを使ってテストできるようにする。\u003c/p\u003e\n\u003cul data-sourcepos=\"25:3-28:0\"\u003e\n\u003cli data-sourcepos=\"25:3-25:36\"\u003etest データを用意する。\u003c/li\u003e\n\u003cli data-sourcepos=\"26:3-28:0\"\u003eある動作をしたときの期待値と実際の値とを比較する\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"29:1-29:21\"\u003e\n\u003cp data-sourcepos=\"29:3-29:21\"\u003epytest の作り方\u003c/p\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"30:1-30:66\"\u003e\n\u003cp data-sourcepos=\"30:3-30:66\"\u003eリポジトリにtestのディレクトリを追加します。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"31:1-32:66\"\u003e\n\u003cp data-sourcepos=\"31:3-32:66\"\u003e目標\u003cbr\u003e\n以下のようにしてテストができるようにします。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"33:1-35:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003ecd test; pytest test*.py \n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"38:2-38:16\"\u003ewhlの作り方\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"39:2-41:4\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003epython setup.py bdist_wheel\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"43:1-43:57\"\u003eそうすると*.whl ファイルが作成されます。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"45:1-45:42\"\u003e\n\u003cspan id=\"データファイルが大きいとき\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%83%87%E3%83%BC%E3%82%BF%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%81%8C%E5%A4%A7%E3%81%8D%E3%81%84%E3%81%A8%E3%81%8D\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eデータファイルが大きいとき\u003c/h2\u003e\n\u003cul data-sourcepos=\"46:1-49:1\"\u003e\n\u003cli data-sourcepos=\"46:1-49:1\"\u003e例： Google Drive にデータを置く場合\u003cbr\u003e\ngdown を用いてデータをダウンロードする。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 data-sourcepos=\"50:1-50:20\"\u003e\n\u003cspan id=\"makefileを作る\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#makefile%E3%82%92%E4%BD%9C%E3%82%8B\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eMakefileを作る\u003c/h2\u003e\n\u003cp data-sourcepos=\"51:1-51:76\"\u003eローカルPC上で必要な作業が簡潔するようにMakefileを書く\u003c/p\u003e\n\u003cul data-sourcepos=\"52:1-57:0\"\u003e\n\u003cli data-sourcepos=\"52:1-52:78\"\u003ebuild (C++やCythonなどのコードからbuildする必要がある場合)\u003c/li\u003e\n\u003cli data-sourcepos=\"53:1-53:51\"\u003einstall (pythonモジュールのインスール)\u003c/li\u003e\n\u003cli data-sourcepos=\"54:1-54:24\"\u003etest (pythonの実行)\u003c/li\u003e\n\u003cli data-sourcepos=\"55:1-55:32\"\u003ewhl (whlファイルの作成)\u003c/li\u003e\n\u003cli data-sourcepos=\"56:1-57:0\"\u003ereformat (blackなどでスクリプトの書式の整形)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 data-sourcepos=\"58:1-58:27\"\u003e\n\u003cspan id=\"packagecloudへの登録\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#packagecloud%E3%81%B8%E3%81%AE%E7%99%BB%E9%8C%B2\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003epackagecloudへの登録\u003c/h2\u003e\n\u003cul data-sourcepos=\"59:1-61:95\"\u003e\n\u003cli data-sourcepos=\"59:1-59:112\"\u003e業務の範囲で共有するパッケージを社内で共有するには, packageCloudを利用します。\u003c/li\u003e\n\u003cli data-sourcepos=\"60:1-60:93\"\u003ePackageCloud はパッケージへのアクセスコントロールをしてくれます。\u003c/li\u003e\n\u003cli data-sourcepos=\"61:1-61:95\"\u003eバージョンごとのwhlファイル、あるいはdebファイルを管理できます。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 data-sourcepos=\"62:1-62:43\"\u003e\n\u003cspan id=\"packagecloudからのダウンロード\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#packagecloud%E3%81%8B%E3%82%89%E3%81%AE%E3%83%80%E3%82%A6%E3%83%B3%E3%83%AD%E3%83%BC%E3%83%89\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003epackageCloudからのダウンロード\u003c/h3\u003e\n\u003cul data-sourcepos=\"63:1-65:0\"\u003e\n\u003cli data-sourcepos=\"63:1-63:30\"\u003ePACKAGECLOUD_TOKEN を利用\u003c/li\u003e\n\u003cli data-sourcepos=\"64:1-65:0\"\u003e例：packageCloudからのダウンロードし、ローカルでpip を使ってインストールする。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 data-sourcepos=\"66:1-66:74\"\u003e\n\u003cspan id=\"学習用評価用の大量のデータの扱い入門レベル\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E5%AD%A6%E7%BF%92%E7%94%A8%E8%A9%95%E4%BE%A1%E7%94%A8%E3%81%AE%E5%A4%A7%E9%87%8F%E3%81%AE%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E6%89%B1%E3%81%84%E5%85%A5%E9%96%80%E3%83%AC%E3%83%99%E3%83%AB\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e学習用・評価用の大量のデータの扱い（入門レベル）\u003c/h1\u003e\n\u003cp data-sourcepos=\"67:1-69:47\"\u003e段階０：ユーザーがバックアップを考えなくていいレベル\u003cbr\u003e\nGoogle　Drive にフォルダを置く\u003cbr\u003e\ngdown でそのフォルダをコピーする\u003c/p\u003e\n\u003cp data-sourcepos=\"71:1-71:120\"\u003eデータは追加するだけのデータか？　データファイルのバージョン管理が必要になるか？\u003c/p\u003e\n\u003ch3 data-sourcepos=\"73:1-73:52\"\u003e\n\u003cspan id=\"データを追加するだけの場合の手順\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%83%87%E3%83%BC%E3%82%BF%E3%82%92%E8%BF%BD%E5%8A%A0%E3%81%99%E3%82%8B%E3%81%A0%E3%81%91%E3%81%AE%E5%A0%B4%E5%90%88%E3%81%AE%E6%89%8B%E9%A0%86\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eデータを追加するだけの場合の手順\u003c/h3\u003e\n\u003cp data-sourcepos=\"74:1-74:323\"\u003e\u003ca href=\"https://azure.microsoft.com/ja-jp/resources/cloud-computing-dictionary/what-is-a-data-lake\" rel=\"nofollow noopener\" target=\"_blank\"\u003eデータ レイクは、大量のデータを元の形式で取り込んで保存する集中型リポジトリです。その後、データを処理して、さまざまな分析ニーズの基礎として使用できます。\u003c/a\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"77:1-81:111\"\u003e\u003ca href=\"https://qiita.com/nonbiri15/items/54c1fac76a8af5e51fdf\" id=\"reference-8b63b894f42a5d0fbbb5\"\u003eGit LFS からDVC(Data Version Control) に移行した話\u003c/a\u003e\u003cbr\u003e\n　この記事では、機械学習用にアノテーションなどの処理が済んだデータをバージョン管理する話を書いています。\u003cbr\u003e\n　データ全体の管理の責任者が一人いて、その一人が作業をする前提で書いています。\u003cbr\u003e\n　学習用のデータをサブフォルダ単位で追加していくことを想定しています。\u003cbr\u003e\n　アノテーションの付け直しなどを想定していないので手順を単純化しています。\u003c/p\u003e\n\u003cp data-sourcepos=\"83:1-84:157\"\u003e\u003ca href=\"https://qiita.com/nonbiri15/items/70ab04440211e35ebff2\" id=\"reference-2ae0c34b3709eb155c7b\"\u003e機械学習用の画像のデータレイクを作る\u003c/a\u003e\u003cbr\u003e\nこの記事では、上記の作業の前段で、個々の撮影などでデータを取得した時点でのデータの管理を想定しています。\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"85:1-98:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e--uniq                \n  撮影した時点でユニークな名前になっていない場合に、ユニークな名前に変更する。\n--show_local        \n  ローカルデータのディレクトリを示す。\n--show                \n  リモートストレージにおいてあるディレクトリを示す。\n--push ローカルデータのディレクトリ          \n  ローカルデータのディレクトリ　をリモートストレージにコピーする。\n--safe_remove ローカルデータのディレクトリ\n  リモートストレージに該当のデータがあることを確認して、ローカルデータのディレクトリを削除する。\n--get 該当のシーンのフォルダ\n  リモートストレージの該当のシーンのフォルダをローカルにコピーする。\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"100:1-102:30\"\u003eこれらの管理をすることを想定した記事になっています。\u003cbr\u003e\nデータの取得をして、生データを登録する作業を行う人が複数人いるときに、バッティングを生じないようにすることを\u003cbr\u003e\n目的とした記事です。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"104:1-104:21\"\u003e\n\u003cspan id=\"評価の自動化\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E8%A9%95%E4%BE%A1%E3%81%AE%E8%87%AA%E5%8B%95%E5%8C%96\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e評価の自動化\u003c/h2\u003e\n\u003cul data-sourcepos=\"105:1-108:1\"\u003e\n\u003cli data-sourcepos=\"105:1-105:68\"\u003e評価の自動化をできるスクリプトを用意します。\u003c/li\u003e\n\u003cli data-sourcepos=\"106:1-106:50\"\u003eドキュメントを自動生成させます。\u003c/li\u003e\n\u003cli data-sourcepos=\"107:1-108:1\"\u003ejupyter notebook を使うという方法もあります。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 data-sourcepos=\"109:1-109:15\"\u003e\n\u003cspan id=\"参考記事\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E5%8F%82%E8%80%83%E8%A8%98%E4%BA%8B\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e参考記事\u003c/h2\u003e\n\u003cp data-sourcepos=\"110:1-111:131\"\u003e\u003ca href=\"https://qiita.com/studio_haneya/items/9aad8f9ede11e58b41a8\" id=\"reference-8ebeadf97439ac787351\"\u003ePythonパッケージの作り方\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://qiita.com/Yarimizu14/items/12ea87fa577e6734962d\" id=\"reference-3bd65f9c2c14398026e3\"\u003e【packagecloud】pythonのパッケージをpackagecloudで管理する\u003c/a\u003e\u003c/p\u003e\n",
    "body": "Pythonプログラムを業務として開発する場合、チームとしてのスキルを高めることが大事。\n\n### 前提\ngithub を使用していること\nLinuxを使用していること\n\n## 業務で活用するパッケージの作り方\n-  [スタイルのガイドライン](https://pep8-ja.readthedocs.io/ja/latest/)にそったコーディングをする。\n- pythonスクリプトの書式を[black](https://pypi.org/project/black/)[blackの解説記事](https://book.st-hakky.com/hakky/application-python-black/) などで書式をそろえる。\n- [type hint](https://docs.python.org/ja/3/library/typing.html)を書く\n- doc comment [numpy style doc comment](https://numpydoc.readthedocs.io/en/latest/format.html)を書く\n\n- 自作コードを、モジュールとサンプルスクリプトに分離する。\n- モジュールのパッケージ名を決める。\n- モジュールのファイルはモジュールのディレクトリに移動する。\n- 自作モジュールのインストール手順をsetup.cfg に書く\n- モジュールのバージョン情報を追加する。\n- モジュールをlocal にインストールする。\n 従来のやり方\n```\npython setup.py install\n```\n  python3 -m pip install .\n- pytestを使ってテストできるようにする。\n  - test データを用意する。\n  - ある動作をしたときの期待値と実際の値とを比較する\n\n\n- pytest の作り方\n- リポジトリにtestのディレクトリを追加します。\n- 目標\n以下のようにしてテストができるようにします。\n```\ncd test; pytest test*.py \n```\n\n  \n whlの作り方\n ```\n python setup.py bdist_wheel\n ```\n \nそうすると*.whl ファイルが作成されます。\n \n## データファイルが大きいとき\n- 例： Google Drive にデータを置く場合\n  gdown を用いてデータをダウンロードする。\n\n \n## Makefileを作る\nローカルPC上で必要な作業が簡潔するようにMakefileを書く\n- build (C++やCythonなどのコードからbuildする必要がある場合)\n- install (pythonモジュールのインスール)\n- test (pythonの実行)\n- whl (whlファイルの作成)\n- reformat (blackなどでスクリプトの書式の整形)\n\n## packagecloudへの登録\n- 業務の範囲で共有するパッケージを社内で共有するには, packageCloudを利用します。\n- PackageCloud はパッケージへのアクセスコントロールをしてくれます。\n- バージョンごとのwhlファイル、あるいはdebファイルを管理できます。\n### packageCloudからのダウンロード\n- PACKAGECLOUD_TOKEN を利用\n- 例：packageCloudからのダウンロードし、ローカルでpip を使ってインストールする。\n\n# 学習用・評価用の大量のデータの扱い（入門レベル）\n段階０：ユーザーがバックアップを考えなくていいレベル\n  Google　Drive にフォルダを置く\n  gdown でそのフォルダをコピーする\n\nデータは追加するだけのデータか？　データファイルのバージョン管理が必要になるか？\n\n### データを追加するだけの場合の手順\n[データ レイクは、大量のデータを元の形式で取り込んで保存する集中型リポジトリです。その後、データを処理して、さまざまな分析ニーズの基礎として使用できます。](https://azure.microsoft.com/ja-jp/resources/cloud-computing-dictionary/what-is-a-data-lake)\n\n\n[Git LFS からDVC(Data Version Control) に移行した話](https://qiita.com/nonbiri15/items/54c1fac76a8af5e51fdf)\n　この記事では、機械学習用にアノテーションなどの処理が済んだデータをバージョン管理する話を書いています。\n　データ全体の管理の責任者が一人いて、その一人が作業をする前提で書いています。\n　学習用のデータをサブフォルダ単位で追加していくことを想定しています。\n　アノテーションの付け直しなどを想定していないので手順を単純化しています。\n\n[機械学習用の画像のデータレイクを作る](https://qiita.com/nonbiri15/items/70ab04440211e35ebff2)\n この記事では、上記の作業の前段で、個々の撮影などでデータを取得した時点でのデータの管理を想定しています。\n```\n--uniq                \n  撮影した時点でユニークな名前になっていない場合に、ユニークな名前に変更する。\n--show_local        \n  ローカルデータのディレクトリを示す。\n--show                \n  リモートストレージにおいてあるディレクトリを示す。\n--push ローカルデータのディレクトリ          \n  ローカルデータのディレクトリ　をリモートストレージにコピーする。\n--safe_remove ローカルデータのディレクトリ\n  リモートストレージに該当のデータがあることを確認して、ローカルデータのディレクトリを削除する。\n--get 該当のシーンのフォルダ\n  リモートストレージの該当のシーンのフォルダをローカルにコピーする。\n```\n\nこれらの管理をすることを想定した記事になっています。\nデータの取得をして、生データを登録する作業を行う人が複数人いるときに、バッティングを生じないようにすることを\n目的とした記事です。\n \n## 評価の自動化\n- 評価の自動化をできるスクリプトを用意します。\n- ドキュメントを自動生成させます。\n- jupyter notebook を使うという方法もあります。\n \n## 参考記事 \n[Pythonパッケージの作り方](https://qiita.com/studio_haneya/items/9aad8f9ede11e58b41a8)\n[【packagecloud】pythonのパッケージをpackagecloudで管理する](https://qiita.com/Yarimizu14/items/12ea87fa577e6734962d)\n",
    "coediting": false,
    "comments_count": 0,
    "created_at": "2024-08-22T13:35:39+09:00",
    "group": null,
    "id": "6461aa4dd82183fa7014",
    "likes_count": 0,
    "private": false,
    "reactions_count": 0,
    "stocks_count": 0,
    "tags": [
      {
        "name": "Python",
        "versions": []
      },
      {
        "name": "setup",
        "versions": []
      },
      {
        "name": "テスト",
        "versions": []
      },
      {
        "name": "パッケージ開発",
        "versions": []
      }
    ],
    "title": "Python プログラマのスキル標準化のための項目",
    "updated_at": "2024-08-22T13:35:39+09:00",
    "url": "https://qiita.com/nonbiri15/items/6461aa4dd82183fa7014",
    "user": {
      "description": "Python, scikit-learn, OpenCV使いです。\r\n画像認識と機械学習のエンジニアです。\r\n最近ロボットの会社には所属しているものの、ロボットエンジニアにはなれていません。\r\nここに述べる見解は個人としての見解です。",
      "facebook_id": "",
      "followees_count": 15,
      "followers_count": 764,
      "github_login_name": null,
      "id": "nonbiri15",
      "items_count": 538,
      "linkedin_id": "",
      "location": "",
      "name": "",
      "organization": "",
      "permanent_id": 83761,
      "profile_image_url": "https://qiita-image-store.s3.amazonaws.com/0/83761/profile-images/1473702877",
      "team_only": false,
      "twitter_screen_name": null,
      "website_url": ""
    },
    "page_views_count": null,
    "team_membership": null,
    "organization_url_name": null,
    "slide": false
  },
  {
    "rendered_body": "\u003ch1 data-sourcepos=\"1:1-1:14\"\u003e\n\u003cspan id=\"きっかけ\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%81%8D%E3%81%A3%E3%81%8B%E3%81%91\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eきっかけ\u003c/h1\u003e\n\u003cp data-sourcepos=\"3:1-4:117\"\u003e最近Slackで新しいアプリを作ってテストしていたところ\u003cbr\u003e\nSlack APIのfiles.uploadエンドポイントで以下のエラーが返却されうまくいきませんでした。\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"5:1-7:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e{'ok': False, 'error': 'method_deprecated'}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"8:1-8:111\"\u003e原因がさっぱりわからなくてググってみたところ以下の記事を偶然見かけました。\u003c/p\u003e\n\u003cp data-sourcepos=\"10:1-10:46\"\u003e\u003ciframe id=\"qiita-embed-content__d0ab3e206131a3863082415ada1f6961\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__d0ab3e206131a3863082415ada1f6961\" data-content=\"https%3A%2F%2Fzenn.dev%2Fslack%2Farticles%2F7ce5065cc4daa7\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\"\u003e\n\u003c/iframe\u003e\n\u003c/p\u003e\n\u003cp data-sourcepos=\"12:1-15:63\"\u003e特に重要だったのが以下の内容です。\u003cbr\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/f84892f48c5d33cda06f4b73d1e8c875967d4ba7/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3132333433392f63353961636366302d626135322d656438302d386566652d6132613161393662386331622e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F123439%2Fc59accf0-ba52-ed80-8efe-a2a1a96b8c1b.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=7650bc2672e433427d3ab56f0b1cdc74\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/123439/c59accf0-ba52-ed80-8efe-a2a1a96b8c1b.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F123439%2Fc59accf0-ba52-ed80-8efe-a2a1a96b8c1b.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=8f9293a0cf9c020500850e2e87dc8cc9 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003cbr\u003e\n昔作ったアプリでは動作していたので変だなと思っていたら\u003cbr\u003e\nこの条件に該当していたことが判明しました。\u003c/p\u003e\n\u003ch1 data-sourcepos=\"17:1-17:8\"\u003e\n\u003cspan id=\"詳細\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E8%A9%B3%E7%B4%B0\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e詳細\u003c/h1\u003e\n\u003cp data-sourcepos=\"19:1-21:51\"\u003eSDKが提供されているプラットフォームであれば既存メソッド名にv2を付与すれば利用できるようだが\u003cbr\u003e\nGASではスクラッチで実装しているのでこの方法は不可能でした。\u003cbr\u003e\n以下の手順を踏む必要がありました。\u003c/p\u003e\n\u003cp data-sourcepos=\"23:1-23:121\"\u003e\u003ca href=\"https://camo.qiitausercontent.com/7749da3abc1f38ce83a026988dd5339ba8532fda/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3132333433392f30393432313034622d333562362d306333322d303135612d3330626361323738666534652e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F123439%2F0942104b-35b6-0c32-015a-30bca278fe4e.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=14f6a70bf0a1ff2481b14cadfe1f1021\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/123439/0942104b-35b6-0c32-015a-30bca278fe4e.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F123439%2F0942104b-35b6-0c32-015a-30bca278fe4e.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=9bbd826e82a1232098b1395f5f7b18b6 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"25:1-26:138\"\u003e最初にファイルアップロード専用のURLを取得してそこにファイル実体をアップロードし処理のクロージング通知をしてあげる必要があるみたいです。\u003cbr\u003e\n以下の関数では一つのファイルをアップロードしてSlackに投稿するサンプル関数です。（ほぼChatGPT作）\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"javascript\" data-sourcepos=\"28:1-99:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"cm\"\u003e/**\n * Slackにファイルをアップロードする関数\n * @param {string} name - ファイル名\n * @param {Blob} fileBlob - アップロードするファイルのBlob\n * @param {string} token - SlackのAPIトークン\n * @param {string} channel - ファイルを投稿するチャンネルID\n */\u003c/span\u003e\n\u003cspan class=\"kd\"\u003efunction\u003c/span\u003e \u003cspan class=\"nf\"\u003euploadFileToSlack\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nx\"\u003efileBlob\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nx\"\u003etoken\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nx\"\u003echannel\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e// 1. ファイルのサイズを取得\u003c/span\u003e\n  \u003cspan class=\"kd\"\u003evar\u003c/span\u003e \u003cspan class=\"nx\"\u003efileSize\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nx\"\u003efileBlob\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003egetBytes\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"nx\"\u003elength\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\n  \u003cspan class=\"c1\"\u003e// 2. files.getUploadURLExternal APIを呼び出してURLとファイルIDを取得\u003c/span\u003e\n  \u003cspan class=\"kd\"\u003evar\u003c/span\u003e \u003cspan class=\"nx\"\u003euploadUrlResponse\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nx\"\u003eUrlFetchApp\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003efetch\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e`https://slack.com/api/files.getUploadURLExternal?filename=\u003c/span\u003e\u003cspan class=\"p\"\u003e${\u003c/span\u003e\u003cspan class=\"nx\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026amp;length=\u003c/span\u003e\u003cspan class=\"p\"\u003e${\u003c/span\u003e\u003cspan class=\"nx\"\u003efileSize\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e`\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"na\"\u003emethod\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"s2\"\u003eget\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"na\"\u003eheaders\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n      \u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"s2\"\u003eAuthorization\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"s2\"\u003eBearer \u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"nx\"\u003etoken\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e});\u003c/span\u003e\n  \n  \u003cspan class=\"kd\"\u003evar\u003c/span\u003e \u003cspan class=\"nx\"\u003euploadUrlJson\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nx\"\u003eJSON\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eparse\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003euploadUrlResponse\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003egetContentText\u003c/span\u003e\u003cspan class=\"p\"\u003e());\u003c/span\u003e\n  \u003cspan class=\"k\"\u003eif \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e!\u003c/span\u003e\u003cspan class=\"nx\"\u003euploadUrlJson\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003eok\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"k\"\u003ethrow\u003c/span\u003e \u003cspan class=\"k\"\u003enew\u003c/span\u003e \u003cspan class=\"nc\"\u003eError\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"s2\"\u003eFailed to get upload URL: \u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"nx\"\u003euploadUrlJson\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003eerror\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n  \n  \u003cspan class=\"kd\"\u003evar\u003c/span\u003e \u003cspan class=\"nx\"\u003euploadUrl\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nx\"\u003euploadUrlJson\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003eupload_url\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n  \u003cspan class=\"kd\"\u003evar\u003c/span\u003e \u003cspan class=\"nx\"\u003efileId\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nx\"\u003euploadUrlJson\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003efile_id\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\n  \u003cspan class=\"nx\"\u003econsole\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003elog\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003euploadUrl\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n  \u003cspan class=\"nx\"\u003econsole\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003elog\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003efileId\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n  \u003cspan class=\"c1\"\u003e// 3. 取得したURLにファイルをアップロード\u003c/span\u003e\n  \u003cspan class=\"kd\"\u003evar\u003c/span\u003e \u003cspan class=\"nx\"\u003euploadResponse\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nx\"\u003eUrlFetchApp\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003efetch\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003euploadUrl\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"na\"\u003emethod\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"s2\"\u003epost\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"na\"\u003epayload\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n      \u003cspan class=\"na\"\u003etoken\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nx\"\u003etoken\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n      \u003cspan class=\"na\"\u003efile\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nx\"\u003efileBlob\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n      \u003cspan class=\"na\"\u003efilename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nx\"\u003ename\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e});\u003c/span\u003e\n\n  \u003cspan class=\"k\"\u003eif \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003euploadResponse\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003egetResponseCode\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e!==\u003c/span\u003e \u003cspan class=\"mi\"\u003e200\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"k\"\u003ethrow\u003c/span\u003e \u003cspan class=\"k\"\u003enew\u003c/span\u003e \u003cspan class=\"nc\"\u003eError\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"s2\"\u003eFailed to upload file: \u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"nx\"\u003euploadResponse\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003egetContentText\u003c/span\u003e\u003cspan class=\"p\"\u003e());\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\n  \u003cspan class=\"c1\"\u003e// 4. files.completeUploadExternal APIにファイルIDとオプション引数を送信してアップロードを完了\u003c/span\u003e\n  \u003cspan class=\"kd\"\u003evar\u003c/span\u003e \u003cspan class=\"nx\"\u003ecompleteUploadPayload\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"na\"\u003efiles\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e[{\u003c/span\u003e \u003cspan class=\"na\"\u003eid\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nx\"\u003efileId\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"na\"\u003etitle\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nx\"\u003ename\u003c/span\u003e \u003cspan class=\"p\"\u003e}]\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e};\u003c/span\u003e\n\n  \u003cspan class=\"k\"\u003eif \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003echannel\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"nx\"\u003ecompleteUploadPayload\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003echannel_id\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nx\"\u003echannel\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\n  \u003cspan class=\"kd\"\u003evar\u003c/span\u003e \u003cspan class=\"nx\"\u003ecompleteUploadResponse\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nx\"\u003eUrlFetchApp\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003efetch\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"s2\"\u003ehttps://slack.com/api/files.completeUploadExternal\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"na\"\u003emethod\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"s2\"\u003epost\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"na\"\u003eheaders\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n      \u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"s2\"\u003eAuthorization\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"s2\"\u003eBearer \u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"nx\"\u003etoken\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n      \u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"s2\"\u003eContent-Type\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"s2\"\u003eapplication/json\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e},\u003c/span\u003e\n    \u003cspan class=\"na\"\u003epayload\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nx\"\u003eJSON\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003estringify\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003ecompleteUploadPayload\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e});\u003c/span\u003e\n\n  \u003cspan class=\"kd\"\u003evar\u003c/span\u003e \u003cspan class=\"nx\"\u003ecompleteUploadJson\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nx\"\u003eJSON\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eparse\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003ecompleteUploadResponse\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003egetContentText\u003c/span\u003e\u003cspan class=\"p\"\u003e());\u003c/span\u003e\n  \u003cspan class=\"k\"\u003eif \u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e!\u003c/span\u003e\u003cspan class=\"nx\"\u003ecompleteUploadJson\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003eok\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"k\"\u003ethrow\u003c/span\u003e \u003cspan class=\"k\"\u003enew\u003c/span\u003e \u003cspan class=\"nc\"\u003eError\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"s2\"\u003eFailed to complete upload: \u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"nx\"\u003ecompleteUploadJson\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003eerror\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\n  \u003cspan class=\"nx\"\u003econsole\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003elog\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003ecompleteUploadJson\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n  \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"nx\"\u003ecompleteUploadJson\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"100:1-101:102\"\u003e\u003ca href=\"https://api.slack.com/methods/files.completeUploadExternal\" rel=\"nofollow noopener\" target=\"_blank\"\u003efiles.completeUploadExternal\u003c/a\u003eにはさらにinitial_commentやthread_tsといったパラメータもありますのでアップロードファイルにメッセージを付与したりスレッド内に投稿したりするこも可能です。\u003cbr\u003e\nサンプルソースですと以下のような感じでパラメータを拡張してあげます。\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"javascript\" data-sourcepos=\"102:1-104:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"nx\"\u003ecompleteUploadPayload\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003echannel_id\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nx\"\u003echannel\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"107:1-107:60\"\u003eちなみに旧バージョンの関数はこちらです。\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"javascript\" data-sourcepos=\"109:1-136:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"cm\"\u003e/**\n * Slackにファイルをアップロードする関数\n * @param {string} name - ファイル名\n * @param {Blob} fileBlob - アップロードするファイルのBlob\n * @param {string} token - SlackのAPIトークン\n * @param {string} channel - ファイルを投稿するチャンネルID\n */\u003c/span\u003e\n\u003cspan class=\"kd\"\u003efunction\u003c/span\u003e \u003cspan class=\"nf\"\u003euploadFileToSlack\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nx\"\u003efileBlob\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nx\"\u003etoken\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nx\"\u003echannel\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\n  \u003cspan class=\"kd\"\u003evar\u003c/span\u003e \u003cspan class=\"nx\"\u003eurl\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"s1\"\u003ehttps://slack.com/api/files.upload\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n  \u003cspan class=\"kd\"\u003evar\u003c/span\u003e \u003cspan class=\"nx\"\u003eform\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n   \u003cspan class=\"na\"\u003etoken\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nx\"\u003etoken\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n   \u003cspan class=\"na\"\u003etitle\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nx\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n   \u003cspan class=\"na\"\u003efilename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"s2\"\u003efilename\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n   \u003cspan class=\"na\"\u003efiletype\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"s2\"\u003eauto\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n   \u003cspan class=\"na\"\u003echannels\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nx\"\u003echannel\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n   \u003cspan class=\"na\"\u003efile\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nx\"\u003efileBlob\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e};\u003c/span\u003e\n\n  \u003cspan class=\"kd\"\u003evar\u003c/span\u003e \u003cspan class=\"nx\"\u003eoptions\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"na\"\u003emethod\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"s1\"\u003epost\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"na\"\u003epayload\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nx\"\u003eform\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e};\u003c/span\u003e\n\n  \u003cspan class=\"nx\"\u003eUrlFetchApp\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003efetch\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003eurl\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nx\"\u003eoptions\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n",
    "body": "# きっかけ\n\n最近Slackで新しいアプリを作ってテストしていたところ\nSlack APIのfiles.uploadエンドポイントで以下のエラーが返却されうまくいきませんでした。\n```\n{'ok': False, 'error': 'method_deprecated'}\n```\n原因がさっぱりわからなくてググってみたところ以下の記事を偶然見かけました。\n\nhttps://zenn.dev/slack/articles/7ce5065cc4daa7\n\n特に重要だったのが以下の内容です。\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/123439/c59accf0-ba52-ed80-8efe-a2a1a96b8c1b.png)\n昔作ったアプリでは動作していたので変だなと思っていたら\nこの条件に該当していたことが判明しました。\n\n# 詳細\n\nSDKが提供されているプラットフォームであれば既存メソッド名にv2を付与すれば利用できるようだが\nGASではスクラッチで実装しているのでこの方法は不可能でした。\n以下の手順を踏む必要がありました。\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/123439/0942104b-35b6-0c32-015a-30bca278fe4e.png)\n\n最初にファイルアップロード専用のURLを取得してそこにファイル実体をアップロードし処理のクロージング通知をしてあげる必要があるみたいです。\n以下の関数では一つのファイルをアップロードしてSlackに投稿するサンプル関数です。（ほぼChatGPT作）\n\n```javascript\n/**\n * Slackにファイルをアップロードする関数\n * @param {string} name - ファイル名\n * @param {Blob} fileBlob - アップロードするファイルのBlob\n * @param {string} token - SlackのAPIトークン\n * @param {string} channel - ファイルを投稿するチャンネルID\n */\nfunction uploadFileToSlack(name, fileBlob, token, channel) {\n  // 1. ファイルのサイズを取得\n  var fileSize = fileBlob.getBytes().length;\n\n  // 2. files.getUploadURLExternal APIを呼び出してURLとファイルIDを取得\n  var uploadUrlResponse = UrlFetchApp.fetch(`https://slack.com/api/files.getUploadURLExternal?filename=${name}\u0026length=${fileSize}`, {\n    method: \"get\",\n    headers: {\n      \"Authorization\": \"Bearer \" + token,\n    }\n  });\n  \n  var uploadUrlJson = JSON.parse(uploadUrlResponse.getContentText());\n  if (!uploadUrlJson.ok) {\n    throw new Error(\"Failed to get upload URL: \" + uploadUrlJson.error);\n  }\n  \n  var uploadUrl = uploadUrlJson.upload_url;\n  var fileId = uploadUrlJson.file_id;\n\n  console.log(uploadUrl)\n  console.log(fileId)\n\n  // 3. 取得したURLにファイルをアップロード\n  var uploadResponse = UrlFetchApp.fetch(uploadUrl, {\n    method: \"post\",\n    payload: {\n      token: token,\n      file: fileBlob,\n      filename: name\n    }\n  });\n\n  if (uploadResponse.getResponseCode() !== 200) {\n    throw new Error(\"Failed to upload file: \" + uploadResponse.getContentText());\n  }\n\n  // 4. files.completeUploadExternal APIにファイルIDとオプション引数を送信してアップロードを完了\n  var completeUploadPayload = {\n    files: [{ id: fileId, title: name }]\n  };\n\n  if (channel) {\n    completeUploadPayload.channel_id = channel;\n  }\n\n  var completeUploadResponse = UrlFetchApp.fetch(\"https://slack.com/api/files.completeUploadExternal\", {\n    method: \"post\",\n    headers: {\n      \"Authorization\": \"Bearer \" + token,\n      \"Content-Type\": \"application/json\"\n    },\n    payload: JSON.stringify(completeUploadPayload)\n  });\n\n  var completeUploadJson = JSON.parse(completeUploadResponse.getContentText());\n  if (!completeUploadJson.ok) {\n    throw new Error(\"Failed to complete upload: \" + completeUploadJson.error);\n  }\n\n  console.log(completeUploadJson);\n  return completeUploadJson;\n}\n```\n[files.completeUploadExternal](https://api.slack.com/methods/files.completeUploadExternal)にはさらにinitial_commentやthread_tsといったパラメータもありますのでアップロードファイルにメッセージを付与したりスレッド内に投稿したりするこも可能です。\nサンプルソースですと以下のような感じでパラメータを拡張してあげます。\n```javascript\ncompleteUploadPayload.channel_id = channel;\n```\n\n\nちなみに旧バージョンの関数はこちらです。\n\n```javascript\n/**\n * Slackにファイルをアップロードする関数\n * @param {string} name - ファイル名\n * @param {Blob} fileBlob - アップロードするファイルのBlob\n * @param {string} token - SlackのAPIトークン\n * @param {string} channel - ファイルを投稿するチャンネルID\n */\nfunction uploadFileToSlack(name, fileBlob, token, channel) {\n\n  var url = 'https://slack.com/api/files.upload';\n  var form = {\n   token: token,\n   title: name,\n   filename: \"filename\",\n   filetype: \"auto\",\n   channels: channel,\n   file: fileBlob\n  };\n\n  var options = {\n    method: 'post',\n    payload: form,\n  };\n\n  UrlFetchApp.fetch(url, options);\n}\n```\n",
    "coediting": false,
    "comments_count": 0,
    "created_at": "2024-08-22T13:35:21+09:00",
    "group": null,
    "id": "60f349fa4f73ed38641d",
    "likes_count": 0,
    "private": false,
    "reactions_count": 0,
    "stocks_count": 0,
    "tags": [
      {
        "name": "GoogleAppsScript",
        "versions": []
      },
      {
        "name": "Slack",
        "versions": []
      },
      {
        "name": "slack-api",
        "versions": []
      }
    ],
    "title": "files.upload API 廃止におけるGASの改修",
    "updated_at": "2024-08-22T13:35:21+09:00",
    "url": "https://qiita.com/yahayuta/items/60f349fa4f73ed38641d",
    "user": {
      "description": "おっさんプログラマ",
      "facebook_id": "yahayutagpt",
      "followees_count": 3,
      "followers_count": 3,
      "github_login_name": "yahayuta",
      "id": "yahayuta",
      "items_count": 9,
      "linkedin_id": "",
      "location": "",
      "name": "やは ゆた",
      "organization": "",
      "permanent_id": 123439,
      "profile_image_url": "https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/123439/profile-images/1696428151",
      "team_only": false,
      "twitter_screen_name": "yahayuta",
      "website_url": "https://linktr.ee/yahayuta"
    },
    "page_views_count": null,
    "team_membership": null,
    "organization_url_name": null,
    "slide": false
  },
  {
    "rendered_body": "\u003cp data-sourcepos=\"2:1-2:250\"\u003eWhen it comes to the 1Ace App Download, Indian users have the opportunity to experience a unique blend of financial growth and thrilling online casino games. Let's explore how the 1Ace App can open the road to wealth and excitement for Indian users.\u003c/p\u003e\n\u003cp data-sourcepos=\"4:1-4:82\"\u003e\u003ciframe id=\"qiita-embed-content__679b2b513f8ababae85d58c05beb5f8e\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__679b2b513f8ababae85d58c05beb5f8e\" data-content=\"https%3A%2F%2Frumble.com%2Fv5bu6d9-1ace-app-download-apk-for-android-and-ios-for-free.html\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\"\u003e\n\u003c/iframe\u003e\n\u003c/p\u003e\n",
    "body": "\nWhen it comes to the 1Ace App Download, Indian users have the opportunity to experience a unique blend of financial growth and thrilling online casino games. Let's explore how the 1Ace App can open the road to wealth and excitement for Indian users. \n\nhttps://rumble.com/v5bu6d9-1ace-app-download-apk-for-android-and-ios-for-free.html\n",
    "coediting": false,
    "comments_count": 0,
    "created_at": "2024-08-22T13:26:03+09:00",
    "group": null,
    "id": "80efc2dbed22e02b3e81",
    "likes_count": 0,
    "private": false,
    "reactions_count": 0,
    "stocks_count": 0,
    "tags": [
      {
        "name": "#ludo",
        "versions": []
      },
      {
        "name": "#andarbahar",
        "versions": []
      },
      {
        "name": "#teenpatti",
        "versions": []
      }
    ],
    "title": "  1ACE App Download APK for Android and iOS for Free",
    "updated_at": "2024-08-22T13:26:03+09:00",
    "url": "https://qiita.com/fdgsdfgfdf/items/80efc2dbed22e02b3e81",
    "user": {
      "description": "\r\nDaftar Situs Slot Gacor Online Demo PG Gampang Maxwin x1024 https://omyague.com/",
      "facebook_id": "",
      "followees_count": 1,
      "followers_count": 0,
      "github_login_name": null,
      "id": "fdgsdfgfdf",
      "items_count": 2,
      "linkedin_id": "",
      "location": "",
      "name": "",
      "organization": "",
      "permanent_id": 3866518,
      "profile_image_url": "https://lh3.googleusercontent.com/a/ACg8ocJB7dT2sAvlZwUhp53nnrYE2aWhRnpBhyDwH-Ttjj3GVBwqzTai=s96-c",
      "team_only": false,
      "twitter_screen_name": null,
      "website_url": ""
    },
    "page_views_count": null,
    "team_membership": null,
    "organization_url_name": null,
    "slide": false
  },
  {
    "rendered_body": "\u003cp data-sourcepos=\"1:1-2:83\"\u003eWhen it comes to the 1Ace App Download, Indian users have the opportunity to experience a unique blend of financial growth and thrilling online casino games. Let's explore how the 1Ace App can open the road to wealth and excitement for Indian users.\u003cbr\u003e\n\u003ca href=\"https://rumble.com/v5bu60d-download-and-install-1ace-app-for-android-and-ios.html\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://rumble.com/v5bu60d-download-and-install-1ace-app-for-android-and-ios.html\u003c/a\u003e\u003c/p\u003e\n",
    "body": "When it comes to the 1Ace App Download, Indian users have the opportunity to experience a unique blend of financial growth and thrilling online casino games. Let's explore how the 1Ace App can open the road to wealth and excitement for Indian users.\nhttps://rumble.com/v5bu60d-download-and-install-1ace-app-for-android-and-ios.html  \n",
    "coediting": false,
    "comments_count": 0,
    "created_at": "2024-08-22T13:24:20+09:00",
    "group": null,
    "id": "615fd4f11986d3e990d7",
    "likes_count": 0,
    "private": false,
    "reactions_count": 0,
    "stocks_count": 0,
    "tags": [
      {
        "name": "#ludo",
        "versions": []
      },
      {
        "name": "#andarbahar",
        "versions": []
      },
      {
        "name": "#teenpatti",
        "versions": []
      }
    ],
    "title": "Download \u0026 Install 1ACE App for Android and iOS",
    "updated_at": "2024-08-22T13:24:20+09:00",
    "url": "https://qiita.com/fdgsdfgfdf/items/615fd4f11986d3e990d7",
    "user": {
      "description": "\r\nDaftar Situs Slot Gacor Online Demo PG Gampang Maxwin x1024 https://omyague.com/",
      "facebook_id": "",
      "followees_count": 1,
      "followers_count": 0,
      "github_login_name": null,
      "id": "fdgsdfgfdf",
      "items_count": 2,
      "linkedin_id": "",
      "location": "",
      "name": "",
      "organization": "",
      "permanent_id": 3866518,
      "profile_image_url": "https://lh3.googleusercontent.com/a/ACg8ocJB7dT2sAvlZwUhp53nnrYE2aWhRnpBhyDwH-Ttjj3GVBwqzTai=s96-c",
      "team_only": false,
      "twitter_screen_name": null,
      "website_url": ""
    },
    "page_views_count": null,
    "team_membership": null,
    "organization_url_name": null,
    "slide": false
  },
  {
    "rendered_body": "\u003cp data-sourcepos=\"3:1-3:73\"\u003eNymプロジェクトには現在、2つのコア技術があります。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"5:1-5:57\"\u003e\n\u003cspan id=\"1-mixnetによるデータとメタデータの保護\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#1-mixnet%E3%81%AB%E3%82%88%E3%82%8B%E3%83%87%E3%83%BC%E3%82%BF%E3%81%A8%E3%83%A1%E3%82%BF%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E4%BF%9D%E8%AD%B7\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e1. Mixnetによるデータとメタデータの保護\u003c/h2\u003e\n\u003cp data-sourcepos=\"7:1-7:378\"\u003eNymのMixnetは、インターネットを通過するすべてのTCP/IPパケットの匿名化を実現します。これにより、国家や企業レベルの強力な敵に対しても有効です。Mixnetは、メッセージの内容だけでなく、誰が誰と通信しているか、パケットがどのような経路を辿ったかなどの情報も隠します。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"9:1-9:73\"\u003e\n\u003cspan id=\"2-選択的認証情報の開示selective-disclosure-credential\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#2-%E9%81%B8%E6%8A%9E%E7%9A%84%E8%AA%8D%E8%A8%BC%E6%83%85%E5%A0%B1%E3%81%AE%E9%96%8B%E7%A4%BAselective-disclosure-credential\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e2. 選択的認証情報の開示（Selective Disclosure Credential）\u003c/h2\u003e\n\u003cp data-sourcepos=\"11:1-11:222\"\u003eNymは、EUのHorizon 2020が出資した「Decode」プロジェクトの「Coconut」を基盤とした設計技術を使用しています。ここでは、選択的な認証情報の開示の概念を説明します。\u003c/p\u003e\n\u003cp data-sourcepos=\"13:1-13:150\"\u003e\u003ca href=\"https://camo.qiitausercontent.com/392e92dd71c587b5ce1deed2cdc655d2b25d8e42/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f333637323835312f62663236353333612d323132652d633234632d646231362d3261363534303536643432382e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3672851%2Fbf26533a-212e-c24c-db16-2a654056d428.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=361d402fbecf19e7173d1b729b1323be\" alt=\"Screenshot 2024-08-22 at 13.23.00.png\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3672851/bf26533a-212e-c24c-db16-2a654056d428.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3672851%2Fbf26533a-212e-c24c-db16-2a654056d428.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=94aaf665fe690c325e5d9673d9a2f4b4 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3 data-sourcepos=\"16:1-16:47\"\u003e\n\u003cspan id=\"ココナッツcoconutとその機能\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%82%B3%E3%82%B3%E3%83%8A%E3%83%83%E3%83%84coconut%E3%81%A8%E3%81%9D%E3%81%AE%E6%A9%9F%E8%83%BD\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eココナッツ（Coconut）とその機能\u003c/h3\u003e\n\u003cp data-sourcepos=\"18:1-18:100\"\u003eココナッツは、以下の3つの基本的なアルゴリズムから構成されています：\u003c/p\u003e\n\u003col data-sourcepos=\"20:1-23:0\"\u003e\n\u003cli data-sourcepos=\"20:1-20:28\"\u003e\u003cstrong\u003e鍵生成（Keygen）\u003c/strong\u003e\u003c/li\u003e\n\u003cli data-sourcepos=\"21:1-21:25\"\u003e\u003cstrong\u003e検証（Verify）\u003c/strong\u003e\u003c/li\u003e\n\u003cli data-sourcepos=\"22:1-23:0\"\u003e\u003cstrong\u003e署名（Sign）\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp data-sourcepos=\"24:1-24:479\"\u003eユーザーは、公開鍵/秘密鍵ペアを生成（keygen）することができなければなりません。そして、秘密鍵を使って平文の一部に署名することができます。その後、署名者の公開鍵を持っている人なら誰でも、その署名が有効であることを検証できるはずです。RSA、DSA、そしてED25519のような新しく魅力的なものまで、すべての署名方式は同じように機能します。\u003c/p\u003e\n\u003cp data-sourcepos=\"26:1-26:310\"\u003eココナッツは、この基本的な3つの機能を、さらにいくつかの機能で拡張しています。追加された機能を理解することが、それを使って何ができるかを理解する鍵になります。どんなアルゴリズムが追加されるのか見てみましょう：\u003c/p\u003e\n\u003ch3 data-sourcepos=\"28:1-28:48\"\u003e\n\u003cspan id=\"4-ブラインド発行blind-issuance\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#4-%E3%83%96%E3%83%A9%E3%82%A4%E3%83%B3%E3%83%89%E7%99%BA%E8%A1%8Cblind-issuance\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e4. ブラインド発行（Blind Issuance）\u003c/h3\u003e\n\u003cp data-sourcepos=\"30:1-30:182\"\u003eavid Chaumは1980年代にブラインド署名を発明しました。ブラインド署名は、上記の性質に加えて、いくつかの特別な性質を持っています。\u003c/p\u003e\n\u003cp data-sourcepos=\"32:1-32:337\"\u003ekeygen、verify、signの各アルゴリズムはまだ存在します。また、署名の秘密鍵を持たない人がメッセージをブラインドさせるアルゴリズムもあります。平文の一部を取り出し、秘密鍵を持つ署名者が署名できるような無意味な文章に変えてしまうのです。\u003c/p\u003e\n\u003cp data-sourcepos=\"34:1-34:505\"\u003eこの署名は、メッセージを作成した人物に戻され、まだブラインドされたままです。この署名は、その人物によって解除することができます。この署名は、元の平文メッセージに対する有効な署名と同等になります。いったんブラインドが解除されると、署名者はブラインドされたメッセージの署名とブラインド解除されたメッセージを結びつけることができなくなり ます。\u003c/p\u003e\n\u003cp data-sourcepos=\"36:1-36:231\"\u003e理解を深めるには、例を挙げるのが効果的です。仮に存在するシンプルな個人向け電子マネーのスキームを例にとってみましょう。それを利用する際の手順を説明します：\u003c/p\u003e\n\u003cp data-sourcepos=\"38:1-38:79\"\u003e・1ドルを持っているというメッセージを平文で作成する。\u003c/p\u003e\n\u003cp data-sourcepos=\"40:1-40:150\"\u003e・ブラインド署名のアルゴリズムを使って、それを見えなくする。平文のメッセージは無意味な言葉に変わる。\u003c/p\u003e\n\u003cp data-sourcepos=\"42:1-42:82\"\u003e・ブラインドされたメッセージを1ドルと一緒に銀行に送る。\u003c/p\u003e\n\u003cp data-sourcepos=\"44:1-44:118\"\u003e・銀行は、私が1ドルを持っていることを証明するために署名し、その署名を私に返す。\u003c/p\u003e\n\u003cp data-sourcepos=\"46:1-46:183\"\u003e・私はその署名を解除し、銀行から戻ってきた署名とのリンクを解除する。これが、私のプライバシーを保証する重要なステップです。\u003c/p\u003e\n\u003cp data-sourcepos=\"48:1-48:118\"\u003e・私は今、私が1ドルを持っていることを証明する匿名のbearerトークンを持っています。\u003c/p\u003e\n\u003cp data-sourcepos=\"50:1-50:162\"\u003eBearerトークンは銀行からもらったものとリンクできませんが、その有効性は銀行の公開鍵があれば誰でも検証できます。\u003c/p\u003e\n\u003cp data-sourcepos=\"52:1-52:356\"\u003eこれがblind issuance（ブラインド発行）であり、そのプロセスは平文作成、ブラインド、署名者への送信、署名、作成者への返却、ブラインド解除、使用となります。ブラインド発行には問題があります。署名者は自分が何に署名しているのか全く分からないのです。\u003c/p\u003e\n\u003cp data-sourcepos=\"54:1-54:538\"\u003eつまりあなたが銀行の立場なら、訳のわからない言葉を受け取る、ということになります。銀行が「100万ドルあります」と書いていないことをどうやって確認するのでしょう？それは銀行にとって不透明なことなのです。一般的な解決策は、メッセージに1ドルが含まれていることを証明するゼロ知識証明を使うことですが、それ以外のこと（シリアル番号、私の身元など）は何も明らかにしません。\u003c/p\u003e\n\u003cp data-sourcepos=\"56:1-56:333\"\u003eココナッツはこのゼロ知識証明の能力を持っているので、ブラインドされた発行が可能なのです。通貨に関するゼロ知識証明と、身元保証や匿名投票プロトコルに関するゼロ知識証明は異なるので、それぞれの領域でコード化する必要があります。\u003c/p\u003e\n\u003ch3 data-sourcepos=\"59:1-59:69\"\u003e\n\u003cspan id=\"5-再ランダム化可能な署名randomizable-signatures\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#5-%E5%86%8D%E3%83%A9%E3%83%B3%E3%83%80%E3%83%A0%E5%8C%96%E5%8F%AF%E8%83%BD%E3%81%AA%E7%BD%B2%E5%90%8Drandomizable-signatures\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e5. 再ランダム化可能な署名（Randomizable Signatures）\u003c/h3\u003e\n\u003cp data-sourcepos=\"61:1-61:540\"\u003e署名に関連するもう一つの概念として、再ランダム化があります。このタイプの署名には、randomizeというアルゴリズムが1つ追加されています。これにより、ユーザーはある署名を受け取り、それに対してrandomize()を呼び出し、同じメッセージに対して有効な全く新しい署名を生成することができます。ランダム化された署名の新しいビット列は、元の署名と同等ですが、リンクすることはできません。\u003c/p\u003e\n\u003cp data-sourcepos=\"63:1-63:33\"\u003eどんなことに使えるのか\u003c/p\u003e\n\u003cp data-sourcepos=\"65:1-65:424\"\u003eあなたは署名を様々な人に見せることができ、見せた人はその署名が互いに、あるいはあなたにリンクしていることを理解することはできません。どの場合も根本的なメッセージは同じですから、偽造の可能性はありません。基本的には、複数の匿名化されたベアラートークン（bearer tokens）を持つことになります。\u003c/p\u003e\n\u003cp data-sourcepos=\"67:1-67:57\"\u003eココナッツは再ランダム化を行うのです。\u003c/p\u003e\n\u003ch3 data-sourcepos=\"69:1-69:48\"\u003e\n\u003cspan id=\"6-選択的開示selective-disclosure\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#6-%E9%81%B8%E6%8A%9E%E7%9A%84%E9%96%8B%E7%A4%BAselective-disclosure\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e6. 選択的開示（Selective Disclosure）\u003c/h3\u003e\n\u003cp data-sourcepos=\"71:1-71:113\"\u003eココナッツは、選択的開示（selective disclosure）と呼ばれるものも可能にしています。\u003c/p\u003e\n\u003cp data-sourcepos=\"73:1-73:36\"\u003e選択的開示とは何ですか？\u003c/p\u003e\n\u003cp data-sourcepos=\"75:1-75:306\"\u003e従来の署名方式では、メッセージに署名し、後で公開鍵を持っている人なら誰でもそれを検証することができます。通常、検証者は検証するためにメッセージの全文を持っていなければならない、という暗黙の制約があります。\u003c/p\u003e\n\u003cp data-sourcepos=\"77:1-77:204\"\u003e選択的開示では、検証のためにメッセージ全体を見せる必要はなく、メッセージの一部だけを見せ、他の部分は非公開のまま検証することができます。\u003c/p\u003e\n\u003cp data-sourcepos=\"79:1-79:189\"\u003e別の例として、パブに行きたいが、入り口で年齢を証明する必要があるとする。このとき、次のような属性を持つパスポートを提示します：\u003c/p\u003e\n\u003cp data-sourcepos=\"81:1-81:282\"\u003eあなたはパブに行きます。ドアの前にいる用心棒は、あなたの外見と年齢だけで、他のことは何も気にしません。しかし、毎回認証情報を全部見せるので、用心棒に対してすべての情報を渡す必要があります。\u003c/p\u003e\n\u003cp data-sourcepos=\"83:1-83:468\"\u003e選択的開示では、このプロセスを改善することができます。パスポートの全フィールドのタプルに署名し、いざ検証するときに、検証者は一部の属性だけを検証することができます。ここでいう検証者は公開されており、(a)パスポート発行当局の公開鍵を知っていて、(b)パスポート発行当局の署名を見ることができる人であれば誰でもいいのです。\u003c/p\u003e\n\u003cp data-sourcepos=\"85:1-85:70\"\u003eつまり、3つのパーティが存在することになります：\u003c/p\u003e\n\u003cp data-sourcepos=\"87:1-89:64\"\u003e1.パスポート発行当局（passport authority）\u003cbr\u003e\n2.主体 (あなた) (subject (you))\u003cbr\u003e\n3.証明書の利用者 (用心棒)（relying party (bouncer)）\u003c/p\u003e\n\u003cp data-sourcepos=\"91:1-91:132\"\u003eあなたは署名されたメッセージの一部、またはすべての属性を選択的に開示することができます。\u003c/p\u003e\n\u003cp data-sourcepos=\"93:1-93:734\"\u003e選択的開示は「一般的な」署名方式に適用できます(ブラインドされず、 再ランダム化もされない)。つまり、権威から署名付きメッセージを入手し、属性を選択的に開示することができるということです。これにより、必要以上の情報を公開しないことで、relying party（用心棒）に対するプライバシーが確保されるのです。しかし、同じパブにもう一度行ったり、パスポート当局が（署名した）メッセージを見たりすると、それらの当事者は署名と検証を結びつけることができ、その結果、subject（あなた）のプライバシーを破ることができます。\u003c/p\u003e\n\u003cp data-sourcepos=\"95:1-95:114\"\u003eさて、Nymの観点からすると、本当に興味深いのは、それをすべてまとめることです。\u003c/p\u003e\n\u003cp data-sourcepos=\"97:1-97:510\"\u003eブラインドされた認証情報、再ランダム化可能な署名、選択的開示を組み合わせれば、パブで複数の「提示」（認証情報を使用）からユーザーのプライバシーを守ることができます。用心棒は、あなたがそこに行ったのはいつも初めてだと思うだろうし、パスポート当局は、あなたが使っているブラインドされた認証情報を彼らが発行したものとリンクさせる方法がありません。\u003c/p\u003e\n\u003cp data-sourcepos=\"99:1-99:359\"\u003eある属性を示し、他の属性を示さないことで選択的に開示できるという考えは単純化したもので、それよりも強力なものです。年齢を直接明かす代わりに、実際の年齢を明かすことなく「私は18歳以上です」と断言するゼロ知識証明を関連付けることができるのです。\u003c/p\u003e\n\u003ch3 data-sourcepos=\"101:1-101:43\"\u003e\n\u003cspan id=\"7-閾値発行threshold-issuance\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#7-%E9%96%BE%E5%80%A4%E7%99%BA%E8%A1%8Cthreshold-issuance\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e7. 閾値発行（Threshold Issuance）\u003c/h3\u003e\n\u003cp data-sourcepos=\"103:1-103:468\"\u003eブラインド署名、再ランダム化可能な認証情報、選択的開示は、いずれも何年も、あるいは何十年も前から暗号の文献に存在するものです。これまで見てきたように、これらをつなぎ合わせると、いくつかの興味深い性質があることが分かります。しかし、これまで、これらの方式には、署名が常に中央集権的である、という共通点がありました。\u003c/p\u003e\n\u003cp data-sourcepos=\"105:1-105:491\"\u003eココナッツの大きな進歩は閾値発行（threshold issuance）です。Coconutでは、署名を行う権威が1つではなく、複数の権威が（まとめて、あるいはm個のうち閾値n個の署名として）署名を行います。これにより、Coconutベースの署名を分散的に発行することが可能になり、以前は中央集権的なシステムでのみ利用可能だったプライバシーの良さをすべて得ることができます。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"109:1-109:12\"\u003e\n\u003cspan id=\"最後に\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E6%9C%80%E5%BE%8C%E3%81%AB\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e最後に\u003c/h2\u003e\n\u003cp data-sourcepos=\"111:1-111:196\"\u003eNymのバリデーターはココナッツに基づいた署名を発行します。これにより、以下の4つの機能が追加され、強力なプライバシー保証を提供します：\u003c/p\u003e\n\u003col data-sourcepos=\"113:1-120:0\"\u003e\n\u003cli data-sourcepos=\"113:1-113:24\"\u003e鍵生成（Keygen）\u003c/li\u003e\n\u003cli data-sourcepos=\"114:1-114:21\"\u003e検証（Verify）\u003c/li\u003e\n\u003cli data-sourcepos=\"115:1-115:19\"\u003e署名（Sign）\u003c/li\u003e\n\u003cli data-sourcepos=\"116:1-116:24\"\u003eブラインド発行\u003c/li\u003e\n\u003cli data-sourcepos=\"117:1-117:36\"\u003e再ランダム化可能な署名\u003c/li\u003e\n\u003cli data-sourcepos=\"118:1-118:18\"\u003e選択的開示\u003c/li\u003e\n\u003cli data-sourcepos=\"119:1-120:0\"\u003e閾値発行\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp data-sourcepos=\"121:1-121:318\"\u003e当初はアクセス認証情報やサービスプロバイダーへの支払いを匿名化するために使用され、将来的にはブロックチェーンシステム（Bitcoin、Ethereumなど）にオンチェーンのトランザクションプライバシーを追加するためにも利用可能です。\u003c/p\u003e\n\u003ch1 data-sourcepos=\"123:1-123:17\"\u003e\n\u003cspan id=\"参考リンク\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E5%8F%82%E8%80%83%E3%83%AA%E3%83%B3%E3%82%AF\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e参考リンク\u003c/h1\u003e\n\u003cul data-sourcepos=\"125:1-131:0\"\u003e\n\u003cli data-sourcepos=\"125:1-125:111\"\u003e\u003ca href=\"https://ja.wikipedia.org/wiki/Nym%E3%83%9F%E3%83%83%E3%82%AF%E3%82%B9%E3%83%8D%E3%83%83%E3%83%88\" rel=\"nofollow noopener\" target=\"_blank\"\u003eNymのミキシングネットワーク - Wikipedia\u003c/a\u003e\u003c/li\u003e\n\u003cli data-sourcepos=\"126:1-126:44\"\u003e\u003ca href=\"https://nymtech.net/\" rel=\"nofollow noopener\" target=\"_blank\"\u003eNym公式サイト\u003c/a\u003e\u003c/li\u003e\n\u003cli data-sourcepos=\"127:1-127:71\"\u003e\u003ca href=\"https://nymtech.net/nym-whitepaper.pdf\" rel=\"nofollow noopener\" target=\"_blank\"\u003eNymホワイトペーパー\u003c/a\u003e\u003c/li\u003e\n\u003cli data-sourcepos=\"128:1-128:94\"\u003e\u003ca href=\"https://note.com/koko_crypto/n/n71fe7e92b68a\" rel=\"nofollow noopener\" target=\"_blank\"\u003e分散型VPN と従来型VPN:違いの全て\u003c/a\u003e\u003c/li\u003e\n\u003cli data-sourcepos=\"129:1-129:171\"\u003e\u003ca href=\"https://note.com/koko_crypto/n/n302e7516f691?magazine_key=m0129d65e3526\" rel=\"nofollow noopener\" target=\"_blank\"\u003eミックスネットとは何か？ VPNによる比類なきオンラインプライバシー\u003c/a\u003e\u003c/li\u003e\n\u003cli data-sourcepos=\"130:1-131:0\"\u003e\u003ca href=\"https://note.com/koko_crypto/n/n410daa0fdeec\" rel=\"nofollow noopener\" target=\"_blank\"\u003eSphinx暗号\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 data-sourcepos=\"132:1-132:20\"\u003e\n\u003cspan id=\"コミュニティ\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%82%B3%E3%83%9F%E3%83%A5%E3%83%8B%E3%83%86%E3%82%A3\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eコミュニティ\u003c/h1\u003e\n\u003cul data-sourcepos=\"134:1-141:0\"\u003e\n\u003cli data-sourcepos=\"134:1-134:68\"\u003e\u003ca href=\"https://t.me/nymjapanese\" rel=\"nofollow noopener\" target=\"_blank\"\u003eNym日本コミュニティ - Telegram\u003c/a\u003e\u003c/li\u003e\n\u003cli data-sourcepos=\"135:1-135:73\"\u003e\u003ca href=\"https://discord.com/invite/nym\" rel=\"nofollow noopener\" target=\"_blank\"\u003eNym日本コミュニティ - Discord\u003c/a\u003e\u003c/li\u003e\n\u003cli data-sourcepos=\"136:1-136:188\"\u003e\u003ca href=\"https://line.me/ti/g2/0YGYYq4EoQZ8fL5FYQmqU4O9SRuE_a6YX19GtA?utm_source=invitation\u0026amp;utm_medium=link_copy\u0026amp;utm_campaign=default\" rel=\"nofollow noopener\" target=\"_blank\"\u003eNym日本コミュニティ - LINEオープンチャット\u003c/a\u003e\u003c/li\u003e\n\u003cli data-sourcepos=\"137:1-137:81\"\u003e\u003ca href=\"https://line.me/R/ti/p/@986ytsdx?from=page\u0026amp;accountId=986ytsdx\" rel=\"nofollow noopener\" target=\"_blank\"\u003eNym 公式LINE\u003c/a\u003e\u003c/li\u003e\n\u003cli data-sourcepos=\"138:1-141:0\"\u003e\u003ca href=\"https://x.com/nymproject_jp\" rel=\"nofollow noopener\" target=\"_blank\"\u003eNymプロジェクト日本語アカウント - X\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"142:1-142:87\"\u003e原文は、\u003ca href=\"https://constructiveproof.com%E3%80%812020%E5%B9%B43%E6%9C%8824%E6%97%A5%E3%81%AB%E5%85%AC%E9%96%8B%E3%81%95%E3%82%8C%E3%81%BE%E3%81%97%E3%81%9F%E3%80%82\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://constructiveproof.com、2020年3月24日に公開されました。\u003c/a\u003e\u003c/p\u003e\n",
    "body": "\n\nNymプロジェクトには現在、2つのコア技術があります。\n\n## 1. Mixnetによるデータとメタデータの保護\n\nNymのMixnetは、インターネットを通過するすべてのTCP/IPパケットの匿名化を実現します。これにより、国家や企業レベルの強力な敵に対しても有効です。Mixnetは、メッセージの内容だけでなく、誰が誰と通信しているか、パケットがどのような経路を辿ったかなどの情報も隠します。\n\n## 2. 選択的認証情報の開示（Selective Disclosure Credential）\n\nNymは、EUのHorizon 2020が出資した「Decode」プロジェクトの「Coconut」を基盤とした設計技術を使用しています。ここでは、選択的な認証情報の開示の概念を説明します。\n\n![Screenshot 2024-08-22 at 13.23.00.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3672851/bf26533a-212e-c24c-db16-2a654056d428.png)\n\n\n### ココナッツ（Coconut）とその機能\n\nココナッツは、以下の3つの基本的なアルゴリズムから構成されています：\n\n1. **鍵生成（Keygen）**\n2. **検証（Verify）**\n3. **署名（Sign）**\n\nユーザーは、公開鍵/秘密鍵ペアを生成（keygen）することができなければなりません。そして、秘密鍵を使って平文の一部に署名することができます。その後、署名者の公開鍵を持っている人なら誰でも、その署名が有効であることを検証できるはずです。RSA、DSA、そしてED25519のような新しく魅力的なものまで、すべての署名方式は同じように機能します。\n\nココナッツは、この基本的な3つの機能を、さらにいくつかの機能で拡張しています。追加された機能を理解することが、それを使って何ができるかを理解する鍵になります。どんなアルゴリズムが追加されるのか見てみましょう：\n\n### 4. ブラインド発行（Blind Issuance）\n\navid Chaumは1980年代にブラインド署名を発明しました。ブラインド署名は、上記の性質に加えて、いくつかの特別な性質を持っています。\n\nkeygen、verify、signの各アルゴリズムはまだ存在します。また、署名の秘密鍵を持たない人がメッセージをブラインドさせるアルゴリズムもあります。平文の一部を取り出し、秘密鍵を持つ署名者が署名できるような無意味な文章に変えてしまうのです。\n\nこの署名は、メッセージを作成した人物に戻され、まだブラインドされたままです。この署名は、その人物によって解除することができます。この署名は、元の平文メッセージに対する有効な署名と同等になります。いったんブラインドが解除されると、署名者はブラインドされたメッセージの署名とブラインド解除されたメッセージを結びつけることができなくなり ます。\n\n理解を深めるには、例を挙げるのが効果的です。仮に存在するシンプルな個人向け電子マネーのスキームを例にとってみましょう。それを利用する際の手順を説明します：\n\n・1ドルを持っているというメッセージを平文で作成する。\n\n・ブラインド署名のアルゴリズムを使って、それを見えなくする。平文のメッセージは無意味な言葉に変わる。\n\n・ブラインドされたメッセージを1ドルと一緒に銀行に送る。\n\n・銀行は、私が1ドルを持っていることを証明するために署名し、その署名を私に返す。\n\n・私はその署名を解除し、銀行から戻ってきた署名とのリンクを解除する。これが、私のプライバシーを保証する重要なステップです。\n\n・私は今、私が1ドルを持っていることを証明する匿名のbearerトークンを持っています。\n\nBearerトークンは銀行からもらったものとリンクできませんが、その有効性は銀行の公開鍵があれば誰でも検証できます。\n\nこれがblind issuance（ブラインド発行）であり、そのプロセスは平文作成、ブラインド、署名者への送信、署名、作成者への返却、ブラインド解除、使用となります。ブラインド発行には問題があります。署名者は自分が何に署名しているのか全く分からないのです。\n\nつまりあなたが銀行の立場なら、訳のわからない言葉を受け取る、ということになります。銀行が「100万ドルあります」と書いていないことをどうやって確認するのでしょう？それは銀行にとって不透明なことなのです。一般的な解決策は、メッセージに1ドルが含まれていることを証明するゼロ知識証明を使うことですが、それ以外のこと（シリアル番号、私の身元など）は何も明らかにしません。\n\nココナッツはこのゼロ知識証明の能力を持っているので、ブラインドされた発行が可能なのです。通貨に関するゼロ知識証明と、身元保証や匿名投票プロトコルに関するゼロ知識証明は異なるので、それぞれの領域でコード化する必要があります。\n\n\n### 5. 再ランダム化可能な署名（Randomizable Signatures）\n\n署名に関連するもう一つの概念として、再ランダム化があります。このタイプの署名には、randomizeというアルゴリズムが1つ追加されています。これにより、ユーザーはある署名を受け取り、それに対してrandomize()を呼び出し、同じメッセージに対して有効な全く新しい署名を生成することができます。ランダム化された署名の新しいビット列は、元の署名と同等ですが、リンクすることはできません。\n\nどんなことに使えるのか\n\nあなたは署名を様々な人に見せることができ、見せた人はその署名が互いに、あるいはあなたにリンクしていることを理解することはできません。どの場合も根本的なメッセージは同じですから、偽造の可能性はありません。基本的には、複数の匿名化されたベアラートークン（bearer tokens）を持つことになります。\n\nココナッツは再ランダム化を行うのです。\n\n### 6. 選択的開示（Selective Disclosure）\n\nココナッツは、選択的開示（selective disclosure）と呼ばれるものも可能にしています。\n\n選択的開示とは何ですか？\n\n従来の署名方式では、メッセージに署名し、後で公開鍵を持っている人なら誰でもそれを検証することができます。通常、検証者は検証するためにメッセージの全文を持っていなければならない、という暗黙の制約があります。\n\n選択的開示では、検証のためにメッセージ全体を見せる必要はなく、メッセージの一部だけを見せ、他の部分は非公開のまま検証することができます。\n\n別の例として、パブに行きたいが、入り口で年齢を証明する必要があるとする。このとき、次のような属性を持つパスポートを提示します：\n\nあなたはパブに行きます。ドアの前にいる用心棒は、あなたの外見と年齢だけで、他のことは何も気にしません。しかし、毎回認証情報を全部見せるので、用心棒に対してすべての情報を渡す必要があります。\n\n選択的開示では、このプロセスを改善することができます。パスポートの全フィールドのタプルに署名し、いざ検証するときに、検証者は一部の属性だけを検証することができます。ここでいう検証者は公開されており、(a)パスポート発行当局の公開鍵を知っていて、(b)パスポート発行当局の署名を見ることができる人であれば誰でもいいのです。\n\nつまり、3つのパーティが存在することになります：\n\n1.パスポート発行当局（passport authority）\n2.主体 (あなた) (subject (you))\n3.証明書の利用者 (用心棒)（relying party (bouncer)）\n\nあなたは署名されたメッセージの一部、またはすべての属性を選択的に開示することができます。\n\n選択的開示は「一般的な」署名方式に適用できます(ブラインドされず、 再ランダム化もされない)。つまり、権威から署名付きメッセージを入手し、属性を選択的に開示することができるということです。これにより、必要以上の情報を公開しないことで、relying party（用心棒）に対するプライバシーが確保されるのです。しかし、同じパブにもう一度行ったり、パスポート当局が（署名した）メッセージを見たりすると、それらの当事者は署名と検証を結びつけることができ、その結果、subject（あなた）のプライバシーを破ることができます。\n\nさて、Nymの観点からすると、本当に興味深いのは、それをすべてまとめることです。\n\nブラインドされた認証情報、再ランダム化可能な署名、選択的開示を組み合わせれば、パブで複数の「提示」（認証情報を使用）からユーザーのプライバシーを守ることができます。用心棒は、あなたがそこに行ったのはいつも初めてだと思うだろうし、パスポート当局は、あなたが使っているブラインドされた認証情報を彼らが発行したものとリンクさせる方法がありません。\n\nある属性を示し、他の属性を示さないことで選択的に開示できるという考えは単純化したもので、それよりも強力なものです。年齢を直接明かす代わりに、実際の年齢を明かすことなく「私は18歳以上です」と断言するゼロ知識証明を関連付けることができるのです。\n\n### 7. 閾値発行（Threshold Issuance）\n\nブラインド署名、再ランダム化可能な認証情報、選択的開示は、いずれも何年も、あるいは何十年も前から暗号の文献に存在するものです。これまで見てきたように、これらをつなぎ合わせると、いくつかの興味深い性質があることが分かります。しかし、これまで、これらの方式には、署名が常に中央集権的である、という共通点がありました。\n\nココナッツの大きな進歩は閾値発行（threshold issuance）です。Coconutでは、署名を行う権威が1つではなく、複数の権威が（まとめて、あるいはm個のうち閾値n個の署名として）署名を行います。これにより、Coconutベースの署名を分散的に発行することが可能になり、以前は中央集権的なシステムでのみ利用可能だったプライバシーの良さをすべて得ることができます。\n\n\n\n## 最後に\n\nNymのバリデーターはココナッツに基づいた署名を発行します。これにより、以下の4つの機能が追加され、強力なプライバシー保証を提供します：\n\n1. 鍵生成（Keygen）\n2. 検証（Verify）\n3. 署名（Sign）\n4. ブラインド発行\n5. 再ランダム化可能な署名\n6. 選択的開示\n7. 閾値発行\n\n当初はアクセス認証情報やサービスプロバイダーへの支払いを匿名化するために使用され、将来的にはブロックチェーンシステム（Bitcoin、Ethereumなど）にオンチェーンのトランザクションプライバシーを追加するためにも利用可能です。\n\n# 参考リンク\n\n- [Nymのミキシングネットワーク - Wikipedia](https://ja.wikipedia.org/wiki/Nymミックスネット)\n- [Nym公式サイト](https://nymtech.net/)\n- [Nymホワイトペーパー](https://nymtech.net/nym-whitepaper.pdf)\n- [分散型VPN と従来型VPN:違いの全て](https://note.com/koko_crypto/n/n71fe7e92b68a)\n- [ミックスネットとは何か？ VPNによる比類なきオンラインプライバシー](https://note.com/koko_crypto/n/n302e7516f691?magazine_key=m0129d65e3526)\n- [Sphinx暗号](https://note.com/koko_crypto/n/n410daa0fdeec)\n\n# コミュニティ\n\n- [Nym日本コミュニティ - Telegram](https://t.me/nymjapanese)\n- [Nym日本コミュニティ - Discord](https://discord.com/invite/nym)\n- [Nym日本コミュニティ - LINEオープンチャット](https://line.me/ti/g2/0YGYYq4EoQZ8fL5FYQmqU4O9SRuE_a6YX19GtA?utm_source=invitation\u0026utm_medium=link_copy\u0026utm_campaign=default)\n- [Nym 公式LINE](https://line.me/R/ti/p/@986ytsdx?from=page\u0026accountId=986ytsdx)\n- [Nymプロジェクト日本語アカウント - X](https://x.com/nymproject_jp)\n\n\n\n原文は、https://constructiveproof.com、2020年3月24日に公開されました。\n",
    "coediting": false,
    "comments_count": 0,
    "created_at": "2024-08-22T13:23:42+09:00",
    "group": null,
    "id": "b2739ae32f4fb29de0b6",
    "likes_count": 4,
    "private": false,
    "reactions_count": 0,
    "stocks_count": 4,
    "tags": [
      {
        "name": "Security",
        "versions": []
      },
      {
        "name": "VPN",
        "versions": []
      },
      {
        "name": "初心者",
        "versions": []
      },
      {
        "name": "ブロックチェーン",
        "versions": []
      },
      {
        "name": "#プライバシー",
        "versions": []
      }
    ],
    "title": "ココナッツ認証（Coconut Credentials）とは？- プライバシーを保護するゼロ知識証明技術",
    "updated_at": "2024-08-22T13:25:19+09:00",
    "url": "https://qiita.com/pseudonym2/items/b2739ae32f4fb29de0b6",
    "user": {
      "description": "",
      "facebook_id": "",
      "followees_count": 6,
      "followers_count": 11,
      "github_login_name": null,
      "id": "pseudonym2",
      "items_count": 21,
      "linkedin_id": "",
      "location": "",
      "name": "",
      "organization": "",
      "permanent_id": 3672851,
      "profile_image_url": "https://s3-ap-northeast-1.amazonaws.com/qiita-image-store/0/3672851/272c5c10d9ec68ae3445a46256cfca8c7d9905fd/x_large.png?1721665575",
      "team_only": false,
      "twitter_screen_name": null,
      "website_url": ""
    },
    "page_views_count": null,
    "team_membership": null,
    "organization_url_name": null,
    "slide": false
  },
  {
    "rendered_body": "\u003cp data-sourcepos=\"1:1-3:96\"\u003eこんにちは、推しと一緒に住んでいる元保育士のりゆです。\u003cbr\u003e\n\u003cstrong\u003e推し\u003c/strong\u003eとは\u003cstrong\u003e夫\u003c/strong\u003eのことなのですが、最近特に夫との待ち合わせが至福のひと時なのです。\u003cbr\u003e\nただ現実で待ち合わせている時、あることに気づいてしまったのです。\u003c/p\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/5c2a769aceec71045bca91943cd0e7a2a270e6a1/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f333732303833312f30656237643361632d363961382d306235382d346631632d6336346436633464333833342e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3720831%2F0eb7d3ac-69a8-0b58-4f1c-c64d6c4d3834.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=92781b7e93c3e055e299a67a56b24d1f\" width=\"70%\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3720831/0eb7d3ac-69a8-0b58-4f1c-c64d6c4d3834.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3720831%2F0eb7d3ac-69a8-0b58-4f1c-c64d6c4d3834.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=fb5720aa852415212aa205df462aabcd 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\n\u003ch2 data-sourcepos=\"7:1-7:66\"\u003e\n\u003cspan id=\"推しが近づいてくるところが見られないぞ\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E6%8E%A8%E3%81%97%E3%81%8C%E8%BF%91%E3%81%A5%E3%81%84%E3%81%A6%E3%81%8F%E3%82%8B%E3%81%A8%E3%81%93%E3%82%8D%E3%81%8C%E8%A6%8B%E3%82%89%E3%82%8C%E3%81%AA%E3%81%84%E3%81%9E\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e推しが近づいてくるところが見られないぞ！？\u003c/h2\u003e\n\u003cp data-sourcepos=\"9:1-11:81\"\u003eかといって世に出ているGPSアプリは、それぞれの個人情報を大切にしたいのでやりたくないのですね。\u003cbr\u003e\n私自身、現在地を把握したいという欲求はありませんでした。\u003cbr\u003e\nただ推しが近づいてくるドキドキ感を味わいたいのです！！\u003c/p\u003e\n\u003cp data-sourcepos=\"13:1-15:225\"\u003e現状世の中を調べてみましたが、私が求めているサービスはありませんでした・・・😅\u003cbr\u003e\nなので！ないなら作るしかないですね！！\u003cbr\u003e\n大まかな距離で居場所がわかり、数字ではなく直感的に近づいてきていることが認識できて待ち合わせ時間が今までより楽しい体験になるものを作ろうと考えました！\u003c/p\u003e\n\u003ch2 data-sourcepos=\"16:1-16:57\"\u003e\n\u003cspan id=\"距離に応じて色が光るwebアプリ爆誕\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E8%B7%9D%E9%9B%A2%E3%81%AB%E5%BF%9C%E3%81%98%E3%81%A6%E8%89%B2%E3%81%8C%E5%85%89%E3%82%8Bweb%E3%82%A2%E3%83%97%E3%83%AA%E7%88%86%E8%AA%95\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e距離に応じて色が光るWEBアプリ爆誕！！\u003c/h2\u003e\n\u003cp data-sourcepos=\"18:1-19:93\"\u003e色の変化をわかりやすくするため、動画にしました！ご覧ください↓\u003cbr\u003e\n※動画編集上人揺れが起きていますが、アプリ内では起こりません！\u003c/p\u003e\n\u003cp data-sourcepos=\"21:1-21:50\"\u003e\u003ciframe id=\"qiita-embed-content__e49e6dfb4f2a60a8a3352ea890c0011b\" src=\"https://qiita.com/embed-contents/tweet#qiita-embed-content__e49e6dfb4f2a60a8a3352ea890c0011b\" data-content=\"https%3A%2F%2Fx.com%2Friyu___11%2Fstatus%2F1826442136440766845\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\"\u003e\n\u003c/iframe\u003e\n\u003c/p\u003e\n\u003cp data-sourcepos=\"24:1-25:72\"\u003e距離５００mごとに応じて色が変わっていきます。（最大４０００mまで）\u003cbr\u003e\n色は赤〜紫色の中でグラデーションになっています。\u003c/p\u003e\n\u003cp data-sourcepos=\"27:1-28:111\"\u003e２人の距離が、近いと赤色、遠いと紫色になっています。\u003cbr\u003e\n推しをイメージした人型の心臓吹きにあるハートの部分の色が変わっていきます！\u003c/p\u003e\n\u003ch3 data-sourcepos=\"30:1-30:43\"\u003e\n\u003cspan id=\"今回作ったもののイメージ図\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E4%BB%8A%E5%9B%9E%E4%BD%9C%E3%81%A3%E3%81%9F%E3%82%82%E3%81%AE%E3%81%AE%E3%82%A4%E3%83%A1%E3%83%BC%E3%82%B8%E5%9B%B3\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e今回作ったもののイメージ図\u003c/h3\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/8d81ef46ed2fab9f035251d89e5f10dd5d49841a/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f333732303833312f39336537336564392d386130632d663031362d383064392d3962636433383535636538322e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3720831%2F93e73ed9-8a0c-f016-80d9-9bcd3855ce82.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=e0e5c724651163165d366e9ae693f059\" width=\"70%\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3720831/93e73ed9-8a0c-f016-80d9-9bcd3855ce82.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3720831%2F93e73ed9-8a0c-f016-80d9-9bcd3855ce82.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=6110fec0bcd718ff925ac027663e563a 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\n\u003ch3 data-sourcepos=\"34:1-34:16\"\u003e\n\u003cspan id=\"制作環境\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E5%88%B6%E4%BD%9C%E7%92%B0%E5%A2%83\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e制作環境\u003c/h3\u003e\n\u003cp data-sourcepos=\"36:1-41:93\"\u003e→ウェブアプリ開発プラットフォームです！！\u003cbr\u003e\n・\u003cstrong\u003eNetlify\u003c/strong\u003e：\u003ca href=\"https://www.netlify.com/\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://www.netlify.com/\u003c/a\u003e\u003cbr\u003e\n→静的サイトをホスティングできる\u003cbr\u003e\n・\u003cstrong\u003eFirebase\u003c/strong\u003e：\u003ca href=\"https://firebase.google.com/?hl=ja\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://firebase.google.com/?hl=ja\u003c/a\u003e\u003cbr\u003e\n→動的コンテンツの提供\u003cbr\u003e\n→リアルタイムにデータを同期・保存できるものも提供されている。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"43:1-43:56\"\u003e\n\u003cspan id=\"netlify-でgps座標を取得するアプリを作る\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#netlify-%E3%81%A7gps%E5%BA%A7%E6%A8%99%E3%82%92%E5%8F%96%E5%BE%97%E3%81%99%E3%82%8B%E3%82%A2%E3%83%97%E3%83%AA%E3%82%92%E4%BD%9C%E3%82%8B\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eNetlify でGPS座標を取得するアプリを作る\u003c/h2\u003e\n\u003cp data-sourcepos=\"45:1-45:55\"\u003e・Netlifyでログインし、アプリを作成する\u003c/p\u003e\n\u003cdetails\u003e\u003csummary\u003eアプリのコード\u003c/summary\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"　html\" data-sourcepos=\"49:1-139:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\n\u0026lt;html\u0026gt;\n  \u0026lt;head\u0026gt;\n    \u0026lt;meta charset=\"utf-8\"\u0026gt;\n    \u0026lt;title\u0026gt;GPS\u0026lt;/title\u0026gt;\n    \u0026lt;meta name=\"viewport\" content=\"width=device-width\" /\u0026gt;\n    \u0026lt;meta name=\"description\" content=\"Displays the straight line distance to Station.\" /\u0026gt;\n    \u0026lt;script src=\"https://geographiclib.sourceforge.io/scripts/geographiclib-geodesic.min.js\"\u0026gt;\u0026lt;/script\u0026gt;\n    \u0026lt;style\u0026gt;\n      * {\n        margin: 0;\n        padding: 0;\n      }\n\n      body {\n        display: flex;\n        align-items: center;\n        justify-content: center;\n        color: #FAFAFA;\n        font-size: 24px;\n        font-weight: 900;\n        background: #FFC0CB; /* 背景色はピンク色 */\n      }\n\n      .display {\n        display: flex;\n        flex-direction: column;\n        align-items: center;\n      }\n\n      #distanceText {\n        font-size: 24px;\n        margin-bottom: 10px;\n      }\n\n      #m {\n        font-size: 40px;\n      }\n\n      #timestamp {\n        position: fixed;\n        bottom: 8px; right: 8px;\n        color: #0D47A1;\n        font-size: 12px;\n      }\n    \u0026lt;/style\u0026gt;\n  \u0026lt;/head\u0026gt;\n  \u0026lt;body\u0026gt;\n    \u0026lt;div\u0026gt;\n      \u0026lt;div class=\"display\"\u0026gt;\n        \u0026lt;p id=\"distanceText\"\u0026gt;2人の距離\u0026lt;/p\u0026gt; \u0026lt;!-- 距離表示の上にテキストを追加 --\u0026gt;\n        \u0026lt;p id=\"m\"\u0026gt;\u0026lt;/p\u0026gt;\n      \u0026lt;/div\u0026gt;\n      \u0026lt;p id=\"timestamp\"\u0026gt;\u0026lt;/p\u0026gt;\n    \u0026lt;/div\u0026gt;\n    \u0026lt;script\u0026gt;\n      const goalLatLong = [,]; // 目標地点の緯度,経度\n\n      navigator.geolocation.watchPosition(handleWatchPosition, null, {\n        enableHighAccuracy: true,\n        timeout: 60000,\n        maximumAge: 0\n      });\n\n      function handleWatchPosition(evt) {\n        const latitude = evt.coords.latitude;\n        const longitude = evt.coords.longitude;\n        const accuracy = evt.coords.accuracy;\n        const altitude = evt.coords.altitude;\n        const altitudeAccuracy = evt.coords.altitudeAccuracy;\n        const heading = evt.coords.heading || 0;\n        const speed = evt.coords.speed;\n        const timestamp = evt.timestamp;\n\n        document.getElementById('timestamp').textContent = timestamp;\n\n        if (latitude \u0026amp;\u0026amp; longitude) {\n          const geod = window.geodesic.Geodesic.WGS84;\n          const r = geod.Inverse(\n            latitude, longitude,\n            ...goalLatLong\n          );\n\n          document.getElementById('m').textContent = `${ r.s12.toFixed(2) }m`;\n        }\n      }\n    \u0026lt;/script\u0026gt;\n  \u0026lt;/body\u0026gt;\n\u0026lt;/html\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003c/details\u003e\n\u003cp data-sourcepos=\"142:1-143:130\"\u003e↓実際の画面\u003cbr\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/c318c19f07061e9c78e4c86342243f3f9fc83dff/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f333732303833312f61326463303135342d623365392d623138612d313565642d3633366435393166313033652e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3720831%2Fa2dc0154-b3e9-b18a-15ed-636d591f103e.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=4ba4d077260bc5c267c0e4d8b2332eda\" width=\"70%\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3720831/a2dc0154-b3e9-b18a-15ed-636d591f103e.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3720831%2Fa2dc0154-b3e9-b18a-15ed-636d591f103e.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=9726997ba5fceccb44e1322df6814e72 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"145:1-146:52\"\u003e・距離に応じて色の変化を緩やかなグラデーションにした画面\u003cbr\u003e\n（コードはChatGPTに生成してもらった）\u003c/p\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/98683115ad4b0814cd56f89ae73aae4a51119abc/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f333732303833312f34626466636632642d336362342d326537612d653732652d6464346639613931656334312e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3720831%2F4bdfcf2d-3cb4-2e7a-e72e-dd4f9a91ec41.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=21a77f04c238cbd3257285c1ccf96a16\" width=\"70%\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3720831/4bdfcf2d-3cb4-2e7a-e72e-dd4f9a91ec41.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3720831%2F4bdfcf2d-3cb4-2e7a-e72e-dd4f9a91ec41.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=4a525b475bba794307493df1891ac37c 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/6504600da46db70a9b4cc1c98c6570bcb96e50d6/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f333732303833312f37623530613461652d656235342d393531642d646637362d6266666631653363653131302e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3720831%2F7b50a4ae-eb54-951d-df76-bfff1e3ce110.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=016ceb75a8a9299f716e8c6889216a82\" width=\"70%\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3720831/7b50a4ae-eb54-951d-df76-bfff1e3ce110.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3720831%2F7b50a4ae-eb54-951d-df76-bfff1e3ce110.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=46e16f30b4579dc2e0326162b723db83 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\n\u003cp data-sourcepos=\"151:1-154:131\"\u003e・人の形をimageとしてコードに挿入\u003cbr\u003e\n→人の画像はprocreateで描きました！\u003cbr\u003e\n　ハートの形は色が変わるところが見たいため、部分的に透過して画像を作っています。\u003cbr\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/ca43c1a5999d568a0ab659843bbfa5612547cf96/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f333732303833312f37383939313737632d346239322d323332362d383832622d6538353962303665366161392e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3720831%2F7899177c-4b92-2326-882b-e859b06e6aa9.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=7511095d16b0cefc3166a27fa7ff712b\" width=\"70%\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3720831/7899177c-4b92-2326-882b-e859b06e6aa9.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3720831%2F7899177c-4b92-2326-882b-e859b06e6aa9.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=9ca1a52540d92cdbf4f7afe757078134 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2 data-sourcepos=\"156:1-156:57\"\u003e\n\u003cspan id=\"firebaseでgpsの座標を保存するindexhtml\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#firebase%E3%81%A7gps%E3%81%AE%E5%BA%A7%E6%A8%99%E3%82%92%E4%BF%9D%E5%AD%98%E3%81%99%E3%82%8Bindexhtml\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eFirebaseでGPSの座標を保存する（index.html）\u003c/h2\u003e\n\u003cdetails\u003e\u003csummary\u003e最終コード\u003c/summary\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"　html\" data-sourcepos=\"160:1-305:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\n\u0026lt;!DOCTYPE html\u0026gt;\n\u0026lt;html lang=\"ja\"\u0026gt;\n  \u0026lt;head\u0026gt;\n    \u0026lt;meta charset=\"UTF-8\"\u0026gt;\n    \u0026lt;title\u0026gt;GPS-based Color Gradient\u0026lt;/title\u0026gt;\n    \u0026lt;meta name=\"viewport\" content=\"width=device-width\" /\u0026gt;\n    \u0026lt;meta name=\"description\" content=\"Displays the straight line distance to the current location.\" /\u0026gt;\n    \u0026lt;script src=\"https://cdn.jsdelivr.net/npm/geographiclib@1.52.2/geographiclib.min.js\"\u0026gt;\u0026lt;/script\u0026gt;\n    \u0026lt;script defer src=\"path/to/firebase-app-compat.js\"\u0026gt;\u0026lt;/script\u0026gt;\n    \u0026lt;script defer src=\"path/to/firebase-firestore-compat.js\"\u0026gt;\u0026lt;/script\u0026gt;\n    \u0026lt;script defer src=\"path/to/init.js\"\u0026gt;\u0026lt;/script\u0026gt;\n    \u0026lt;style\u0026gt;\n      * {\n        margin: 0;\n        padding: 0;\n      }\n\n      body {\n        display: flex;\n        align-items: center;\n        justify-content: center;\n        color: #000000;\n        font-size: 24px;\n        font-weight: 900;\n        background: hsl(0, 70%, 90%);\n        transition: background 0.5s ease;\n        height: 100vh;\n        text-align: center;\n      }\n\n      .display {\n        display: none;\n        flex-direction: column;\n        align-items: center;\n        justify-content: center;\n      }\n\n      #distanceText {\n        font-size: 24px;\n        margin-bottom: 10px;\n      }\n\n      #m {\n        font-size: 40px;\n      }\n\n      #timestamp {\n        position: fixed;\n        bottom: 8px;\n        right: 8px;\n        color: #000000;\n        font-size: 12px;\n        z-index: 1;\n      }\n\n      img {\n        position: absolute;\n        top: 50%;\n        left: 50%;\n        transform: translate(-50%, -50%);\n        height: 100vh;\n        width: 100vw;\n        object-fit: cover;\n        z-index: 0;\n      }\n    \u0026lt;/style\u0026gt;\n  \u0026lt;/head\u0026gt;\n  \u0026lt;body\u0026gt;\n    \u0026lt;img src=\"path/to/image.png\" alt=\"Character Image\"\u0026gt;\n    \u0026lt;div class=\"display\"\u0026gt;\n      \u0026lt;p id=\"distanceText\"\u0026gt;２人の距離\u0026lt;/p\u0026gt;\n      \u0026lt;p id=\"m\"\u0026gt;\u0026lt;/p\u0026gt;\n    \u0026lt;/div\u0026gt;\n    \u0026lt;p id=\"timestamp\"\u0026gt;\u0026lt;/p\u0026gt;\n    \u0026lt;script type=\"module\"\u0026gt;\n      const app = firebase.app();\n      const db = firebase.firestore();\n\n      (async () =\u0026gt; {\n        const updateRef = db.doc('path/to/document');\n        const now = new Date();\n        await updateRef.set({\n          value: `${now.toLocaleString()}`\n        });\n        const { value } = (await updateRef.get()).data();\n\n        console.log(value);\n      })();\n\n      const goalLatLong = [,]; // Replace with target location's latitude, longitude\n      navigator.geolocation.watchPosition(handleWatchPosition, null, {\n        enableHighAccuracy: true,\n        timeout: 60000,\n        maximumAge: 0\n      });\n\n      function handleWatchPosition(evt) {\n        const latitude = evt.coords.latitude;\n        const longitude = evt.coords.longitude;\n        const timestamp = evt.timestamp;\n        document.getElementById('timestamp').textContent = new Date(timestamp).toLocaleString();\n        if (latitude \u0026amp;\u0026amp; longitude) {\n          const geod = window.GeographicLib.Geodesic.WGS84;\n          const r = geod.Inverse(latitude, longitude, ...goalLatLong);\n          const distance = r.s12;\n          document.getElementById('m').textContent = `${distance.toFixed(2)}m`;\n\n          const maxDistance = 4000;\n          const hueIntervals = [0, 30, 60, 90, 120, 180, 210, 270];\n          let hue;\n          if (distance \u0026lt;= 500) {\n            hue = interpolateHue(distance, 0, 500, hueIntervals[0], hueIntervals[1]);\n          } else if (distance \u0026lt;= 1000) {\n            hue = interpolateHue(distance, 500, 1000, hueIntervals[1], hueIntervals[2]);\n          } else if (distance \u0026lt;= 1500) {\n            hue = interpolateHue(distance, 1000, 1500, hueIntervals[2], hueIntervals[3]);\n          } else if (distance \u0026lt;= 2000) {\n            hue = interpolateHue(distance, 1500, 2000, hueIntervals[3], hueIntervals[4]);\n          } else if (distance \u0026lt;= 2500) {\n            hue = interpolateHue(distance, 2000, 2500, hueIntervals[4], hueIntervals[5]);\n          } else if (distance \u0026lt;= 3000) {\n            hue = interpolateHue(distance, 2500, 3000, hueIntervals[5], hueIntervals[6]);\n          } else if (distance \u0026lt;= 3500) {\n            hue = interpolateHue(distance, 3000, 3500, hueIntervals[6], hueIntervals[7]);\n          } else if (distance \u0026lt;= 4000) {\n            hue = interpolateHue(distance, 3500, 4000, hueIntervals[7], hueIntervals[7]);\n          } else {\n            hue = hueIntervals[7];\n          }\n          const saturation = 70;\n          const lightness = 90;\n          document.body.style.backgroundColor = `hsl(${hue}, ${saturation}%, ${lightness}%)`;\n        }\n      }\n\n      function interpolateHue(distance, minDist, maxDist, startHue, endHue) {\n        const ratio = (distance - minDist) / (maxDist - minDist);\n        return startHue + ratio * (endHue - startHue);\n      }\n    \u0026lt;/script\u0026gt;\n  \u0026lt;/body\u0026gt;\n\u0026lt;/html\u0026gt;\n\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003c/details\u003e\n\u003cp data-sourcepos=\"308:1-309:56\"\u003eターミナルで「firebase.deploy」をして値を取る\u003cbr\u003e\nCloud storeの中にGPSの値を取ることができる\u003c/p\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/d9be49fc1e51d743eb759646bcf6e87df89b85f9/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f333732303833312f63636165633164352d383434302d616231332d326533372d6562353766353466323332382e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3720831%2Fccaec1d5-8440-ab13-2e37-eb57f54f2328.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=a2dbeb2ae6a380ff5cf42b66c4b2169b\" width=\"70%\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3720831/ccaec1d5-8440-ab13-2e37-eb57f54f2328.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3720831%2Fccaec1d5-8440-ab13-2e37-eb57f54f2328.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=3cbfbbe180d0938e1c3d46d911d8dd5f 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\n\u003ch2 data-sourcepos=\"313:1-313:42\"\u003e\n\u003cspan id=\"完成したので使ってもらった\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E5%AE%8C%E6%88%90%E3%81%97%E3%81%9F%E3%81%AE%E3%81%A7%E4%BD%BF%E3%81%A3%E3%81%A6%E3%82%82%E3%82%89%E3%81%A3%E3%81%9F\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e完成したので使ってもらった\u003c/h2\u003e\n\u003cp data-sourcepos=\"314:1-315:53\"\u003e・家族２名に使ってもらって感想を聞いてみました！\u003cbr\u003e\n　とてもよいFBをもらえたと思います。\u003c/p\u003e\n\u003cdiv data-sourcepos=\"317:1-323:3\" class=\"note info\"\u003e\n\u003cspan class=\"fa fa-fw fa-check-circle\"\u003e\u003c/span\u003e\u003cdiv\u003e\n\u003cp data-sourcepos=\"318:1-322:87\"\u003e\u003cstrong\u003eWEBアプリの価値について\u003c/strong\u003e\u003cbr\u003e\n・日常にない体験ができるね！\u003cbr\u003e\n・アイデアが今まで見たことがなくて、面白そう。\u003cbr\u003e\n・好きな人が近づいてくることがわかると、ワクワクするね！\u003cbr\u003e\n・具体的な居場所が分からないので、個人情報も守られて良い。\u003c/p\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv data-sourcepos=\"325:2-329:3\" class=\"note info\"\u003e\n\u003cspan class=\"fa fa-fw fa-check-circle\"\u003e\u003c/span\u003e\u003cdiv\u003e\n\u003cp data-sourcepos=\"326:1-328:76\"\u003e\u003cstrong\u003eWEBアプリの機能・UIについて\u003c/strong\u003e\u003cbr\u003e\n・顔が似てるって特別感があって良いね\u003cbr\u003e\n・５００mという距離の感覚と、色合いもちょうどいいよ\u003c/p\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv data-sourcepos=\"331:1-336:3\" class=\"note warn\"\u003e\n\u003cspan class=\"fa fa-fw fa-exclamation-circle\"\u003e\u003c/span\u003e\u003cdiv\u003e\n\u003cp data-sourcepos=\"332:1-335:93\"\u003e\u003cstrong\u003eWEBアプリの機能・UIについて\u003c/strong\u003e\u003cbr\u003e\n・常にアプリを開いている状態だったら、めちゃめちゃ困る。\u003cbr\u003e\n　タブを後ろに持っていくなど、画面の切り替えができたらいいな。\u003cbr\u003e\n・GPSをとることでバッテリーがたくさん使われる、その点が気になる\u003c/p\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003ch3 data-sourcepos=\"338:1-338:45\"\u003e\n\u003cspan id=\"fbをうけて私はどう思った\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#fb%E3%82%92%E3%81%86%E3%81%91%E3%81%A6%E7%A7%81%E3%81%AF%E3%81%A9%E3%81%86%E6%80%9D%E3%81%A3%E3%81%9F\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eFBをうけて、私はどう思った？\u003c/h3\u003e\n\u003cp data-sourcepos=\"339:1-340:183\"\u003e・日常にない体験で新しいアイデアだという言葉がとても嬉しかった！\u003cbr\u003e\n・GPSアプリを常に開いている状態になってしまうのは私も気になっていて、これから他に手段はないか模索していきたいと考えている。\u003c/p\u003e\n\u003cp data-sourcepos=\"342:1-342:85\"\u003e\u003cstrong\u003eその他まだまだ検証していくことがたくさんありそうだ！！\u003c/strong\u003e\u003c/p\u003e\n",
    "body": "こんにちは、推しと一緒に住んでいる元保育士のりゆです。\n**推し**とは**夫**のことなのですが、最近特に夫との待ち合わせが至福のひと時なのです。\nただ現実で待ち合わせている時、あることに気づいてしまったのです。\n\n\u003cimg src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3720831/0eb7d3ac-69a8-0b58-4f1c-c64d6c4d3834.png\" width=70%\u003e\n\n## 推しが近づいてくるところが見られないぞ！？\n\nかといって世に出ているGPSアプリは、それぞれの個人情報を大切にしたいのでやりたくないのですね。\n私自身、現在地を把握したいという欲求はありませんでした。\nただ推しが近づいてくるドキドキ感を味わいたいのです！！\n\n現状世の中を調べてみましたが、私が求めているサービスはありませんでした・・・😅\nなので！ないなら作るしかないですね！！\n大まかな距離で居場所がわかり、数字ではなく直感的に近づいてきていることが認識できて待ち合わせ時間が今までより楽しい体験になるものを作ろうと考えました！\n## 距離に応じて色が光るWEBアプリ爆誕！！\n\n色の変化をわかりやすくするため、動画にしました！ご覧ください↓\n※動画編集上人揺れが起きていますが、アプリ内では起こりません！\n\nhttps://x.com/riyu___11/status/1826442136440766845\n\n\n距離５００mごとに応じて色が変わっていきます。（最大４０００mまで）\n色は赤〜紫色の中でグラデーションになっています。\n\n２人の距離が、近いと赤色、遠いと紫色になっています。\n推しをイメージした人型の心臓吹きにあるハートの部分の色が変わっていきます！\n\n### 今回作ったもののイメージ図\n\n\u003cimg src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3720831/93e73ed9-8a0c-f016-80d9-9bcd3855ce82.png\" width=70%\u003e\n\n### 制作環境\n\n→ウェブアプリ開発プラットフォームです！！\n・**Netlify**：https://www.netlify.com/\n→静的サイトをホスティングできる\n・**Firebase**：https://firebase.google.com/?hl=ja\n→動的コンテンツの提供\n→リアルタイムにデータを同期・保存できるものも提供されている。\n\n## Netlify でGPS座標を取得するアプリを作る\n\n・Netlifyでログインし、アプリを作成する\n\n\u003cdetails\u003e\u003csummary\u003eアプリのコード\u003c/summary\u003e\n\n```　html\n\n\u003chtml\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003ctitle\u003eGPS\u003c/title\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width\" /\u003e\n    \u003cmeta name=\"description\" content=\"Displays the straight line distance to Station.\" /\u003e\n    \u003cscript src=\"https://geographiclib.sourceforge.io/scripts/geographiclib-geodesic.min.js\"\u003e\u003c/script\u003e\n    \u003cstyle\u003e\n      * {\n        margin: 0;\n        padding: 0;\n      }\n\n      body {\n        display: flex;\n        align-items: center;\n        justify-content: center;\n        color: #FAFAFA;\n        font-size: 24px;\n        font-weight: 900;\n        background: #FFC0CB; /* 背景色はピンク色 */\n      }\n\n      .display {\n        display: flex;\n        flex-direction: column;\n        align-items: center;\n      }\n\n      #distanceText {\n        font-size: 24px;\n        margin-bottom: 10px;\n      }\n\n      #m {\n        font-size: 40px;\n      }\n\n      #timestamp {\n        position: fixed;\n        bottom: 8px; right: 8px;\n        color: #0D47A1;\n        font-size: 12px;\n      }\n    \u003c/style\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003cdiv\u003e\n      \u003cdiv class=\"display\"\u003e\n        \u003cp id=\"distanceText\"\u003e2人の距離\u003c/p\u003e \u003c!-- 距離表示の上にテキストを追加 --\u003e\n        \u003cp id=\"m\"\u003e\u003c/p\u003e\n      \u003c/div\u003e\n      \u003cp id=\"timestamp\"\u003e\u003c/p\u003e\n    \u003c/div\u003e\n    \u003cscript\u003e\n      const goalLatLong = [,]; // 目標地点の緯度,経度\n\n      navigator.geolocation.watchPosition(handleWatchPosition, null, {\n        enableHighAccuracy: true,\n        timeout: 60000,\n        maximumAge: 0\n      });\n\n      function handleWatchPosition(evt) {\n        const latitude = evt.coords.latitude;\n        const longitude = evt.coords.longitude;\n        const accuracy = evt.coords.accuracy;\n        const altitude = evt.coords.altitude;\n        const altitudeAccuracy = evt.coords.altitudeAccuracy;\n        const heading = evt.coords.heading || 0;\n        const speed = evt.coords.speed;\n        const timestamp = evt.timestamp;\n\n        document.getElementById('timestamp').textContent = timestamp;\n\n        if (latitude \u0026\u0026 longitude) {\n          const geod = window.geodesic.Geodesic.WGS84;\n          const r = geod.Inverse(\n            latitude, longitude,\n            ...goalLatLong\n          );\n\n          document.getElementById('m').textContent = `${ r.s12.toFixed(2) }m`;\n        }\n      }\n    \u003c/script\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n```\n\u003c/details\u003e\n\n↓実際の画面\n\u003cimg src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3720831/a2dc0154-b3e9-b18a-15ed-636d591f103e.png\" width=70%\u003e\n\n・距離に応じて色の変化を緩やかなグラデーションにした画面\n（コードはChatGPTに生成してもらった）\n\n\u003cimg src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3720831/4bdfcf2d-3cb4-2e7a-e72e-dd4f9a91ec41.png\" width=70%\u003e\n\u003cimg src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3720831/7b50a4ae-eb54-951d-df76-bfff1e3ce110.png\" width=70%\u003e\n\n・人の形をimageとしてコードに挿入\n→人の画像はprocreateで描きました！\n　ハートの形は色が変わるところが見たいため、部分的に透過して画像を作っています。\n \u003cimg src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3720831/7899177c-4b92-2326-882b-e859b06e6aa9.png\" width=70%\u003e\n\n## FirebaseでGPSの座標を保存する（index.html）\n\n\u003cdetails\u003e\u003csummary\u003e最終コード\u003c/summary\u003e\n\n```　html\n\n\u003c!DOCTYPE html\u003e\n\u003chtml lang=\"ja\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"UTF-8\"\u003e\n    \u003ctitle\u003eGPS-based Color Gradient\u003c/title\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width\" /\u003e\n    \u003cmeta name=\"description\" content=\"Displays the straight line distance to the current location.\" /\u003e\n    \u003cscript src=\"https://cdn.jsdelivr.net/npm/geographiclib@1.52.2/geographiclib.min.js\"\u003e\u003c/script\u003e\n    \u003cscript defer src=\"path/to/firebase-app-compat.js\"\u003e\u003c/script\u003e\n    \u003cscript defer src=\"path/to/firebase-firestore-compat.js\"\u003e\u003c/script\u003e\n    \u003cscript defer src=\"path/to/init.js\"\u003e\u003c/script\u003e\n    \u003cstyle\u003e\n      * {\n        margin: 0;\n        padding: 0;\n      }\n\n      body {\n        display: flex;\n        align-items: center;\n        justify-content: center;\n        color: #000000;\n        font-size: 24px;\n        font-weight: 900;\n        background: hsl(0, 70%, 90%);\n        transition: background 0.5s ease;\n        height: 100vh;\n        text-align: center;\n      }\n\n      .display {\n        display: none;\n        flex-direction: column;\n        align-items: center;\n        justify-content: center;\n      }\n\n      #distanceText {\n        font-size: 24px;\n        margin-bottom: 10px;\n      }\n\n      #m {\n        font-size: 40px;\n      }\n\n      #timestamp {\n        position: fixed;\n        bottom: 8px;\n        right: 8px;\n        color: #000000;\n        font-size: 12px;\n        z-index: 1;\n      }\n\n      img {\n        position: absolute;\n        top: 50%;\n        left: 50%;\n        transform: translate(-50%, -50%);\n        height: 100vh;\n        width: 100vw;\n        object-fit: cover;\n        z-index: 0;\n      }\n    \u003c/style\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003cimg src=\"path/to/image.png\" alt=\"Character Image\"\u003e\n    \u003cdiv class=\"display\"\u003e\n      \u003cp id=\"distanceText\"\u003e２人の距離\u003c/p\u003e\n      \u003cp id=\"m\"\u003e\u003c/p\u003e\n    \u003c/div\u003e\n    \u003cp id=\"timestamp\"\u003e\u003c/p\u003e\n    \u003cscript type=\"module\"\u003e\n      const app = firebase.app();\n      const db = firebase.firestore();\n\n      (async () =\u003e {\n        const updateRef = db.doc('path/to/document');\n        const now = new Date();\n        await updateRef.set({\n          value: `${now.toLocaleString()}`\n        });\n        const { value } = (await updateRef.get()).data();\n\n        console.log(value);\n      })();\n\n      const goalLatLong = [,]; // Replace with target location's latitude, longitude\n      navigator.geolocation.watchPosition(handleWatchPosition, null, {\n        enableHighAccuracy: true,\n        timeout: 60000,\n        maximumAge: 0\n      });\n\n      function handleWatchPosition(evt) {\n        const latitude = evt.coords.latitude;\n        const longitude = evt.coords.longitude;\n        const timestamp = evt.timestamp;\n        document.getElementById('timestamp').textContent = new Date(timestamp).toLocaleString();\n        if (latitude \u0026\u0026 longitude) {\n          const geod = window.GeographicLib.Geodesic.WGS84;\n          const r = geod.Inverse(latitude, longitude, ...goalLatLong);\n          const distance = r.s12;\n          document.getElementById('m').textContent = `${distance.toFixed(2)}m`;\n\n          const maxDistance = 4000;\n          const hueIntervals = [0, 30, 60, 90, 120, 180, 210, 270];\n          let hue;\n          if (distance \u003c= 500) {\n            hue = interpolateHue(distance, 0, 500, hueIntervals[0], hueIntervals[1]);\n          } else if (distance \u003c= 1000) {\n            hue = interpolateHue(distance, 500, 1000, hueIntervals[1], hueIntervals[2]);\n          } else if (distance \u003c= 1500) {\n            hue = interpolateHue(distance, 1000, 1500, hueIntervals[2], hueIntervals[3]);\n          } else if (distance \u003c= 2000) {\n            hue = interpolateHue(distance, 1500, 2000, hueIntervals[3], hueIntervals[4]);\n          } else if (distance \u003c= 2500) {\n            hue = interpolateHue(distance, 2000, 2500, hueIntervals[4], hueIntervals[5]);\n          } else if (distance \u003c= 3000) {\n            hue = interpolateHue(distance, 2500, 3000, hueIntervals[5], hueIntervals[6]);\n          } else if (distance \u003c= 3500) {\n            hue = interpolateHue(distance, 3000, 3500, hueIntervals[6], hueIntervals[7]);\n          } else if (distance \u003c= 4000) {\n            hue = interpolateHue(distance, 3500, 4000, hueIntervals[7], hueIntervals[7]);\n          } else {\n            hue = hueIntervals[7];\n          }\n          const saturation = 70;\n          const lightness = 90;\n          document.body.style.backgroundColor = `hsl(${hue}, ${saturation}%, ${lightness}%)`;\n        }\n      }\n\n      function interpolateHue(distance, minDist, maxDist, startHue, endHue) {\n        const ratio = (distance - minDist) / (maxDist - minDist);\n        return startHue + ratio * (endHue - startHue);\n      }\n    \u003c/script\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n\n```\n\u003c/details\u003e\n\nターミナルで「firebase.deploy」をして値を取る\nCloud storeの中にGPSの値を取ることができる\n\n\u003cimg src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3720831/ccaec1d5-8440-ab13-2e37-eb57f54f2328.png\" width=70%\u003e\n\n## 完成したので使ってもらった\n・家族２名に使ってもらって感想を聞いてみました！\n　とてもよいFBをもらえたと思います。\n\n:::note info\n**WEBアプリの価値について**\n・日常にない体験ができるね！\n・アイデアが今まで見たことがなくて、面白そう。\n・好きな人が近づいてくることがわかると、ワクワクするね！\n・具体的な居場所が分からないので、個人情報も守られて良い。\n:::\n\n :::note info\n**WEBアプリの機能・UIについて**\n・顔が似てるって特別感があって良いね\n・５００mという距離の感覚と、色合いもちょうどいいよ\n:::\n\n:::note warn\n**WEBアプリの機能・UIについて**\n・常にアプリを開いている状態だったら、めちゃめちゃ困る。\n　タブを後ろに持っていくなど、画面の切り替えができたらいいな。\n・GPSをとることでバッテリーがたくさん使われる、その点が気になる\n:::\n\n### FBをうけて、私はどう思った？\n・日常にない体験で新しいアイデアだという言葉がとても嬉しかった！\n・GPSアプリを常に開いている状態になってしまうのは私も気になっていて、これから他に手段はないか模索していきたいと考えている。\n\n**その他まだまだ検証していくことがたくさんありそうだ！！**\n",
    "coediting": false,
    "comments_count": 0,
    "created_at": "2024-08-22T13:23:25+09:00",
    "group": null,
    "id": "fbe28ad9dabbc3dbb9e2",
    "likes_count": 0,
    "private": false,
    "reactions_count": 0,
    "stocks_count": 0,
    "tags": [
      {
        "name": "JavaScript",
        "versions": []
      },
      {
        "name": "GPS",
        "versions": []
      },
      {
        "name": "Webアプリケーション",
        "versions": []
      },
      {
        "name": "Firebase",
        "versions": []
      },
      {
        "name": "Netlify",
        "versions": []
      }
    ],
    "title": "待ち合わせ専用！距離で色が変わるアプリをNetlifyとFirebaseで作ってみた！",
    "updated_at": "2024-08-22T13:23:25+09:00",
    "url": "https://qiita.com/riyu_11/items/fbe28ad9dabbc3dbb9e2",
    "user": {
      "description": "児童福祉事業｜待ち合わせにときめきを！💓人形を通して２人の距離を見える化！近づくたびに色が変わりときめきを感じるアプリ【#ときメーター】を製作中👩‍💻",
      "facebook_id": "",
      "followees_count": 40,
      "followers_count": 4,
      "github_login_name": null,
      "id": "riyu_11",
      "items_count": 8,
      "linkedin_id": "",
      "location": "",
      "name": "",
      "organization": "",
      "permanent_id": 3720831,
      "profile_image_url": "https://lh3.googleusercontent.com/a/ACg8ocLdQtZDyUYP6Eurv2pw6SEgP5nynN1Tm0QvIDs1ggMrEpk=s96-c",
      "team_only": false,
      "twitter_screen_name": null,
      "website_url": ""
    },
    "page_views_count": null,
    "team_membership": null,
    "organization_url_name": "protoout-studio",
    "slide": false
  },
  {
    "rendered_body": "\u003ch3 data-sourcepos=\"1:1-1:70\"\u003e\n\u003cspan id=\"1-任意の名前でプロジェクトディレクトリを作成\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#1-%E4%BB%BB%E6%84%8F%E3%81%AE%E5%90%8D%E5%89%8D%E3%81%A7%E3%83%97%E3%83%AD%E3%82%B8%E3%82%A7%E3%82%AF%E3%83%88%E3%83%87%E3%82%A3%E3%83%AC%E3%82%AF%E3%83%88%E3%83%AA%E3%82%92%E4%BD%9C%E6%88%90\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e1. 任意の名前でプロジェクトディレクトリを作成\u003c/h3\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"2:1-4:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003emkdir [ プロジェクトディレクトリ名 ]\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003ch3 data-sourcepos=\"6:1-6:66\"\u003e\n\u003cspan id=\"2-作成したディレクトリ内に-packagejson-を作成\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#2-%E4%BD%9C%E6%88%90%E3%81%97%E3%81%9F%E3%83%87%E3%82%A3%E3%83%AC%E3%82%AF%E3%83%88%E3%83%AA%E5%86%85%E3%81%AB-packagejson-%E3%82%92%E4%BD%9C%E6%88%90\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e2. 作成したディレクトリ内に package.json を作成\u003c/h3\u003e\n\u003cp data-sourcepos=\"7:1-7:26\"\u003e↓ package.json の内容\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"8:1-16:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e{\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\"\n  }\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003ch3 data-sourcepos=\"18:1-18:86\"\u003e\n\u003cspan id=\"3-プロジェクトディレクトリ配下に-pages-ディレクトリを作成\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#3-%E3%83%97%E3%83%AD%E3%82%B8%E3%82%A7%E3%82%AF%E3%83%88%E3%83%87%E3%82%A3%E3%83%AC%E3%82%AF%E3%83%88%E3%83%AA%E9%85%8D%E4%B8%8B%E3%81%AB-pages-%E3%83%87%E3%82%A3%E3%83%AC%E3%82%AF%E3%83%88%E3%83%AA%E3%82%92%E4%BD%9C%E6%88%90\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e3. プロジェクトディレクトリ配下に pages ディレクトリを作成\u003c/h3\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"19:1-21:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003emkdir pages\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003ch3 data-sourcepos=\"23:1-23:76\"\u003e\n\u003cspan id=\"4-pages-ディレクトリ内にトップページファイルを作成\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#4-pages-%E3%83%87%E3%82%A3%E3%83%AC%E3%82%AF%E3%83%88%E3%83%AA%E5%86%85%E3%81%AB%E3%83%88%E3%83%83%E3%83%97%E3%83%9A%E3%83%BC%E3%82%B8%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%82%92%E4%BD%9C%E6%88%90\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e4. pages ディレクトリ内にトップページファイルを作成\u003c/h3\u003e\n\u003cp data-sourcepos=\"24:1-24:25\"\u003e↓ サンプルソース\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"25:1-30:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003eexport default () =\u0026gt; \u0026lt;div\u0026gt;\n\t\u0026lt;h1\u0026gt;Next.js\u0026lt;/h1\u0026gt;\n\t\u0026lt;div\u0026gt;Welcome to Next.js!\u0026lt;/div\u0026gt;\n\u0026lt;/div\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003ch3 data-sourcepos=\"32:1-32:46\"\u003e\n\u003cspan id=\"5-プロジェクトを実行して確認\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#5-%E3%83%97%E3%83%AD%E3%82%B8%E3%82%A7%E3%82%AF%E3%83%88%E3%82%92%E5%AE%9F%E8%A1%8C%E3%81%97%E3%81%A6%E7%A2%BA%E8%AA%8D\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e5. プロジェクトを実行して確認\u003c/h3\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"33:1-35:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003enpm run dev\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003ch3 data-sourcepos=\"37:1-37:28\"\u003e\n\u003cspan id=\"6-ブラウザで確認\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#6-%E3%83%96%E3%83%A9%E3%82%A6%E3%82%B6%E3%81%A7%E7%A2%BA%E8%AA%8D\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e6. ブラウザで確認\u003c/h3\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"38:1-40:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003ehttp://localhost:3000/\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003ch3 data-sourcepos=\"42:1-42:77\"\u003e\n\u003cspan id=\"7-プロジェクトディレクトリ配下に-nextconfigjs-を作成\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#7-%E3%83%97%E3%83%AD%E3%82%B8%E3%82%A7%E3%82%AF%E3%83%88%E3%83%87%E3%82%A3%E3%83%AC%E3%82%AF%E3%83%88%E3%83%AA%E9%85%8D%E4%B8%8B%E3%81%AB-nextconfigjs-%E3%82%92%E4%BD%9C%E6%88%90\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e7. プロジェクトディレクトリ配下に next.config.js を作成\u003c/h3\u003e\n\u003cp data-sourcepos=\"43:1-43:28\"\u003e↓ next.config.js の内容\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"44:1-53:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003emodule.exports = {\n  output: 'export',\n  exportPathMap: async function() {\n    return {\n      '/': { page: '/' },\n    };\n  },\n};\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003ch3 data-sourcepos=\"55:1-55:37\"\u003e\n\u003cspan id=\"8-プロジェクトをビルド\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#8-%E3%83%97%E3%83%AD%E3%82%B8%E3%82%A7%E3%82%AF%E3%83%88%E3%82%92%E3%83%93%E3%83%AB%E3%83%89\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e8. プロジェクトをビルド\u003c/h3\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"56:1-58:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003enpm run build\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003ch3 data-sourcepos=\"60:1-60:58\"\u003e\n\u003cspan id=\"9-ビルド後のファイルをブラウザで確認\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#9-%E3%83%93%E3%83%AB%E3%83%89%E5%BE%8C%E3%81%AE%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%82%92%E3%83%96%E3%83%A9%E3%82%A6%E3%82%B6%E3%81%A7%E7%A2%BA%E8%AA%8D\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e9. ビルド後のファイルをブラウザで確認\u003c/h3\u003e\n\u003cp data-sourcepos=\"61:1-63:69\"\u003eビルド完了後、プロジェクトディレクトリ内に\u003cbr\u003e\n「 out 」というディレクトリが生成されているので\u003cbr\u003e\nこの out ディレクトリを Webサーバ上に配置して確認\u003c/p\u003e\n",
    "body": "### 1. 任意の名前でプロジェクトディレクトリを作成\n```\nmkdir [ プロジェクトディレクトリ名 ]\n```\n\n### 2. 作成したディレクトリ内に package.json を作成\n↓ package.json の内容\n```\n{\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\"\n  }\n}\n```\n\n### 3. プロジェクトディレクトリ配下に pages ディレクトリを作成\n```\nmkdir pages\n```\n\n### 4. pages ディレクトリ内にトップページファイルを作成\n↓ サンプルソース\n```\nexport default () =\u003e \u003cdiv\u003e\n\t\u003ch1\u003eNext.js\u003c/h1\u003e\n\t\u003cdiv\u003eWelcome to Next.js!\u003c/div\u003e\n\u003c/div\u003e\n```\n\n### 5. プロジェクトを実行して確認\n```\nnpm run dev\n```\n\n### 6. ブラウザで確認\n```\nhttp://localhost:3000/\n```\n\n### 7. プロジェクトディレクトリ配下に next.config.js を作成\n↓ next.config.js の内容\n```\nmodule.exports = {\n  output: 'export',\n  exportPathMap: async function() {\n    return {\n      '/': { page: '/' },\n    };\n  },\n};\n```\n\n### 8. プロジェクトをビルド\n```\nnpm run build\n```\n\n### 9. ビルド後のファイルをブラウザで確認\nビルド完了後、プロジェクトディレクトリ内に\n「 out 」というディレクトリが生成されているので\nこの out ディレクトリを Webサーバ上に配置して確認\n",
    "coediting": false,
    "comments_count": 0,
    "created_at": "2024-08-22T13:22:48+09:00",
    "group": null,
    "id": "9304b8aba7caf6a154ed",
    "likes_count": 0,
    "private": false,
    "reactions_count": 0,
    "stocks_count": 0,
    "tags": [
      {
        "name": "Next.js",
        "versions": []
      },
      {
        "name": "Webフロントエンド開発",
        "versions": []
      }
    ],
    "title": "Next.js プロジェクト作成",
    "updated_at": "2024-08-22T13:22:48+09:00",
    "url": "https://qiita.com/kenji-takahashi-app/items/9304b8aba7caf6a154ed",
    "user": {
      "description": "",
      "facebook_id": "",
      "followees_count": 0,
      "followers_count": 0,
      "github_login_name": "kenji-takahashi-app",
      "id": "kenji-takahashi-app",
      "items_count": 2,
      "linkedin_id": "",
      "location": "Tokyo, Japan",
      "name": "Kenji Takahashi",
      "organization": "",
      "permanent_id": 167550,
      "profile_image_url": "https://avatars.githubusercontent.com/u/26020900?v=3",
      "team_only": false,
      "twitter_screen_name": null,
      "website_url": ""
    },
    "page_views_count": null,
    "team_membership": null,
    "organization_url_name": null,
    "slide": false
  },
  {
    "rendered_body": "\u003ch1 data-sourcepos=\"1:1-1:14\"\u003e\n\u003cspan id=\"はじめに\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eはじめに\u003c/h1\u003e\n\u003cp data-sourcepos=\"3:1-3:127\"\u003e生成AIを用いてSE blockの論文「Squeeze-and-Excitation Networks」の内容を(なるべく)把握してみました。\u003c/p\u003e\n\u003cp data-sourcepos=\"5:1-5:61\"\u003e以降で、ChatGPTに聞いてみた例を記載します。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"7:1-7:18\"\u003e\n\u003cspan id=\"同類の記事\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E5%90%8C%E9%A1%9E%E3%81%AE%E8%A8%98%E4%BA%8B\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e同類の記事\u003c/h2\u003e\n\u003cp data-sourcepos=\"9:1-9:60\"\u003e\u003ciframe id=\"qiita-embed-content__07f6b23319d7c66171e3ffbecb7d4edd\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__07f6b23319d7c66171e3ffbecb7d4edd\" data-content=\"https%3A%2F%2Fqiita.com%2Fdl_from_scratch%2Fitems%2Ff6e6a27fc4752e28dd5a\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\"\u003e\n\u003c/iframe\u003e\n\u003c/p\u003e\n\u003cp data-sourcepos=\"11:1-11:60\"\u003e\u003ciframe id=\"qiita-embed-content__e084272af8147d142084233c5bed3594\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__e084272af8147d142084233c5bed3594\" data-content=\"https%3A%2F%2Fqiita.com%2Fdl_from_scratch%2Fitems%2F42196ffd7740ed44209f\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\"\u003e\n\u003c/iframe\u003e\n\u003c/p\u003e\n\u003ch2 data-sourcepos=\"13:1-13:18\"\u003e\n\u003cspan id=\"対象の論文\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E5%AF%BE%E8%B1%A1%E3%81%AE%E8%AB%96%E6%96%87\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e対象の論文\u003c/h2\u003e\n\u003cp data-sourcepos=\"15:1-15:36\"\u003e論文: (SE blockに関する論文)\u003c/p\u003e\n\u003cp data-sourcepos=\"17:1-19:41\"\u003e[1709.01507] Squeeze-and-Excitation Networks\u003cbr\u003e\n\u003ca href=\"https://arxiv.org/abs/1709.01507\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://arxiv.org/abs/1709.01507\u003c/a\u003e\u003cbr\u003e\n(PDF: \u003ca href=\"https://arxiv.org/pdf/1709.01507v3\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://arxiv.org/pdf/1709.01507v3\u003c/a\u003e)\u003c/p\u003e\n\u003ch2 data-sourcepos=\"21:1-21:42\"\u003e\n\u003cspan id=\"質問時の各章節の区切り部分\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E8%B3%AA%E5%95%8F%E6%99%82%E3%81%AE%E5%90%84%E7%AB%A0%E7%AF%80%E3%81%AE%E5%8C%BA%E5%88%87%E3%82%8A%E9%83%A8%E5%88%86\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e質問時の各章節の区切り部分\u003c/h2\u003e\n\u003cp data-sourcepos=\"23:1-23:128\"\u003e論文の中にある各章節を、下記のように区切って、部分毎に生成AIに内容を質問していきます。\u003c/p\u003e\n\u003cul data-sourcepos=\"25:1-65:0\"\u003e\n\u003cli data-sourcepos=\"25:1-25:10\"\u003eAbstract\u003c/li\u003e\n\u003cli data-sourcepos=\"26:1-26:6\"\u003e---\u003c/li\u003e\n\u003cli data-sourcepos=\"27:1-27:16\"\u003e1 INTRODUCTION\u003c/li\u003e\n\u003cli data-sourcepos=\"28:1-28:6\"\u003e---\u003c/li\u003e\n\u003cli data-sourcepos=\"29:1-29:16\"\u003e2 RELATED WORK\u003c/li\u003e\n\u003cli data-sourcepos=\"30:1-30:6\"\u003e---\u003c/li\u003e\n\u003cli data-sourcepos=\"31:1-31:33\"\u003e3 SQUEEZE-AND-EXCITATION BLOCKS\u003c/li\u003e\n\u003cli data-sourcepos=\"32:1-32:6\"\u003e---\u003c/li\u003e\n\u003cli data-sourcepos=\"33:1-33:43\"\u003e3.1 Squeeze: Global Information Embedding\u003c/li\u003e\n\u003cli data-sourcepos=\"34:1-34:6\"\u003e---\u003c/li\u003e\n\u003cli data-sourcepos=\"35:1-35:40\"\u003e3.2 Excitation: Adaptive Recalibration\u003c/li\u003e\n\u003cli data-sourcepos=\"36:1-36:6\"\u003e---\u003c/li\u003e\n\u003cli data-sourcepos=\"37:1-37:20\"\u003e3.3 Instantiations\u003c/li\u003e\n\u003cli data-sourcepos=\"38:1-38:6\"\u003e---\u003c/li\u003e\n\u003cli data-sourcepos=\"39:1-39:38\"\u003e4 MODEL AND COMPUTATIONAL COMPLEXITY\u003c/li\u003e\n\u003cli data-sourcepos=\"40:1-40:6\"\u003e---\u003c/li\u003e\n\u003cli data-sourcepos=\"41:1-41:15\"\u003e5 EXPERIMENTS\u003c/li\u003e\n\u003cli data-sourcepos=\"42:1-42:26\"\u003e5.1 Image Classification\u003c/li\u003e\n\u003cli data-sourcepos=\"43:1-43:6\"\u003e---\u003c/li\u003e\n\u003cli data-sourcepos=\"44:1-44:26\"\u003e5.2 Scene Classification\u003c/li\u003e\n\u003cli data-sourcepos=\"45:1-45:30\"\u003e5.3 Object Detection on COCO\u003c/li\u003e\n\u003cli data-sourcepos=\"46:1-46:44\"\u003e5.4 ILSVRC 2017 Classification Competition\u003c/li\u003e\n\u003cli data-sourcepos=\"47:1-47:6\"\u003e---\u003c/li\u003e\n\u003cli data-sourcepos=\"48:1-48:18\"\u003e6 ABLATION STUDY\u003c/li\u003e\n\u003cli data-sourcepos=\"49:1-49:21\"\u003e6.1 Reduction ratio\u003c/li\u003e\n\u003cli data-sourcepos=\"50:1-50:6\"\u003e---\u003c/li\u003e\n\u003cli data-sourcepos=\"51:1-51:22\"\u003e6.2 Squeeze Operator\u003c/li\u003e\n\u003cli data-sourcepos=\"52:1-52:25\"\u003e6.3 Excitation Operator\u003c/li\u003e\n\u003cli data-sourcepos=\"53:1-53:22\"\u003e6.4 Different stages\u003c/li\u003e\n\u003cli data-sourcepos=\"54:1-54:6\"\u003e---\u003c/li\u003e\n\u003cli data-sourcepos=\"55:1-55:26\"\u003e6.5 Integration strategy\u003c/li\u003e\n\u003cli data-sourcepos=\"56:1-56:6\"\u003e---\u003c/li\u003e\n\u003cli data-sourcepos=\"57:1-57:21\"\u003e7 ROLE OF SE BLOCKS\u003c/li\u003e\n\u003cli data-sourcepos=\"58:1-58:23\"\u003e7.1 Effect of Squeeze\u003c/li\u003e\n\u003cli data-sourcepos=\"59:1-59:6\"\u003e---\u003c/li\u003e\n\u003cli data-sourcepos=\"60:1-60:24\"\u003e7.2 Role of Excitation\u003c/li\u003e\n\u003cli data-sourcepos=\"61:1-61:6\"\u003e---\u003c/li\u003e\n\u003cli data-sourcepos=\"62:1-62:14\"\u003e8 CONCLUSION\u003c/li\u003e\n\u003cli data-sourcepos=\"63:1-63:6\"\u003e---\u003c/li\u003e\n\u003cli data-sourcepos=\"64:1-65:0\"\u003eAPPENDIX: DETAILS OF SENET-154\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 data-sourcepos=\"66:1-66:29\"\u003e\n\u003cspan id=\"生成aiへの質問方法\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E7%94%9F%E6%88%90ai%E3%81%B8%E3%81%AE%E8%B3%AA%E5%95%8F%E6%96%B9%E6%B3%95\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e生成AIへの質問方法\u003c/h2\u003e\n\u003cp data-sourcepos=\"68:1-68:415\"\u003e生成AIを活用して、知りたい記事・論文の1節分(適度な長さ)のテキストをコピー＆ペーストして、その下に\u003cstrong\u003e質問内容を「①～ ②～ …」と番号付きで書いて\u003c/strong\u003e、生成AIに渡せば、全質問に一発で的確に回答してくれるので、非常に良好でした。記事全体を読む必要なく、知りたい点の情報だけを収集できます。\u003c/p\u003e\n\u003cp data-sourcepos=\"70:1-70:28\"\u003e\u003cstrong\u003e生成AIへの質問例:\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"71:1-81:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e(論文・記事の各章節を貼り付け)\n\n上記の内容に関して質問: (である調で記載)\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\n⑤比較の記載があれば違いを**表で**まとめて下さい。(無ければ無しと記載)\n⑥上記⑤以外で表に出来そうな部分があれば**表で**まとめて下さい。(無ければ無しと記載)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"83:1-83:323\"\u003e質問内容は、記事・論文を読んでいて\u003cstrong\u003eいつも知りたいと思う点\u003c/strong\u003e(\u003cstrong\u003e改良点・工夫点・テクニック・メカニズムの解明\u003c/strong\u003e)にしています。また、表で比較した方が素早く把握できるので、可能であれば\u003cstrong\u003e記事を表に変換する\u003c/strong\u003eようにしています。\u003c/p\u003e\n\u003cp data-sourcepos=\"85:1-85:380\"\u003e論文・記事を貼り付けるテキストの長さは、\u003cstrong\u003e1節分程度の量に\u003c/strong\u003eとどめた方が、良い回答が得られました。生成AIの回答の文量が多くなってくると、回答が長くなり過ぎないように、生成AIが勝手に(適度に)端折り始めてしまい、重要な点が回答から抜けてしまう可能性が高まります。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"87:1-87:12\"\u003e\n\u003cspan id=\"注意点\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E6%B3%A8%E6%84%8F%E7%82%B9\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e注意点\u003c/h2\u003e\n\u003cp data-sourcepos=\"89:1-89:259\"\u003e論文のテキスト内容だけを貼り付けて、生成AIに質問しています。論文の中の図・表の部分は貼り付けていません。図・表の内容は生成AIの回答には含まれず、別途論文を見る必要があります。\u003c/p\u003e\n\u003ch1 data-sourcepos=\"91:1-91:19\"\u003e\n\u003cspan id=\"生成aiに質問\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E7%94%9F%E6%88%90ai%E3%81%AB%E8%B3%AA%E5%95%8F\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e生成AIに質問\u003c/h1\u003e\n\u003cp data-sourcepos=\"93:1-93:70\"\u003e以降で、ChatGPTに実際に聞いてみた例を記載します。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"95:1-95:41\"\u003e\n\u003cspan id=\"生成aiへの質問回答の全容\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E7%94%9F%E6%88%90ai%E3%81%B8%E3%81%AE%E8%B3%AA%E5%95%8F%E5%9B%9E%E7%AD%94%E3%81%AE%E5%85%A8%E5%AE%B9\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e生成AIへの質問＆回答の全容\u003c/h2\u003e\n\u003cp data-sourcepos=\"97:1-97:48\"\u003e生成AIへの質問＆回答の全容・詳細:\u003c/p\u003e\n\u003cp data-sourcepos=\"99:1-99:62\"\u003e\u003ciframe id=\"qiita-embed-content__3305548e01474f441472a8f82b931918\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__3305548e01474f441472a8f82b931918\" data-content=\"https%3A%2F%2Fchatgpt.com%2Fshare%2F77aea68d-52c6-499a-a07b-760431628034\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\"\u003e\n\u003c/iframe\u003e\n\u003c/p\u003e\n\u003ch2 data-sourcepos=\"101:1-101:11\"\u003e\n\u003cspan id=\"abstract\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#abstract\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eAbstract\u003c/h2\u003e\n\u003cp data-sourcepos=\"103:1-114:108\"\u003e\u003cstrong\u003e▼質問(人間)\u003c/strong\u003e\u003cbr\u003e\n(論文・記事の各章節を貼り付け)\u003cbr\u003e\nAbstract—The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to\u003cbr\u003e\nconstruct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad\u003cbr\u003e\nrange of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of\u003cbr\u003e\na CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel\u003cbr\u003e\nrelationship and propose a novel architectural unit, which we term the “Squeeze-and-Excitation” (SE) block, that adaptively recalibrates\u003cbr\u003e\nchannel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be\u003cbr\u003e\nstacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate\u003cbr\u003e\nthat SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at minimal additional computational\u003cbr\u003e\ncost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and\u003cbr\u003e\nreduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ∼25%.\u003c/p\u003e\n\u003cp data-sourcepos=\"116:1-122:123\"\u003e上記の内容に関して質問: (である調で記載)\u003cbr\u003e\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\u003cbr\u003e\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\u003cbr\u003e\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n⑤比較の記載があれば違いを\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003cbr\u003e\n⑥上記⑤以外で表に出来そうな部分があれば\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003c/p\u003e\n\u003cp data-sourcepos=\"124:1-126:254\"\u003e\u003cstrong\u003e▼生成AIの回答(ChatGPT-4o)\u003c/strong\u003e\u003cbr\u003e\n① \u003cstrong\u003e何についての記載か\u003c/strong\u003e\u003cbr\u003e\nこの論文は、畳み込みニューラルネットワーク (CNN) のチャネル間の依存関係を明示的にモデル化する新しいアーキテクチャユニット「Squeeze-and-Excitation (SE) ブロック」について述べている。\u003c/p\u003e\n\u003cp data-sourcepos=\"128:1-128:10\"\u003e\u003cstrong\u003e要旨\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"129:1-133:0\"\u003e\n\u003cli data-sourcepos=\"129:1-129:82\"\u003eCNNのチャネル間の依存関係を再校正するSEブロックを提案。\u003c/li\u003e\n\u003cli data-sourcepos=\"130:1-130:91\"\u003eSEブロックは、CNNの表現力を向上させるための新しい手法である。\u003c/li\u003e\n\u003cli data-sourcepos=\"131:1-131:136\"\u003eSEブロックは、既存の最先端CNNにおいて、わずかな追加の計算コストで大幅な性能向上をもたらす。\u003c/li\u003e\n\u003cli data-sourcepos=\"132:1-133:0\"\u003eILSVRC 2017分類コンペで1位を獲得し、top-5エラー率を2.251%に削減。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"134:1-135:335\"\u003e② \u003cstrong\u003e改良点・工夫点・テクニック等の記載\u003c/strong\u003e\u003cbr\u003e\nこの論文では、チャネル間の依存関係を明示的にモデル化し、チャネルごとの特徴応答を適応的に再校正するSEブロックを提案している。これは、従来のCNNが空間的な情報に重点を置いていたのに対し、チャネル情報の重要性を強調するものである。\u003c/p\u003e\n\u003cp data-sourcepos=\"137:1-138:199\"\u003e③ \u003cstrong\u003e性能が向上した記載\u003c/strong\u003e\u003cbr\u003e\nILSVRC 2017分類コンペで、提案されたSENetアーキテクチャはtop-5エラー率を2.251%にまで削減し、2016年の勝利エントリを相対的に約25%上回ったとされる。\u003c/p\u003e\n\u003cp data-sourcepos=\"140:1-141:6\"\u003e④ \u003cstrong\u003eメカニズムの解明・なぜそうなるのか等の記載\u003c/strong\u003e\u003cbr\u003e\n無し\u003c/p\u003e\n\u003cp data-sourcepos=\"143:1-144:6\"\u003e⑤ \u003cstrong\u003e比較の記載\u003c/strong\u003e\u003cbr\u003e\n無し\u003c/p\u003e\n\u003cp data-sourcepos=\"146:1-147:6\"\u003e⑥ \u003cstrong\u003e上記⑤以外で表に出来そうな部分\u003c/strong\u003e\u003cbr\u003e\n無し\u003c/p\u003e\n\u003ch2 data-sourcepos=\"149:1-149:17\"\u003e\n\u003cspan id=\"1-introduction\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#1-introduction\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e1 INTRODUCTION\u003c/h2\u003e\n\u003cp data-sourcepos=\"151:1-236:8\"\u003e\u003cstrong\u003e▼質問(人間)\u003c/strong\u003e\u003cbr\u003e\n(論文・記事の各章節を貼り付け)\u003cbr\u003e\n1 INTRODUCTION\u003cbr\u003e\nCONVOLUTIONAL neural networks (CNNs) have proven\u003cbr\u003e\nto be useful models for tackling a wide range of\u003cbr\u003e\nvisual tasks [1]–[4]. At each convolutional layer in the\u003cbr\u003e\nnetwork, a collection of filters expresses neighbourhood\u003cbr\u003e\nspatial connectivity patterns along input channels—fusing\u003cbr\u003e\nspatial and channel-wise information together within local\u003cbr\u003e\nreceptive fields. By interleaving a series of convolutional\u003cbr\u003e\nlayers with non-linear activation functions and downsam-\u003cbr\u003e\npling operators, CNNs are able to produce robust represen-\u003cbr\u003e\ntations that capture hierarchical patterns and attain global\u003cbr\u003e\ntheoretical receptive fields. Recent research has shown that\u003cbr\u003e\nthese representations can be strengthened by integrating\u003cbr\u003e\nlearning mechanisms into the network that help capture\u003cbr\u003e\nspatial correlations between features. One such approach,\u003cbr\u003e\npopularised by the Inception family of architectures [5], [6],\u003cbr\u003e\nincorporates multi-scale processes into network modules to\u003cbr\u003e\nachieve improved performance. Further work has sought to\u003cbr\u003e\nbetter model spatial dependencies [7], [8] and incorporate\u003cbr\u003e\nspatial attention into the structure of the network [9].\u003cbr\u003e\nIn this paper, we investigate a different aspect of network\u003cbr\u003e\ndesign - the relationship between channels. We introduce\u003cbr\u003e\na new architectural unit, which we term the Squeeze-and-\u003cbr\u003e\nExcitation (SE) block, with the goal of improving the quality\u003cbr\u003e\nof representations produced by a network by explicitly\u003cbr\u003e\nmodelling the interdependencies between the channels of\u003cbr\u003e\nits convolutional features. To this end, we propose a mecha-\u003cbr\u003e\nnism that allows the network to perform feature recalibration,\u003cbr\u003e\nthrough which it can learn to use global information to\u003cbr\u003e\nselectively emphasise informative features and suppress less\u003cbr\u003e\nuseful ones.\u003cbr\u003e\nThe structure of the SE building block is depicted in\u003cbr\u003e\nFig. 1. For any given transformation Ftr : X 7 → U,\u003cbr\u003e\nX ∈ RH′×W ′×C′\u003cbr\u003e\n, U ∈ RH×W ×C , (e.g. a convolution), we\u003cbr\u003e\ncan construct a corresponding SE block to perform feature\u003cbr\u003e\nrecalibration. The features U are first passed through a\u003cbr\u003e\nsqueeze operation, which produces a channel descriptor by\u003cbr\u003e\naggregating feature maps across their spatial dimensions\u003cbr\u003e\n(H × W ). The function of this descriptor is to produce\u003cbr\u003e\nan embedding of the global distribution of channel-wise\u003cbr\u003e\nfeature responses, allowing information from the global\u003cbr\u003e\nreceptive field of the network to be used by all its layers. The\u003cbr\u003e\naggregation is followed by an excitation operation, which\u003cbr\u003e\ntakes the form of a simple self-gating mechanism that takes\u003cbr\u003e\nthe embedding as input and produces a collection of per-\u003cbr\u003e\nchannel modulation weights. These weights are applied to\u003cbr\u003e\nthe feature maps U to generate the output of the SE block\u003cbr\u003e\nwhich can be fed directly into subsequent layers of the\u003cbr\u003e\nnetwork.\u003cbr\u003e\nIt is possible to construct an SE network (SENet) by\u003cbr\u003e\nsimply stacking a collection of SE blocks. Moreover, these\u003cbr\u003e\nSE blocks can also be used as a drop-in replacement for\u003cbr\u003e\nthe original block at a range of depths in the network\u003cbr\u003e\narchitecture (Sec. 6.4). While the template for the building\u003cbr\u003e\nblock is generic, the role it performs at different depths\u003cbr\u003e\ndiffers throughout the network. In earlier layers, it excites\u003cbr\u003e\ninformative features in a class-agnostic manner, strengthen-\u003cbr\u003e\ning the shared low-level representations. In later layers, the\u003cbr\u003e\nSE blocks become increasingly specialised, and respond to\u003cbr\u003e\ndifferent inputs in a highly class-specific manner (Sec. 7.2).\u003cbr\u003e\nAs a consequence, the benefits of the feature recalibration\u003cbr\u003e\nperformed by SE blocks can be accumulated through the\u003cbr\u003e\nnetwork.\u003cbr\u003e\nThe design and development of new CNN architectures\u003cbr\u003e\nis a difficult engineering task, typically requiring the se-\u003cbr\u003e\nlection of many new hyperparameters and layer configura-\u003cbr\u003e\ntions. By contrast, the structure of the SE block is simple and\u003cbr\u003e\ncan be used directly in existing state-of-the-art architectures\u003cbr\u003e\nby replacing components with their SE counterparts, where\u003cbr\u003e\nthe performance can be effectively enhanced. SE blocks are\u003cbr\u003e\nalso computationally lightweight and impose only a slight\u003cbr\u003e\nincrease in model complexity and computational burden.\u003cbr\u003e\nTo provide evidence for these claims, in Sec. 4 we de-\u003cbr\u003e\nvelop several SENets and conduct an extensive evaluation\u003cbr\u003e\non the ImageNet 2012 dataset [10]. We also present results\u003cbr\u003e\nbeyond ImageNet that indicate that the benefits of our\u003cbr\u003e\napproach are not restricted to a specific dataset or task.\u003cbr\u003e\nBy making use of SENets, we ranked first in the ILSVRC\u003cbr\u003e\n2017 classification competition. Our best model ensemble\u003cbr\u003e\nachieves a 2.251% top-5 error on the test set1. This repre-\u003cbr\u003e\nsents roughly a 25% relative improvement when compared\u003cbr\u003e\nto the winner entry of the previous year (top-5 error of\u003cbr\u003e\n2.991%).\u003c/p\u003e\n\u003cp data-sourcepos=\"238:1-244:123\"\u003e上記の内容に関して質問: (である調で記載)\u003cbr\u003e\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\u003cbr\u003e\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\u003cbr\u003e\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n⑤比較の記載があれば違いを\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003cbr\u003e\n⑥上記⑤以外で表に出来そうな部分があれば\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003c/p\u003e\n\u003cp data-sourcepos=\"246:1-248:265\"\u003e\u003cstrong\u003e▼生成AIの回答(ChatGPT-4o)\u003c/strong\u003e\u003cbr\u003e\n① \u003cstrong\u003e何についての記載か\u003c/strong\u003e\u003cbr\u003e\nこの論文は、畳み込みニューラルネットワーク（CNN）のチャネル間の依存関係をモデル化し、ネットワークが生成する表現の質を向上させる「Squeeze-and-Excitation (SE) ブロック」について述べている。\u003c/p\u003e\n\u003cp data-sourcepos=\"250:1-250:10\"\u003e\u003cstrong\u003e要旨\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"251:1-255:0\"\u003e\n\u003cli data-sourcepos=\"251:1-251:175\"\u003eCNNのチャネル間の依存関係を明示的にモデル化することで、表現の質を向上させる新しいアーキテクチャ「SEブロック」を提案。\u003c/li\u003e\n\u003cli data-sourcepos=\"252:1-252:163\"\u003eSEブロックは、グローバルな情報を活用して有用な特徴を強調し、あまり有用でない特徴を抑制するメカニズムを持つ。\u003c/li\u003e\n\u003cli data-sourcepos=\"253:1-253:115\"\u003eSEブロックは既存の最先端アーキテクチャに直接適用可能で、性能向上をもたらす。\u003c/li\u003e\n\u003cli data-sourcepos=\"254:1-255:0\"\u003eSEブロックを使用したSENetは、ILSVRC 2017分類コンペで1位を獲得し、top-5エラー率を2.251%にまで削減。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"256:1-257:536\"\u003e② \u003cstrong\u003e改良点・工夫点・テクニック等の記載\u003c/strong\u003e\u003cbr\u003e\nこの論文では、CNNのチャネル間の依存関係を明示的にモデル化する「Squeeze-and-Excitation (SE) ブロック」を提案している。SEブロックは、グローバルなチャネル特徴の分布を埋め込むための「squeeze」操作と、それに基づいてチャネルごとの重みを生成する「excitation」操作の2つのステップで構成される。また、SEブロックは計算コストが軽く、既存のアーキテクチャに容易に統合できる点が特徴である。\u003c/p\u003e\n\u003cp data-sourcepos=\"259:1-260:207\"\u003e③ \u003cstrong\u003e性能が向上した記載\u003c/strong\u003e\u003cbr\u003e\nILSVRC 2017分類コンペで、提案されたSENetアーキテクチャはtop-5エラー率を2.251%に削減し、前年度の勝利エントリ（2.991%）を約25%上回る性能向上を達成した。\u003c/p\u003e\n\u003cp data-sourcepos=\"262:1-263:188\"\u003e④ \u003cstrong\u003eメカニズムの解明・なぜそうなるのか等の記載\u003c/strong\u003e\u003cbr\u003e\nSEブロックは、チャネル間の依存関係を再校正することで、ネットワークが有用な特徴を強調し、不要な特徴を抑制する能力を向上させる。\u003c/p\u003e\n\u003cp data-sourcepos=\"265:1-266:6\"\u003e⑤ \u003cstrong\u003e比較の記載\u003c/strong\u003e\u003cbr\u003e\n無し\u003c/p\u003e\n\u003cp data-sourcepos=\"268:1-269:6\"\u003e⑥ \u003cstrong\u003e上記⑤以外で表に出来そうな部分\u003c/strong\u003e\u003cbr\u003e\n無し\u003c/p\u003e\n\u003ch2 data-sourcepos=\"271:1-271:17\"\u003e\n\u003cspan id=\"2-related-work\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#2-related-work\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e2 RELATED WORK\u003c/h2\u003e\n\u003cp data-sourcepos=\"273:1-360:30\"\u003e\u003cstrong\u003e▼質問(人間)\u003c/strong\u003e\u003cbr\u003e\n(論文・記事の各章節を貼り付け)\u003cbr\u003e\n2 RELATED WORK\u003cbr\u003e\nDeeper architectures. VGGNets [11] and Inception mod-\u003cbr\u003e\nels [5] showed that increasing the depth of a network could\u003cbr\u003e\nsignificantly increase the quality of representations that\u003cbr\u003e\nit was capable of learning. By regulating the distribution\u003cbr\u003e\nof the inputs to each layer, Batch Normalization (BN) [6]\u003cbr\u003e\nadded stability to the learning process in deep networks\u003cbr\u003e\nand produced smoother optimisation surfaces [12]. Building\u003cbr\u003e\non these works, ResNets demonstrated that it was pos-\u003cbr\u003e\nsible to learn considerably deeper and stronger networks\u003cbr\u003e\nthrough the use of identity-based skip connections [13], [14].\u003cbr\u003e\nHighway networks [15] introduced a gating mechanism to\u003cbr\u003e\nregulate the flow of information along shortcut connections.\u003cbr\u003e\nFollowing these works, there have been further reformula-\u003cbr\u003e\ntions of the connections between network layers [16], [17],\u003cbr\u003e\nwhich show promising improvements to the learning and\u003cbr\u003e\nrepresentational properties of deep networks.\u003cbr\u003e\nAn alternative, but closely related line of research has\u003cbr\u003e\nfocused on methods to improve the functional form of\u003cbr\u003e\nthe computational elements contained within a network.\u003cbr\u003e\nGrouped convolutions have proven to be a popular ap-\u003cbr\u003e\nproach for increasing the cardinality of learned transforma-\u003cbr\u003e\ntions [18], [19]. More flexible compositions of operators can\u003cbr\u003e\nbe achieved with multi-branch convolutions [5], [6], [20],\u003cbr\u003e\n[21], which can be viewed as a natural extension of the\u003cbr\u003e\ngrouping operator. In prior work, cross-channel correlations\u003cbr\u003e\nare typically mapped as new combinations of features, ei-\u003cbr\u003e\nther independently of spatial structure [22], [23] or jointly\u003cbr\u003e\nby using standard convolutional filters [24] with 1 × 1\u003cbr\u003e\nconvolutions. Much of this research has concentrated on the\u003cbr\u003e\nobjective of reducing model and computational complexity,\u003cbr\u003e\nreflecting an assumption that channel relationships can be\u003cbr\u003e\nformulated as a composition of instance-agnostic functions\u003cbr\u003e\nwith local receptive fields. In contrast, we claim that provid-\u003cbr\u003e\ning the unit with a mechanism to explicitly model dynamic,\u003cbr\u003e\nnon-linear dependencies between channels using global in-\u003cbr\u003e\nformation can ease the learning process, and significantly\u003cbr\u003e\nenhance the representational power of the network.\u003cbr\u003e\nAlgorithmic Architecture Search. Alongside the works\u003cbr\u003e\ndescribed above, there is also a rich history of research\u003cbr\u003e\nthat aims to forgo manual architecture design and instead\u003cbr\u003e\nseeks to learn the structure of the network automatically.\u003cbr\u003e\nMuch of the early work in this domain was conducted in\u003cbr\u003e\nthe neuro-evolution community, which established methods\u003cbr\u003e\nfor searching across network topologies with evolutionary\u003cbr\u003e\nmethods [25], [26]. While often computationally demand-\u003cbr\u003e\ning, evolutionary search has had notable successes which\u003cbr\u003e\ninclude finding good memory cells for sequence models\u003cbr\u003e\n[27], [28] and learning sophisticated architectures for large-\u003cbr\u003e\nscale image classification [29]–[31]. With the goal of reduc-\u003cbr\u003e\ning the computational burden of these methods, efficient\u003cbr\u003e\nalternatives to this approach have been proposed based on\u003cbr\u003e\nLamarckian inheritance [32] and differentiable architecture\u003cbr\u003e\nsearch [33].\u003cbr\u003e\nBy formulating architecture search as hyperparameter\u003cbr\u003e\noptimisation, random search [34] and other more sophis-\u003cbr\u003e\nticated model-based optimisation techniques [35], [36] can\u003cbr\u003e\nalso be used to tackle the problem. Topology selection\u003cbr\u003e\nas a path through a fabric of possible designs [37] and\u003cbr\u003e\ndirect architecture prediction [38], [39] have been proposed\u003cbr\u003e\nas additional viable architecture search tools. Particularly\u003cbr\u003e\nstrong results have been achieved with techniques from\u003cbr\u003e\nreinforcement learning [40]–[44]. SE blocks can be used as\u003cbr\u003e\natomic building blocks for these search algorithms, and\u003cbr\u003e\nwere demonstrated to be highly effective in this capacity\u003cbr\u003e\nin concurrent work [45].\u003cbr\u003e\nAttention and gating mechanisms. Attention can be in-\u003cbr\u003e\nterpreted as a means of biasing the allocation of available\u003cbr\u003e\ncomputational resources towards the most informative com-\u003cbr\u003e\nponents of a signal [46]–[51]. Attention mechanisms have\u003cbr\u003e\ndemonstrated their utility across many tasks including se-\u003cbr\u003e\nquence learning [52], [53], localisation and understanding in\u003cbr\u003e\nimages [9], [54], image captioning [55], [56] and lip reading\u003cbr\u003e\n[57]. In these applications, it can be incorporated as an oper-\u003cbr\u003e\nator following one or more layers representing higher-level\u003cbr\u003e\nabstractions for adaptation between modalities. Concurrent\u003cbr\u003e\nwork provides an interesting study into the combined use of\u003cbr\u003e\nspatial and channel attention around convolutional [58] and\u003cbr\u003e\nbottleneck units [59]. Wang et al. [60] introduced a powerful\u003cbr\u003e\ntrunk-and-mask attention mechanism based on hourglass\u003cbr\u003e\nmodules [8] that is inserted between the intermediate stages\u003cbr\u003e\nof deep residual networks. By contrast, our proposed SE\u003cbr\u003e\nblock comprises a lightweight gating mechanism which\u003cbr\u003e\nfocuses on enhancing the representational power of the\u003cbr\u003e\nnetwork by modelling channel-wise relationships in a com-\u003cbr\u003e\nputationally efficient manner.\u003c/p\u003e\n\u003cp data-sourcepos=\"362:1-368:123\"\u003e上記の内容に関して質問: (である調で記載)\u003cbr\u003e\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\u003cbr\u003e\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\u003cbr\u003e\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n⑤比較の記載があれば違いを\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003cbr\u003e\n⑥上記⑤以外で表に出来そうな部分があれば\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003c/p\u003e\n\u003cp data-sourcepos=\"370:1-372:261\"\u003e\u003cstrong\u003e▼生成AIの回答(ChatGPT-4o)\u003c/strong\u003e\u003cbr\u003e\n① \u003cstrong\u003e何についての記載か\u003c/strong\u003e\u003cbr\u003e\nこの章は、畳み込みニューラルネットワーク（CNN）の関連研究について述べており、特に深層アーキテクチャ、アルゴリズム的なアーキテクチャ探索、注意メカニズムなどに焦点を当てている。\u003c/p\u003e\n\u003cp data-sourcepos=\"374:1-374:10\"\u003e\u003cstrong\u003e要旨\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"375:1-380:0\"\u003e\n\u003cli data-sourcepos=\"375:1-375:144\"\u003eVGGNetsやInceptionモデルなど、ネットワークの深さを増すことにより表現力を向上させる手法について議論。\u003c/li\u003e\n\u003cli data-sourcepos=\"376:1-376:156\"\u003eResNetsなど、ネットワーク層間の接続を再構築する手法が、深層ネットワークの学習を改善することを示している。\u003c/li\u003e\n\u003cli data-sourcepos=\"377:1-377:167\"\u003eグループ化された畳み込みやマルチブランチ畳み込みのような手法が、チャネル間の相関を捉えるために提案されている。\u003c/li\u003e\n\u003cli data-sourcepos=\"378:1-378:140\"\u003eアーキテクチャ探索のための進化的手法や強化学習を用いた自動化手法についての研究が進んでいる。\u003c/li\u003e\n\u003cli data-sourcepos=\"379:1-380:0\"\u003e注意メカニズムやゲーティングメカニズムが、信号の最も情報量の多い部分に計算リソースを集中させる方法として有用であることが示されている。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"381:1-382:524\"\u003e② \u003cstrong\u003e改良点・工夫点・テクニック等の記載\u003c/strong\u003e\u003cbr\u003e\nこの論文では、チャネル間の動的で非線形な依存関係をグローバルな情報を用いて明示的にモデル化するメカニズムを提案し、これによりネットワークの表現力を大幅に強化することを主張している。また、SEブロックは注意メカニズムやゲーティングメカニズムの一環として、計算効率を保ちながらチャネル間の関係をモデル化するための軽量なゲーティングメカニズムを提供している。\u003c/p\u003e\n\u003cp data-sourcepos=\"384:1-385:6\"\u003e③ \u003cstrong\u003e性能が向上した記載\u003c/strong\u003e\u003cbr\u003e\n無し\u003c/p\u003e\n\u003cp data-sourcepos=\"387:1-388:239\"\u003e④ \u003cstrong\u003eメカニズムの解明・なぜそうなるのか等の記載\u003c/strong\u003e\u003cbr\u003e\nSEブロックは、注意メカニズムやゲーティングメカニズムの一部として、信号の最も情報量の多い部分に計算リソースを集中させることで、ネットワークの表現力を向上させる。\u003c/p\u003e\n\u003cp data-sourcepos=\"390:1-391:6\"\u003e⑤ \u003cstrong\u003e比較の記載\u003c/strong\u003e\u003cbr\u003e\n無し\u003c/p\u003e\n\u003cp data-sourcepos=\"393:1-394:6\"\u003e⑥ \u003cstrong\u003e上記⑤以外で表に出来そうな部分\u003c/strong\u003e\u003cbr\u003e\n無し\u003c/p\u003e\n\u003cp data-sourcepos=\"396:1-397:55\"\u003e\u003cstrong\u003e▼質問(人間)\u003c/strong\u003e\u003cbr\u003e\n関連研究について表にまとめられないか?\u003c/p\u003e\n\u003cp data-sourcepos=\"399:1-400:102\"\u003e\u003cstrong\u003e▼生成AIの回答(ChatGPT-4o)\u003c/strong\u003e\u003cbr\u003e\n前述の関連研究については、以下のように表にまとめることが可能である。\u003c/p\u003e\n\u003ctable data-sourcepos=\"402:1-409:336\"\u003e\n\u003cthead\u003e\n\u003ctr data-sourcepos=\"402:1-402:250\"\u003e\n\u003cth data-sourcepos=\"402:2-402:36\"\u003e\u003cstrong\u003eカテゴリー\u003c/strong\u003e\u003c/th\u003e\n\u003cth data-sourcepos=\"402:38-402:81\"\u003e\u003cstrong\u003e主な研究\u003c/strong\u003e\u003c/th\u003e\n\u003cth data-sourcepos=\"402:83-402:194\"\u003e\u003cstrong\u003e内容\u003c/strong\u003e\u003c/th\u003e\n\u003cth data-sourcepos=\"402:196-402:249\"\u003e\u003cstrong\u003e特記事項\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr data-sourcepos=\"404:1-404:305\"\u003e\n\u003ctd data-sourcepos=\"404:2-404:41\"\u003e\u003cstrong\u003e深層アーキテクチャ\u003c/strong\u003e\u003c/td\u003e\n\u003ctd data-sourcepos=\"404:43-404:82\"\u003eVGGNets, Inception, ResNets\u003c/td\u003e\n\u003ctd data-sourcepos=\"404:84-404:240\"\u003eネットワークの深さを増すことで表現力を向上。ResNetsはスキップ接続を使用して学習を容易に。\u003c/td\u003e\n\u003ctd data-sourcepos=\"404:242-404:304\"\u003eResNetsは学習の安定性を改善する。\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"405:1-405:305\"\u003e\n\u003ctd data-sourcepos=\"405:2-405:44\"\u003e\u003cstrong\u003eグループ化された畳み込み\u003c/strong\u003e\u003c/td\u003e\n\u003ctd data-sourcepos=\"405:46-405:85\"\u003eGrouped Convolutions\u003c/td\u003e\n\u003ctd data-sourcepos=\"405:87-405:236\"\u003e畳み込み演算をグループ化することで、変換のカーディナリティを増加。\u003c/td\u003e\n\u003ctd data-sourcepos=\"405:238-405:304\"\u003eモデルと計算の複雑さを低減する手法。\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"406:1-406:327\"\u003e\n\u003ctd data-sourcepos=\"406:2-406:43\"\u003e\u003cstrong\u003eマルチブランチ畳み込み\u003c/strong\u003e\u003c/td\u003e\n\u003ctd data-sourcepos=\"406:45-406:84\"\u003eInception, ResNeXt\u003c/td\u003e\n\u003ctd data-sourcepos=\"406:86-406:262\"\u003e複数のブランチを使用した柔軟なオペレーターの組み合わせによって、空間構造とチャネル間の相関を同時にモデリング。\u003c/td\u003e\n\u003ctd data-sourcepos=\"406:264-406:326\"\u003eモデルの表現力を向上させる。\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"407:1-407:315\"\u003e\n\u003ctd data-sourcepos=\"407:2-407:41\"\u003e\u003cstrong\u003eアーキテクチャ探索\u003c/strong\u003e\u003c/td\u003e\n\u003ctd data-sourcepos=\"407:43-407:88\"\u003eEvolutionary Methods, Reinforcement Learning\u003c/td\u003e\n\u003ctd data-sourcepos=\"407:90-407:246\"\u003e手動での設計を避け、進化的手法や強化学習によってネットワークの構造を自動的に学習。\u003c/td\u003e\n\u003ctd data-sourcepos=\"407:248-407:314\"\u003e高度なネットワーク構造を自動で発見。\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"408:1-408:317\"\u003e\n\u003ctd data-sourcepos=\"408:2-408:39\"\u003e\u003cstrong\u003e注意メカニズム\u003c/strong\u003e\u003c/td\u003e\n\u003ctd data-sourcepos=\"408:41-408:80\"\u003eAttention, Trunk-and-Mask\u003c/td\u003e\n\u003ctd data-sourcepos=\"408:82-408:245\"\u003e信号の最も情報量の多い部分に計算リソースを集中させることで、ネットワークの表現力を強化。\u003c/td\u003e\n\u003ctd data-sourcepos=\"408:247-408:316\"\u003eSEブロックも注意メカニズムの一種として機能。\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"409:1-409:336\"\u003e\n\u003ctd data-sourcepos=\"409:2-409:43\"\u003e\u003cstrong\u003eゲーティングメカニズム\u003c/strong\u003e\u003c/td\u003e\n\u003ctd data-sourcepos=\"409:45-409:84\"\u003eHighway Networks, SE Blocks\u003c/td\u003e\n\u003ctd data-sourcepos=\"409:86-409:263\"\u003e情報の流れを調整するためのメカニズム。SEブロックはチャネル間の依存関係をモデル化し、ネットワークの表現力を向上させる。\u003c/td\u003e\n\u003ctd data-sourcepos=\"409:265-409:335\"\u003e計算効率が高く、既存アーキテクチャに適用可能。\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp data-sourcepos=\"411:1-411:168\"\u003eこの表により、各研究のカテゴリーと主な内容、および特記事項が整理され、関連研究の全体像を理解しやすくなっている。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"413:1-413:34\"\u003e\n\u003cspan id=\"3-squeeze-and-excitation-blocks\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#3-squeeze-and-excitation-blocks\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e3 SQUEEZE-AND-EXCITATION BLOCKS\u003c/h2\u003e\n\u003cp data-sourcepos=\"415:1-455:48\"\u003e\u003cstrong\u003e▼質問(人間)\u003c/strong\u003e\u003cbr\u003e\n(論文・記事の各章節を貼り付け)\u003cbr\u003e\n3 SQUEEZE-AND-EXCITATION BLOCKS\u003cbr\u003e\nThe Squeeze-and-Excitation block is a computational unit\u003cbr\u003e\nwhich can be constructed for any given transformation\u003cbr\u003e\nU = Ftr (X), X ∈ RH′×W ′×C′\u003cbr\u003e\n, U ∈ RH×W ×C . For sim-\u003cbr\u003e\nplicity, in the notation that follows we take Ftr to be a\u003cbr\u003e\nconvolutional operator. Let V = [v1, v2, . . . , vC ] denote\u003cbr\u003e\nthe learned set of filter kernels, where vc refers to the\u003cbr\u003e\nparameters of the c-th filter. We can then write the outputs\u003cbr\u003e\nof Ftr as U = [u1, u2, . . . , uC ], where\u003cbr\u003e\nuc = vc ∗ X =\u003cbr\u003e\nC′\u003cbr\u003e\n∑\u003cbr\u003e\ns=1\u003cbr\u003e\nvs\u003cbr\u003e\nc ∗ xs. (1)\u003cbr\u003e\nHere ∗ denotes convolution, vc = [v1\u003cbr\u003e\nc , v2\u003cbr\u003e\nc , . . . , vC′\u003cbr\u003e\nc ], X =\u003cbr\u003e\n[x1, x2, . . . , xC′\u003cbr\u003e\n] (to simplify the notation, bias terms are\u003cbr\u003e\nomitted) and vs\u003cbr\u003e\nc is a 2D spatial kernel representing a single\u003cbr\u003e\nchannel of vc that acts on the corresponding channel of X.\u003cbr\u003e\nSince the output is produced by a summation through all\u003cbr\u003e\nchannels, channel dependencies are implicitly embedded in\u003cbr\u003e\nvc, but are entangled with the local spatial correlation cap-\u003cbr\u003e\ntured by the filters. As a consequence, the channel relation-\u003cbr\u003e\nships modelled by convolution are inherently local. Since\u003cbr\u003e\nour goal is to ensure that the network is able to increase\u003cbr\u003e\nits sensitivity to informative features so that they can be\u003cbr\u003e\nexploited by subsequent transformations most effectively,\u003cbr\u003e\nwe would like to provide it with access to global infor-\u003cbr\u003e\nmation. We propose to achieve this by explicitly modelling\u003cbr\u003e\nchannel interdependencies to recalibrate filter responses in\u003cbr\u003e\ntwo steps, squeeze and excitation, before they are fed into the\u003cbr\u003e\nnext transformation, described next. A diagram illustrating\u003cbr\u003e\nthe structure of an SE block is shown in Fig. 1.\u003c/p\u003e\n\u003cp data-sourcepos=\"457:1-463:123\"\u003e上記の内容に関して質問: (である調で記載)\u003cbr\u003e\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\u003cbr\u003e\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\u003cbr\u003e\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n⑤比較の記載があれば違いを\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003cbr\u003e\n⑥上記⑤以外で表に出来そうな部分があれば\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003c/p\u003e\n\u003cp data-sourcepos=\"465:1-467:311\"\u003e\u003cstrong\u003e▼生成AIの回答(ChatGPT-4o)\u003c/strong\u003e\u003cbr\u003e\n① \u003cstrong\u003e何についての記載か\u003c/strong\u003e\u003cbr\u003e\nこの節は、Squeeze-and-Excitation（SE）ブロックの設計と機能に関する説明である。SEブロックは、CNNの変換操作に対してチャネル間の依存関係を明示的にモデル化し、フィルタの応答を再キャリブレーションするための計算単位である。\u003c/p\u003e\n\u003cp data-sourcepos=\"469:1-469:10\"\u003e\u003cstrong\u003e要旨\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"470:1-473:0\"\u003e\n\u003cli data-sourcepos=\"470:1-470:94\"\u003eSEブロックは、任意の変換操作に対して構築可能な計算単位である。\u003c/li\u003e\n\u003cli data-sourcepos=\"471:1-471:176\"\u003e変換操作によって生成されるフィルタ応答を、チャネル間の依存関係を明示的にモデル化することで再キャリブレーションする。\u003c/li\u003e\n\u003cli data-sourcepos=\"472:1-473:0\"\u003eSEブロックは、2つのステップ（スクイーズとエキサイト）でフィルタ応答を再キャリブレーションする。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"474:1-475:177\"\u003e② \u003cstrong\u003e改良点・工夫点・テクニック等の記載\u003c/strong\u003e\u003cbr\u003e\nSEブロックの主要な改良点は、チャネル間の依存関係を明示的にモデル化することである。具体的には、以下の工夫がなされている:\u003c/p\u003e\n\u003cul data-sourcepos=\"476:1-478:0\"\u003e\n\u003cli data-sourcepos=\"476:1-476:165\"\u003e\n\u003cstrong\u003eスクイーズ（Squeeze）\u003c/strong\u003e: フィルタ応答を空間次元で集約し、チャネルごとのグローバルなコンテキスト情報を取得する。\u003c/li\u003e\n\u003cli data-sourcepos=\"477:1-478:0\"\u003e\n\u003cstrong\u003eエキサイト（Excitation）\u003c/strong\u003e: スクイーズされたグローバル情報を基に、各チャネルの重要度を決定するための自己ゲーティング機構を導入。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"479:1-480:254\"\u003e③ \u003cstrong\u003e性能が向上した記載\u003c/strong\u003e\u003cbr\u003e\n性能向上に関する具体値はこの節には記載されていないが、SEブロックはフィルタの応答を再キャリブレーションすることで、ネットワークの感度を高めることができると述べられている。\u003c/p\u003e\n\u003cp data-sourcepos=\"482:1-483:447\"\u003e④ \u003cstrong\u003eメカニズムの解明・なぜそうなるのか等の記載\u003c/strong\u003e\u003cbr\u003e\nSEブロックは、チャネル間の依存関係が畳み込み操作で局所的にしかモデル化されていないという問題を解決するために設計された。スクイーズとエキサイトの2つのステップを通じて、チャネル間の依存関係をグローバルなスケールで明示的にモデル化し、ネットワークが有用な特徴に対してより敏感に反応できるようにしている。\u003c/p\u003e\n\u003cp data-sourcepos=\"485:1-486:6\"\u003e⑤ \u003cstrong\u003e比較の記載\u003c/strong\u003e\u003cbr\u003e\n無し\u003c/p\u003e\n\u003cp data-sourcepos=\"488:1-489:33\"\u003e⑥ \u003cstrong\u003e上記⑤以外で表に出来そうな部分\u003c/strong\u003e\u003cbr\u003e\n表にする部分は特に無し\u003c/p\u003e\n\u003ch2 data-sourcepos=\"491:1-491:44\"\u003e\n\u003cspan id=\"31-squeeze-global-information-embedding\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#31-squeeze-global-information-embedding\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e3.1 Squeeze: Global Information Embedding\u003c/h2\u003e\n\u003cp data-sourcepos=\"493:1-521:48\"\u003e\u003cstrong\u003e▼質問(人間)\u003c/strong\u003e\u003cbr\u003e\n(論文・記事の各章節を貼り付け)\u003cbr\u003e\n3.1 Squeeze: Global Information Embedding\u003cbr\u003e\nIn order to tackle the issue of exploiting channel depen-\u003cbr\u003e\ndencies, we first consider the signal to each channel in the\u003cbr\u003e\noutput features. Each of the learned filters operates with\u003cbr\u003e\na local receptive field and consequently each unit of the\u003cbr\u003e\ntransformation output U is unable to exploit contextual\u003cbr\u003e\ninformation outside of this region.\u003cbr\u003e\nTo mitigate this problem, we propose to squeeze global\u003cbr\u003e\nspatial information into a channel descriptor. This is\u003cbr\u003e\nachieved by using global average pooling to generate\u003cbr\u003e\nchannel-wise statistics. Formally, a statistic z ∈ RC is gener-\u003cbr\u003e\nated by shrinking U through its spatial dimensions H × W ,\u003cbr\u003e\nsuch that the c-th element of z is calculated by:\u003cbr\u003e\nzc = Fsq (uc) = 1\u003cbr\u003e\nH × W\u003cbr\u003e\nH∑\u003cbr\u003e\ni=1\u003cbr\u003e\nW∑\u003cbr\u003e\nj=1\u003cbr\u003e\nuc(i, j). (2)\u003cbr\u003e\nDiscussion. The output of the transformation U can be\u003cbr\u003e\ninterpreted as a collection of the local descriptors whose\u003cbr\u003e\nstatistics are expressive for the whole image. Exploiting\u003cbr\u003e\nsuch information is prevalent in prior feature engineering\u003cbr\u003e\nwork [61]–[63]. We opt for the simplest aggregation tech-\u003cbr\u003e\nnique, global average pooling, noting that more sophisti-\u003cbr\u003e\ncated strategies could be employed here as well.\u003c/p\u003e\n\u003cp data-sourcepos=\"523:1-529:123\"\u003e上記の内容に関して質問: (である調で記載)\u003cbr\u003e\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\u003cbr\u003e\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\u003cbr\u003e\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n⑤比較の記載があれば違いを\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003cbr\u003e\n⑥上記⑤以外で表に出来そうな部分があれば\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003c/p\u003e\n\u003cp data-sourcepos=\"531:1-533:275\"\u003e\u003cstrong\u003e▼生成AIの回答(ChatGPT-4o)\u003c/strong\u003e\u003cbr\u003e\n① \u003cstrong\u003e何についての記載か\u003c/strong\u003e\u003cbr\u003e\nこの節は、Squeeze-and-Excitation（SE）ブロックの「Squeeze」操作について説明している。Squeeze操作は、チャネル依存関係を活用するために、グローバルな空間情報をチャネル記述子に圧縮するプロセスである。\u003c/p\u003e\n\u003cp data-sourcepos=\"535:1-535:10\"\u003e\u003cstrong\u003e要旨\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"536:1-538:0\"\u003e\n\u003cli data-sourcepos=\"536:1-536:179\"\u003e各チャネルの信号が局所的な受容野に制約されている問題を解決するために、グローバルな空間情報をチャネル記述子に圧縮する。\u003c/li\u003e\n\u003cli data-sourcepos=\"537:1-538:0\"\u003eグローバル平均プーリングを使用してチャネルごとの統計を生成し、各チャネルの要素を圧縮する。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"539:1-539:61\"\u003e② \u003cstrong\u003e改良点・工夫点・テクニック等の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"540:1-541:0\"\u003e\n\u003cli data-sourcepos=\"540:1-541:0\"\u003eSqueeze操作では、グローバルな空間情報を効果的に活用するために、グローバル平均プーリングを使用している。これにより、各チャネルの統計が生成され、ネットワークが局所的な受容野を超えた情報を利用できるようにしている。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"542:1-543:63\"\u003e③ \u003cstrong\u003e性能が向上した記載\u003c/strong\u003e\u003cbr\u003e\n性能向上に関する具体値は記載されていない。\u003c/p\u003e\n\u003cp data-sourcepos=\"545:1-546:303\"\u003e④ \u003cstrong\u003eメカニズムの解明・なぜそうなるのか等の記載\u003c/strong\u003e\u003cbr\u003e\nグローバル平均プーリングによって生成されたチャネル記述子は、局所的な記述子の集合として解釈される。これにより、ネットワークが画像全体の情報を利用できるようになり、より表現力の高い特徴を得ることができる。\u003c/p\u003e\n\u003cp data-sourcepos=\"548:1-549:6\"\u003e⑤ \u003cstrong\u003e比較の記載\u003c/strong\u003e\u003cbr\u003e\n無し\u003c/p\u003e\n\u003cp data-sourcepos=\"551:1-552:33\"\u003e⑥ \u003cstrong\u003e上記⑤以外で表に出来そうな部分\u003c/strong\u003e\u003cbr\u003e\n表にする部分は特に無し\u003c/p\u003e\n\u003ch2 data-sourcepos=\"554:1-554:41\"\u003e\n\u003cspan id=\"32-excitation-adaptive-recalibration\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#32-excitation-adaptive-recalibration\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e3.2 Excitation: Adaptive Recalibration\u003c/h2\u003e\n\u003cp data-sourcepos=\"556:1-591:10\"\u003e\u003cstrong\u003e▼質問(人間)\u003c/strong\u003e\u003cbr\u003e\n(論文・記事の各章節を貼り付け)\u003cbr\u003e\n3.2 Excitation: Adaptive Recalibration\u003cbr\u003e\nTo make use of the information aggregated in the squeeze\u003cbr\u003e\noperation, we follow it with a second operation which aims\u003cbr\u003e\nto fully capture channel-wise dependencies. To fulfil this\u003cbr\u003e\nobjective, the function must meet two criteria: first, it must\u003cbr\u003e\nbe flexible (in particular, it must be capable of learning\u003cbr\u003e\na nonlinear interaction between channels) and second, it\u003cbr\u003e\nmust learn a non-mutually-exclusive relationship since we\u003cbr\u003e\nwould like to ensure that multiple channels are allowed to\u003cbr\u003e\nbe emphasised (rather than enforcing a one-hot activation).\u003cbr\u003e\nTo meet these criteria, we opt to employ a simple gating\u003cbr\u003e\nmechanism with a sigmoid activation:\u003cbr\u003e\ns = Fex(z, W) = σ(g(z, W)) = σ(W2δ(W1z)), (3)\u003cbr\u003e\nwhere δ refers to the ReLU [64] function, W1 ∈ R C\u003cbr\u003e\nr ×C and\u003cbr\u003e\nW2 ∈ RC× C\u003cbr\u003e\nr . To limit model complexity and aid general-\u003cbr\u003e\nisation, we parameterise the gating mechanism by forming\u003cbr\u003e\na bottleneck with two fully connected (FC) layers around\u003cbr\u003e\nthe non-linearity, i.e. a dimensionality-reduction layer with\u003cbr\u003e\nparameters W1 and reduction ratio r (this parameter choice\u003cbr\u003e\nis discussed in Sec. 6.1), a ReLU and then a dimensionality-\u003cbr\u003e\nincreasing layer with parameters W2. The final output of\u003cbr\u003e\nthe block is obtained by rescaling the transformation output\u003cbr\u003e\nU with the activations:\u003cbr\u003e\n˜xc = Fscale(uc, sc) = sc · uc, (4)\u003cbr\u003e\nwhere ˜X = [˜x1, ˜x2, . . . , ˜xC ] and Fscale(uc, sc) refers to\u003cbr\u003e\nchannel-wise multiplication between the scalar sc and the\u003cbr\u003e\nfeature map uc ∈ RH×W .\u003cbr\u003e\nDiscussion. The excitation operator maps the input-\u003cbr\u003e\nspecific descriptor z to a set of channel specific weights.\u003cbr\u003e\nIn this regard, SE blocks intrinsically introduce dynamics\u003cbr\u003e\nconditioned on the input, helping to boost feature discrim-\u003cbr\u003e\ninability.\u003c/p\u003e\n\u003cp data-sourcepos=\"593:1-599:123\"\u003e上記の内容に関して質問: (である調で記載)\u003cbr\u003e\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\u003cbr\u003e\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\u003cbr\u003e\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n⑤比較の記載があれば違いを\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003cbr\u003e\n⑥上記⑤以外で表に出来そうな部分があれば\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003c/p\u003e\n\u003cp data-sourcepos=\"601:1-603:260\"\u003e\u003cstrong\u003e▼生成AIの回答(ChatGPT-4o)\u003c/strong\u003e\u003cbr\u003e\n① \u003cstrong\u003e何についての記載か\u003c/strong\u003e\u003cbr\u003e\nこの節は、Squeeze-and-Excitation（SE）ブロックの「Excitation」操作について説明している。Excitation操作は、チャネル間の依存関係を捉え、各チャネルの重要度を適応的に再調整するプロセスである。\u003c/p\u003e\n\u003cp data-sourcepos=\"605:1-605:10\"\u003e\u003cstrong\u003e要旨\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"606:1-609:0\"\u003e\n\u003cli data-sourcepos=\"606:1-606:133\"\u003eSqueeze操作で集約された情報を活用し、チャネル間の依存関係を捉えるためにExcitation操作を行う。\u003c/li\u003e\n\u003cli data-sourcepos=\"607:1-607:209\"\u003eチャネルごとの依存関係を学習し、複数のチャネルが強調されるようにするために、シグモイド活性化を伴う単純なゲーティングメカニズムを使用する。\u003c/li\u003e\n\u003cli data-sourcepos=\"608:1-609:0\"\u003eゲーティングメカニズムはボトルネック構造を持ち、モデルの複雑さを制限しつつ、非線形なチャネル間の相互作用を学習する。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"610:1-610:61\"\u003e② \u003cstrong\u003e改良点・工夫点・テクニック等の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"611:1-612:0\"\u003e\n\u003cli data-sourcepos=\"611:1-612:0\"\u003eシンプルなゲーティングメカニズムを使用し、ReLUを使った非線形性とボトルネック構造（2層の全結合層）を導入することで、モデルの複雑さを抑えつつ、チャネル間の依存関係を効果的に捉えている。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"613:1-614:63\"\u003e③ \u003cstrong\u003e性能が向上した記載\u003c/strong\u003e\u003cbr\u003e\n性能向上に関する具体値は記載されていない。\u003c/p\u003e\n\u003cp data-sourcepos=\"616:1-617:222\"\u003e④ \u003cstrong\u003eメカニズムの解明・なぜそうなるのか等の記載\u003c/strong\u003e\u003cbr\u003e\nExcitation操作によって、入力に依存したチャネルごとの重みが生成される。これにより、SEブロックは入力に応じた動的な反応を引き起こし、特徴の識別力を高める。\u003c/p\u003e\n\u003cp data-sourcepos=\"619:1-620:6\"\u003e⑤ \u003cstrong\u003e比較の記載\u003c/strong\u003e\u003cbr\u003e\n無し\u003c/p\u003e\n\u003cp data-sourcepos=\"622:1-623:33\"\u003e⑥ \u003cstrong\u003e上記⑤以外で表に出来そうな部分\u003c/strong\u003e\u003cbr\u003e\n表にする部分は特に無し\u003c/p\u003e\n\u003ch2 data-sourcepos=\"625:1-625:21\"\u003e\n\u003cspan id=\"33-instantiations\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#33-instantiations\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e3.3 Instantiations\u003c/h2\u003e\n\u003cp data-sourcepos=\"627:1-658:12\"\u003e\u003cstrong\u003e▼質問(人間)\u003c/strong\u003e\u003cbr\u003e\n(論文・記事の各章節を貼り付け)\u003cbr\u003e\n3.3 Instantiations\u003cbr\u003e\nThe SE block can be integrated into standard architectures\u003cbr\u003e\nsuch as VGGNet [11] by insertion after the non-linearity\u003cbr\u003e\nfollowing each convolution. Moreover, the flexibility of the\u003cbr\u003e\nSE block means that it can be directly applied to transforma-\u003cbr\u003e\ntions beyond standard convolutions. To illustrate this point,\u003cbr\u003e\nwe develop SENets by incorporating SE blocks into several\u003cbr\u003e\nexamples of more complex architectures, described next.\u003cbr\u003e\nWe first consider the construction of SE blocks for Incep-\u003cbr\u003e\ntion networks [5]. Here, we simply take the transformation\u003cbr\u003e\nFtr to be an entire Inception module (see Fig. 2) and by mak-\u003cbr\u003e\ning this change for each such module in the architecture, we\u003cbr\u003e\nobtain an SE-Inception network. SE blocks can also be used\u003cbr\u003e\ndirectly with residual networks (Fig. 3 depicts the schema\u003cbr\u003e\nof an SE-ResNet module). Here, the SE block transformation\u003cbr\u003e\nFtr is taken to be the non-identity branch of a residual\u003cbr\u003e\nmodule. Squeeze and Excitation both act before summation\u003cbr\u003e\nwith the identity branch. Further variants that integrate SE\u003cbr\u003e\nblocks with ResNeXt [19], Inception-ResNet [21], MobileNet\u003cbr\u003e\n[65] and ShuffleNet [66] can be constructed by following\u003cbr\u003e\nsimilar schemes (Sec. 5.1). For concrete examples of SENet\u003cbr\u003e\narchitectures, a detailed description of SE-ResNet-50 and\u003cbr\u003e\nSE-ResNeXt-50 is given in Table 1.\u003cbr\u003e\nOne consequence of the flexible nature of the SE block\u003cbr\u003e\nis that there are several viable ways in which it could\u003cbr\u003e\nbe integrated into these architectures. Therefore, to assess\u003cbr\u003e\nsensitivity to the integration strategy used to incorporate SE\u003cbr\u003e\nblocks into a network architecture, we also provide ablation\u003cbr\u003e\nexperiments exploring different designs for block inclusion\u003cbr\u003e\nin Sec. 6.5.\u003c/p\u003e\n\u003cp data-sourcepos=\"660:1-666:123\"\u003e上記の内容に関して質問: (である調で記載)\u003cbr\u003e\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\u003cbr\u003e\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\u003cbr\u003e\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n⑤比較の記載があれば違いを\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003cbr\u003e\n⑥上記⑤以外で表に出来そうな部分があれば\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003c/p\u003e\n\u003cp data-sourcepos=\"668:1-670:373\"\u003e\u003cstrong\u003e▼生成AIの回答(ChatGPT-4o)\u003c/strong\u003e\u003cbr\u003e\n① \u003cstrong\u003e何についての記載か\u003c/strong\u003e\u003cbr\u003e\nこの節は、Squeeze-and-Excitation（SE）ブロックの実装に関する記載である。SEブロックがどのように標準的なニューラルネットワークアーキテクチャ（例：VGGNet、Inception、ResNetなど）に統合されるかを説明し、特定のアーキテクチャにおけるSEブロックの適用例について述べている。\u003c/p\u003e\n\u003cp data-sourcepos=\"672:1-672:10\"\u003e\u003cstrong\u003e要旨\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"673:1-677:0\"\u003e\n\u003cli data-sourcepos=\"673:1-673:112\"\u003eSEブロックは、VGGNetやInceptionなどの標準的なアーキテクチャに簡単に統合できる。\u003c/li\u003e\n\u003cli data-sourcepos=\"674:1-674:160\"\u003eInceptionネットワークやResNetにSEブロックを組み込んだSE-Inceptionネットワーク、SE-ResNetなどの具体例が説明されている。\u003c/li\u003e\n\u003cli data-sourcepos=\"675:1-675:125\"\u003e他の複雑なアーキテクチャ（ResNeXt、Inception-ResNet、MobileNet、ShuffleNetなど）への応用も可能。\u003c/li\u003e\n\u003cli data-sourcepos=\"676:1-677:0\"\u003eSEブロックを統合するための異なるデザインの影響を調べるためのアブレーション実験が実施されている。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"678:1-678:61\"\u003e② \u003cstrong\u003e改良点・工夫点・テクニック等の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"679:1-681:0\"\u003e\n\u003cli data-sourcepos=\"679:1-679:181\"\u003eSEブロックを既存の様々なアーキテクチャに統合する際に、その柔軟性を活かして異なるデザインを適用できる点が工夫されている。\u003c/li\u003e\n\u003cli data-sourcepos=\"680:1-681:0\"\u003e異なるアーキテクチャに対して、SEブロックの統合位置や手法を調整することが可能である。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"682:1-683:158\"\u003e③ \u003cstrong\u003e性能が向上した記載\u003c/strong\u003e\u003cbr\u003e\n具体値は記載されていないが、SEブロックを適用することで各アーキテクチャの性能向上が期待されるとされている。\u003c/p\u003e\n\u003cp data-sourcepos=\"685:1-686:6\"\u003e④ \u003cstrong\u003eメカニズムの解明・なぜそうなるのか等の記載\u003c/strong\u003e\u003cbr\u003e\n無し\u003c/p\u003e\n\u003cp data-sourcepos=\"688:1-689:6\"\u003e⑤ \u003cstrong\u003e比較の記載\u003c/strong\u003e\u003cbr\u003e\n無し\u003c/p\u003e\n\u003cp data-sourcepos=\"691:1-692:203\"\u003e⑥ \u003cstrong\u003e上記⑤以外で表に出来そうな部分\u003c/strong\u003e\u003cbr\u003e\n表にできそうな部分として、各アーキテクチャ（VGGNet、Inception、ResNetなど）におけるSEブロックの適用例や統合方法を表形式でまとめることができる。\u003c/p\u003e\n\u003ctable data-sourcepos=\"694:1-702:63\"\u003e\n\u003cthead\u003e\n\u003ctr data-sourcepos=\"694:1-694:57\"\u003e\n\u003cth data-sourcepos=\"694:2-694:24\"\u003eアーキテクチャ\u003c/th\u003e\n\u003cth data-sourcepos=\"694:26-694:56\"\u003eSEブロックの適用方法\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr data-sourcepos=\"696:1-696:72\"\u003e\n\u003ctd data-sourcepos=\"696:2-696:17\"\u003eVGGNet\u003c/td\u003e\n\u003ctd data-sourcepos=\"696:19-696:71\"\u003e各畳み込み層後の非線形性の後に挿入\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"697:1-697:70\"\u003e\n\u003ctd data-sourcepos=\"697:2-697:17\"\u003eInception\u003c/td\u003e\n\u003ctd data-sourcepos=\"697:19-697:69\"\u003e各Inceptionモジュール全体に適用\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"698:1-698:135\"\u003e\n\u003ctd data-sourcepos=\"698:2-698:17\"\u003eResNet\u003c/td\u003e\n\u003ctd data-sourcepos=\"698:19-698:134\"\u003e非アイデンティティブランチに適用し、アイデンティティブランチとの和の前に適用\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"699:1-699:65\"\u003e\n\u003ctd data-sourcepos=\"699:2-699:17\"\u003eResNeXt\u003c/td\u003e\n\u003ctd data-sourcepos=\"699:19-699:64\"\u003eResNetと同様の手法で適用\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"700:1-700:89\"\u003e\n\u003ctd data-sourcepos=\"700:2-700:19\"\u003eInception-ResNet\u003c/td\u003e\n\u003ctd data-sourcepos=\"700:21-700:88\"\u003eInceptionおよびResNetの組み合わせモジュールに適用\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"701:1-701:62\"\u003e\n\u003ctd data-sourcepos=\"701:2-701:17\"\u003eMobileNet\u003c/td\u003e\n\u003ctd data-sourcepos=\"701:19-701:61\"\u003eMobileNetモジュールに適用\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"702:1-702:63\"\u003e\n\u003ctd data-sourcepos=\"702:2-702:17\"\u003eShuffleNet\u003c/td\u003e\n\u003ctd data-sourcepos=\"702:19-702:62\"\u003eShuffleNetモジュールに適用\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 data-sourcepos=\"704:1-704:39\"\u003e\n\u003cspan id=\"4-model-and-computational-complexity\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#4-model-and-computational-complexity\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e4 MODEL AND COMPUTATIONAL COMPLEXITY\u003c/h2\u003e\n\u003cp data-sourcepos=\"706:1-770:58\"\u003e\u003cstrong\u003e▼質問(人間)\u003c/strong\u003e\u003cbr\u003e\n(論文・記事の各章節を貼り付け)\u003cbr\u003e\n4 MODEL AND COMPUTATIONAL COMPLEXITY\u003cbr\u003e\nFor the proposed SE block design to be of practical use, it\u003cbr\u003e\nmust offer a good trade-off between improved performance\u003cbr\u003e\nand increased model complexity. We set the reduction ratio\u003cbr\u003e\nr (introduced in Sec. 3.2) to 16 in all experiments, except\u003cbr\u003e\nwhere stated otherwise (an ablation study of this design\u003cbr\u003e\ndecision is provided in Sec. 6.1). To illustrate the compu-\u003cbr\u003e\ntational burden associated with the module, we consider\u003cbr\u003e\na comparison between ResNet-50 and SE-ResNet-50 as an\u003cbr\u003e\nexample. ResNet-50 requires ∼3.86 GFLOPs in a single\u003cbr\u003e\nforward pass for a 224 × 224 pixel input image. Each SE\u003cbr\u003e\nblock makes use of a global average pooling operation in\u003cbr\u003e\nthe squeeze phase and two small fully connected layers in\u003cbr\u003e\nthe excitation phase, followed by an inexpensive channel-\u003cbr\u003e\nwise scaling operation. In aggregate, SE-ResNet-50 requires\u003cbr\u003e\n∼3.87 GFLOPs, corresponding to a 0.26% relative increase\u003cbr\u003e\nover the original ResNet-50. In exchange for this slight addi-\u003cbr\u003e\ntional computational burden, the accuracy of SE-ResNet-50\u003cbr\u003e\nsurpasses that of ResNet-50 and indeed, approaches that\u003cbr\u003e\nof a deeper ResNet-101 network requiring ∼7.58 GFLOPs\u003cbr\u003e\n(Table 2).\u003cbr\u003e\nIn practical terms, a single pass forwards and backwards\u003cbr\u003e\nthrough ResNet-50 takes 190 ms, compared to 209 ms for\u003cbr\u003e\nSE-ResNet-50 with a training minibatch of 256 images (both\u003cbr\u003e\ntimings are performed on a server with 8 NVIDIA Titan X\u003cbr\u003e\nGPUs). We suggest that this represents a reasonable runtime\u003cbr\u003e\noverhead, which may be further reduced as global pooling\u003cbr\u003e\nand small inner-product operations receive further opti-\u003cbr\u003e\nmisation in popular GPU libraries. Due to its importance\u003cbr\u003e\nfor embedded device applications, we further benchmark\u003cbr\u003e\nCPU inference time for each model: for a 224 × 224 pixel\u003cbr\u003e\ninput image, ResNet-50 takes 164 ms in comparison to 167\u003cbr\u003e\nms for SE-ResNet-50. We believe that the small additional\u003cbr\u003e\ncomputational cost incurred by the SE block is justified by\u003cbr\u003e\nits contribution to model performance.\u003cbr\u003e\nWe next consider the additional parameters introduced\u003cbr\u003e\nby the proposed SE block. These additional parameters\u003cbr\u003e\nresult solely from the two fully-connected layers of the\u003cbr\u003e\ngating mechanism and therefore constitute a small fraction\u003cbr\u003e\nof the total network capacity. Concretely, the total number of\u003cbr\u003e\nadditional parameters introduced by the proposed approach\u003cbr\u003e\nis given by:\u003cbr\u003e\n2\u003cbr\u003e\nr\u003cbr\u003e\nS∑\u003cbr\u003e\ns=1\u003cbr\u003e\nNs · Cs2, (5)\u003cbr\u003e\nwhere r denotes the reduction ratio, S refers to the number\u003cbr\u003e\nof stages (a stage refers to the collection of blocks operating\u003cbr\u003e\non feature maps of a common spatial dimension), Cs de-\u003cbr\u003e\nnotes the dimension of the output channels and Ns denotes\u003cbr\u003e\nthe number of repeated blocks for stage s. SE-ResNet-50\u003cbr\u003e\nintroduces ∼2.5 million additional parameters beyond the\u003cbr\u003e\n∼25 million parameters required by ResNet-50, correspond-\u003cbr\u003e\ning to a ∼10% increase. In practice, the majority of these\u003cbr\u003e\nparameters come from the final stage of the network, where\u003cbr\u003e\nthe excitation operation is performed across the greatest\u003cbr\u003e\nnumber of channels. However, we found that this compara-\u003cbr\u003e\ntively costly final stage of SE blocks could be removed at\u003cbr\u003e\nonly a small cost in performance (\u0026lt;0.1% top-5 error on\u003cbr\u003e\nImageNet) reducing the relative parameter increase to ∼4%,\u003cbr\u003e\nwhich may prove useful in cases where parameter usage is\u003cbr\u003e\na key consideration (see Sec. 7.2 for further discussion).\u003c/p\u003e\n\u003cp data-sourcepos=\"772:1-778:123\"\u003e上記の内容に関して質問: (である調で記載)\u003cbr\u003e\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\u003cbr\u003e\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\u003cbr\u003e\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n⑤比較の記載があれば違いを\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003cbr\u003e\n⑥上記⑤以外で表に出来そうな部分があれば\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003c/p\u003e\n\u003cp data-sourcepos=\"780:1-782:274\"\u003e\u003cstrong\u003e▼生成AIの回答(ChatGPT-4o)\u003c/strong\u003e\u003cbr\u003e\n① \u003cstrong\u003e何についての記載か\u003c/strong\u003e\u003cbr\u003e\nこの節は、提案されたSEブロックのモデルおよび計算複雑性に関する記載である。SEブロックがどの程度の計算負荷とモデル複雑性を追加し、それが性能向上にどのように寄与するかについて述べている。\u003c/p\u003e\n\u003cp data-sourcepos=\"784:1-784:10\"\u003e\u003cstrong\u003e要旨\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"785:1-789:0\"\u003e\n\u003cli data-sourcepos=\"785:1-785:130\"\u003eSEブロックの追加による計算量の増加は僅かであるが、モデルの性能向上には大きく寄与する。\u003c/li\u003e\n\u003cli data-sourcepos=\"786:1-786:125\"\u003eSE-ResNet-50では、ResNet-50に対して約0.26%の計算量増加であり、ResNet-101と同等の精度に近づく。\u003c/li\u003e\n\u003cli data-sourcepos=\"787:1-787:131\"\u003eパラメータの増加は主に完全結合層から生じ、全体のモデル容量に対する影響は比較的小さい。\u003c/li\u003e\n\u003cli data-sourcepos=\"788:1-789:0\"\u003eCPUおよびGPUでの推論時間やトレーニング時間のオーバーヘッドは許容範囲内である。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"790:1-790:61\"\u003e② \u003cstrong\u003e改良点・工夫点・テクニック等の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"791:1-792:0\"\u003e\n\u003cli data-sourcepos=\"791:1-792:0\"\u003e完全結合層によるパラメータの増加があるが、特定のステージでのSEブロックを省略することで、精度にほとんど影響を与えずにパラメータ増加を約4%に抑えられる工夫が記載されている。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"793:1-793:37\"\u003e③ \u003cstrong\u003e性能が向上した記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"794:1-796:0\"\u003e\n\u003cli data-sourcepos=\"794:1-794:146\"\u003eSE-ResNet-50は、約0.26%の計算量増加でResNet-50よりも高い精度を達成しており、ResNet-101と同等の性能に近づく。\u003c/li\u003e\n\u003cli data-sourcepos=\"795:1-796:0\"\u003eResNet-50とSE-ResNet-50の比較において、推論時間はGPUで約19 msの増加、CPUで約3 msの増加が見られるが、精度向上に対して妥当なトレードオフであるとされている。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"797:1-797:73\"\u003e④ \u003cstrong\u003eメカニズムの解明・なぜそうなるのか等の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"798:1-799:0\"\u003e\n\u003cli data-sourcepos=\"798:1-799:0\"\u003eSEブロックが追加する計算負荷の大部分は完全結合層に起因しているが、これがモデルの性能向上に寄与するため、わずかなオーバーヘッドが許容される。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"800:1-801:84\"\u003e⑤ \u003cstrong\u003e比較の記載\u003c/strong\u003e\u003cbr\u003e\nResNet-50とSE-ResNet-50の違いについて表でまとめることができる。\u003c/p\u003e\n\u003ctable data-sourcepos=\"803:1-806:119\"\u003e\n\u003cthead\u003e\n\u003ctr data-sourcepos=\"803:1-803:127\"\u003e\n\u003cth data-sourcepos=\"803:2-803:21\"\u003eモデル\u003c/th\u003e\n\u003cth data-sourcepos=\"803:23-803:42\"\u003e計算量 (GFLOPs)\u003c/th\u003e\n\u003cth data-sourcepos=\"803:44-803:63\"\u003e推論時間 (GPU)\u003c/th\u003e\n\u003cth data-sourcepos=\"803:65-803:84\"\u003e推論時間 (CPU)\u003c/th\u003e\n\u003cth data-sourcepos=\"803:86-803:111\"\u003eパラメータ数増加\u003c/th\u003e\n\u003cth data-sourcepos=\"803:113-803:126\"\u003e精度向上\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr data-sourcepos=\"805:1-805:105\"\u003e\n\u003ctd data-sourcepos=\"805:2-805:17\"\u003eResNet-50\u003c/td\u003e\n\u003ctd data-sourcepos=\"805:19-805:38\"\u003e∼3.86\u003c/td\u003e\n\u003ctd data-sourcepos=\"805:40-805:56\"\u003e190 ms\u003c/td\u003e\n\u003ctd data-sourcepos=\"805:58-805:74\"\u003e164 ms\u003c/td\u003e\n\u003ctd data-sourcepos=\"805:76-805:93\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"805:95-805:104\"\u003e-\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"806:1-806:119\"\u003e\n\u003ctd data-sourcepos=\"806:2-806:17\"\u003eSE-ResNet-50\u003c/td\u003e\n\u003ctd data-sourcepos=\"806:19-806:38\"\u003e∼3.87 (+0.26%)\u003c/td\u003e\n\u003ctd data-sourcepos=\"806:40-806:56\"\u003e209 ms (+19 ms)\u003c/td\u003e\n\u003ctd data-sourcepos=\"806:58-806:74\"\u003e167 ms (+3 ms)\u003c/td\u003e\n\u003ctd data-sourcepos=\"806:76-806:96\"\u003e∼10% (約2.5M)\u003c/td\u003e\n\u003ctd data-sourcepos=\"806:98-806:118\"\u003eResNet-101に接近\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp data-sourcepos=\"808:1-809:6\"\u003e⑥ \u003cstrong\u003e上記⑤以外で表に出来そうな部分\u003c/strong\u003e\u003cbr\u003e\n無し\u003c/p\u003e\n\u003ch2 data-sourcepos=\"811:1-811:16\"\u003e\n\u003cspan id=\"5-experiments\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#5-experiments\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e5 EXPERIMENTS\u003c/h2\u003e\n\u003cp data-sourcepos=\"813:1-946:21\"\u003e\u003cstrong\u003e▼質問(人間)\u003c/strong\u003e\u003cbr\u003e\n(論文・記事の各章節を貼り付け)\u003cbr\u003e\n5 EXPERIMENTS\u003cbr\u003e\nIn this section, we conduct experiments to investigate the\u003cbr\u003e\neffectiveness of SE blocks across a range of tasks, datasets\u003cbr\u003e\nand model architectures.\u003cbr\u003e\n5.1 Image Classification\u003cbr\u003e\nTo evaluate the influence of SE blocks, we first perform\u003cbr\u003e\nexperiments on the ImageNet 2012 dataset [10] which\u003cbr\u003e\ncomprises 1.28 million training images and 50K validation\u003cbr\u003e\nimages from 1000 different classes. We train networks on\u003cbr\u003e\nthe training set and report the top-1 and top-5 error on the\u003cbr\u003e\nvalidation set.\u003cbr\u003e\nEach original network architecture and its correspond-\u003cbr\u003e\ning SE counterpart are trained with identical optimisa-\u003cbr\u003e\ntion schemes. We follow standard practices and perform\u003cbr\u003e\ndata augmentation with random cropping [5] to a size of\u003cbr\u003e\n224 × 224 pixels (or 299 × 299 for Inception-ResNet-v2 [21]\u003cbr\u003e\nand SE-Inception-ResNet-v2) and perform random horizon-\u003cbr\u003e\ntal flipping. Each input image is normalised through mean\u003cbr\u003e\nRGB-channel subtraction. We adopt the data balancing strat-\u003cbr\u003e\negy described in [67] for minibatch sampling. All models\u003cbr\u003e\nare trained on our distributed learning system ROCS which\u003cbr\u003e\nis designed to handle efficient parallel training of large\u003cbr\u003e\nnetworks. Optimisation is performed using synchronous\u003cbr\u003e\nSGD with momentum 0.9 and a minibatch size of 1024.\u003cbr\u003e\nThe initial learning rate is set to 0.6 and decreased by a\u003cbr\u003e\nfactor of 10 every 30 epochs. All models are trained for 100\u003cbr\u003e\nepochs from scratch, using the weight initialisation strategy\u003cbr\u003e\ndescribed in [68].\u003cbr\u003e\nWhen evaluating the models we apply centre-cropping\u003cbr\u003e\nso that 224 × 224 pixels are cropped from each image, after\u003cbr\u003e\nits shorter edge is first resized to 256 (299 × 299 from\u003cbr\u003e\neach image whose shorter edge is first resized to 352 for\u003cbr\u003e\nInception-ResNet-v2 and SE-Inception-ResNet-v2).\u003cbr\u003e\nNetwork depth. We begin by comparing SE-ResNet against\u003cbr\u003e\nResNet architectures with different depths and report the\u003cbr\u003e\nresults in Table 2. We observe that SE blocks consistently\u003cbr\u003e\nimprove performance across different depths with an ex-\u003cbr\u003e\ntremely small increase in computational complexity. Re-\u003cbr\u003e\nmarkably, SE-ResNet-50 achieves a single-crop top-5 valida-\u003cbr\u003e\ntion error of 6.62%, exceeding ResNet-50 (7.48%) by 0.86%\u003cbr\u003e\nand approaching the performance achieved by the much\u003cbr\u003e\ndeeper ResNet-101 network (6.52% top-5 error) with only\u003cbr\u003e\nhalf of the total computational burden (3.87 GFLOPs vs.\u003cbr\u003e\n7.58 GFLOPs). This pattern is repeated at greater depth,\u003cbr\u003e\nwhere SE-ResNet-101 (6.07% top-5 error) not only matches,\u003cbr\u003e\nbut outperforms the deeper ResNet-152 network (6.34%\u003cbr\u003e\ntop-5 error) by 0.27%. While it should be noted that the SE\u003cbr\u003e\nblocks themselves add depth, they do so in an extremely\u003cbr\u003e\ncomputationally efficient manner and yield good returns\u003cbr\u003e\neven at the point at which extending the depth of the base\u003cbr\u003e\narchitecture achieves diminishing returns. Moreover, we see\u003cbr\u003e\nthat the gains are consistent across a range of different\u003cbr\u003e\nnetwork depths, suggesting that the improvements induced\u003cbr\u003e\nby SE blocks may be complementary to those obtained by\u003cbr\u003e\nsimply increasing the depth of the base architecture.\u003cbr\u003e\nIntegration with modern architectures. We next study the\u003cbr\u003e\neffect of integrating SE blocks with two further state-of-\u003cbr\u003e\nthe-art architectures, Inception-ResNet-v2 [21] and ResNeXt\u003cbr\u003e\n(using the setting of 32 × 4d) [19], both of which introduce\u003cbr\u003e\nadditional computational building blocks into the base net-\u003cbr\u003e\nwork. We construct SENet equivalents of these networks,\u003cbr\u003e\nSE-Inception-ResNet-v2 and SE-ResNeXt (the configuration\u003cbr\u003e\nof SE-ResNeXt-50 is given in Table 1) and report results\u003cbr\u003e\nin Table 2. As with the previous experiments, we observe\u003cbr\u003e\nsignificant performance improvements induced by the in-\u003cbr\u003e\ntroduction of SE blocks into both architectures. In partic-\u003cbr\u003e\nular, SE-ResNeXt-50 has a top-5 error of 5.49% which is\u003cbr\u003e\nsuperior to both its direct counterpart ResNeXt-50 (5.90%\u003cbr\u003e\ntop-5 error) as well as the deeper ResNeXt-101 (5.57% top-5\u003cbr\u003e\nerror), a model which has almost twice the total number of\u003cbr\u003e\nparameters and computational overhead. We note a slight\u003cbr\u003e\ndifference in performance between our re-implementation\u003cbr\u003e\nof Inception-ResNet-v2 and the result reported in [21]. How-\u003cbr\u003e\never, we observe a similar trend with regard to the effect of\u003cbr\u003e\nSE blocks, finding that SE-Inception-ResNet-v2 (4.79% top-5\u003cbr\u003e\nerror) outperforms our reimplemented Inception-ResNet-v2\u003cbr\u003e\nbaseline (5.21% top-5 error) by 0.42% (a relative improve-\u003cbr\u003e\nment of 8.1%) as well as the reported result in [21]. The\u003cbr\u003e\ntraining curves for the baseline architectures ResNet-50,\u003cbr\u003e\nResNet-152, ResNeXt-50 and BN-Inception, and their re-\u003cbr\u003e\nspective SE counterparts are depicted in Fig. 4. We observe\u003cbr\u003e\nthat SE blocks yield a steady improvement throughout the\u003cbr\u003e\noptimisation procedure. Moreover, this trend is consistent\u003cbr\u003e\nacross each of the families of state-of-the-art architectures\u003cbr\u003e\nconsidered as baselines.\u003cbr\u003e\nWe also assess the effect of SE blocks when operating\u003cbr\u003e\non non-residual networks by conducting experiments with\u003cbr\u003e\nthe VGG-16 [11] and BN-Inception architecture [6]. To fa-\u003cbr\u003e\ncilitate the training of VGG-16 from scratch, we add Batch\u003cbr\u003e\nNormalization layers after each convolution. As with the\u003cbr\u003e\nprevious models, we use identical training schemes for both\u003cbr\u003e\nVGG-16 and SE-VGG-16. The results of the comparison are\u003cbr\u003e\nshown in Table 2. Similarly to the results reported for the\u003cbr\u003e\nresidual baseline architectures, we observe that SE blocks\u003cbr\u003e\nbring improvements in performance.\u003cbr\u003e\nMobile setting. Finally, we consider two representative\u003cbr\u003e\narchitectures from the class of mobile-optimised networks,\u003cbr\u003e\nMobileNet [65] and ShuffleNet [66]. For these experiments,\u003cbr\u003e\nwe used a minibatch size of 256 and a weight decay of\u003cbr\u003e\n4 × 10−5. We trained the models across 8 GPUs using SGD\u003cbr\u003e\nwith momentum (set to 0.9) and an initial learning rate\u003cbr\u003e\nof 0.1 which was reduced by a factor of 10 each time the\u003cbr\u003e\nvalidation loss plateaued (rather than using a fixed-length\u003cbr\u003e\nschedule). The total training process required ∼ 400 epochs\u003cbr\u003e\n(we found that this approach enabled us to reproduce the\u003cbr\u003e\nbaseline performance of [66]). The results reported in Table 3\u003cbr\u003e\nshow that SE blocks consistently improve the accuracy by a\u003cbr\u003e\nlarge margin at a minimal increase in computational cost.\u003cbr\u003e\nAdditional datasets. We next investigate whether the bene-\u003cbr\u003e\nfits of SE blocks generalise to datasets beyond ImageNet. We\u003cbr\u003e\nperform experiments with several popular baseline archi-\u003cbr\u003e\ntectures and techniques (ResNet-110 [14], ResNet-164 [14],\u003cbr\u003e\nWideResNet-16-8 [69], Shake-Shake [70] and Cutout [71]) on\u003cbr\u003e\nthe CIFAR-10 and CIFAR-100 datasets [73]. These comprise\u003cbr\u003e\na collection of 50k training and 10k test 32 × 32 pixel RGB\u003cbr\u003e\nimages, labelled with 10 and 100 classes respectively. The\u003cbr\u003e\nintegration of SE blocks into these networks follows the\u003cbr\u003e\nsame approach that was described in Sec. 3.3. Each baseline\u003cbr\u003e\nand its SENet counterpart are trained with a standard data\u003cbr\u003e\naugmentation strategy [24], [74]. During training, images\u003cbr\u003e\nare randomly horizontally flipped and zero-padded on each\u003cbr\u003e\nside with four pixels before taking a random 32 × 32 crop.\u003cbr\u003e\nMean and standard deviation normalisation is also applied.\u003cbr\u003e\nThe setting of the training strategy and other hyperparam-\u003cbr\u003e\neters (e.g. minibatch size, initial learning rate, number of\u003cbr\u003e\nepochs, weight decay) match those suggested by the authors\u003cbr\u003e\nof each model. We report the performance of each baseline\u003cbr\u003e\nand its SENet counterpart on CIFAR-10 in Table 4 and per-\u003cbr\u003e\nformance on CIFAR-100 in Table 5. We observe that in every\u003cbr\u003e\ncomparison SENets outperform the baseline architectures,\u003cbr\u003e\nsuggesting that the benefits of SE blocks are not confined to\u003cbr\u003e\nthe ImageNet dataset.\u003c/p\u003e\n\u003cp data-sourcepos=\"948:1-954:123\"\u003e上記の内容に関して質問: (である調で記載)\u003cbr\u003e\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\u003cbr\u003e\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\u003cbr\u003e\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n⑤比較の記載があれば違いを\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003cbr\u003e\n⑥上記⑤以外で表に出来そうな部分があれば\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003c/p\u003e\n\u003cp data-sourcepos=\"956:1-958:188\"\u003e\u003cstrong\u003e▼生成AIの回答(ChatGPT-4o)\u003c/strong\u003e\u003cbr\u003e\n① \u003cstrong\u003e何についての記載か\u003c/strong\u003e\u003cbr\u003e\nこの節は、SEブロックの有効性を様々なタスク、データセット、モデルアーキテクチャにわたって評価するための実験に関する記載である。\u003c/p\u003e\n\u003cp data-sourcepos=\"960:1-960:10\"\u003e\u003cstrong\u003e要旨\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"961:1-964:0\"\u003e\n\u003cli data-sourcepos=\"961:1-961:134\"\u003eImageNet 2012データセットを用いた画像分類タスクで、SEブロックがモデル性能に与える影響を評価。\u003c/li\u003e\n\u003cli data-sourcepos=\"962:1-962:130\"\u003eResNetをはじめとする各種モデルアーキテクチャにSEブロックを組み込んだ際の性能向上を検証。\u003c/li\u003e\n\u003cli data-sourcepos=\"963:1-964:0\"\u003eモバイル向けのアーキテクチャやCIFAR-10、CIFAR-100などの追加データセットに対しても、SEブロックの有効性を確認。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"965:1-965:61\"\u003e② \u003cstrong\u003e改良点・工夫点・テクニック等の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"966:1-968:0\"\u003e\n\u003cli data-sourcepos=\"966:1-966:127\"\u003eSEブロックを様々なアーキテクチャに統合し、それらの効果を評価する実験が行われている。\u003c/li\u003e\n\u003cli data-sourcepos=\"967:1-968:0\"\u003eモバイルネットワークでは、検証損失のプラトーに応じて学習率を調整する手法が使用されている。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"969:1-969:37\"\u003e③ \u003cstrong\u003e性能が向上した記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"970:1-974:0\"\u003e\n\u003cli data-sourcepos=\"970:1-970:149\"\u003eSE-ResNet-50は、ResNet-50に対してトップ5エラー率が7.48%から6.62%に改善（+0.86%）し、ResNet-101に迫る性能を示した。\u003c/li\u003e\n\u003cli data-sourcepos=\"971:1-971:139\"\u003eSE-ResNeXt-50は、ResNeXt-50に対してトップ5エラー率が5.90%から5.49%に改善し、ResNeXt-101（5.57%）を上回った。\u003c/li\u003e\n\u003cli data-sourcepos=\"972:1-972:120\"\u003eSE-Inception-ResNet-v2は、Inception-ResNet-v2に対してトップ5エラー率が5.21%から4.79%に改善した。\u003c/li\u003e\n\u003cli data-sourcepos=\"973:1-974:0\"\u003eCIFAR-10およびCIFAR-100においても、SEブロックを組み込んだモデルがベースラインを上回る性能を示した。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"975:1-975:73\"\u003e④ \u003cstrong\u003eメカニズムの解明・なぜそうなるのか等の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"976:1-977:0\"\u003e\n\u003cli data-sourcepos=\"976:1-977:0\"\u003eSEブロックがモデルの深さを増す代わりに、計算効率よく性能を向上させることが示されている。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"978:1-979:146\"\u003e⑤ \u003cstrong\u003e比較の記載\u003c/strong\u003e\u003cbr\u003e\nResNetやその他のモデルアーキテクチャにSEブロックを組み込んだ際の性能向上を表でまとめることができる。\u003c/p\u003e\n\u003ctable data-sourcepos=\"981:1-989:94\"\u003e\n\u003cthead\u003e\n\u003ctr data-sourcepos=\"981:1-981:123\"\u003e\n\u003cth data-sourcepos=\"981:2-981:27\"\u003eモデル\u003c/th\u003e\n\u003cth data-sourcepos=\"981:29-981:52\"\u003eトップ5エラー率\u003c/th\u003e\n\u003cth data-sourcepos=\"981:54-981:77\"\u003eトップ1エラー率\u003c/th\u003e\n\u003cth data-sourcepos=\"981:79-981:86\"\u003eGFLOPs\u003c/th\u003e\n\u003cth data-sourcepos=\"981:88-981:107\"\u003eパラメータ数\u003c/th\u003e\n\u003cth data-sourcepos=\"981:109-981:122\"\u003e比較対象\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr data-sourcepos=\"983:1-983:97\"\u003e\n\u003ctd data-sourcepos=\"983:2-983:23\"\u003eResNet-50\u003c/td\u003e\n\u003ctd data-sourcepos=\"983:25-983:40\"\u003e7.48%\u003c/td\u003e\n\u003ctd data-sourcepos=\"983:42-983:57\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"983:59-983:67\"\u003e3.86\u003c/td\u003e\n\u003ctd data-sourcepos=\"983:69-983:82\"\u003e25M\u003c/td\u003e\n\u003ctd data-sourcepos=\"983:84-983:96\"\u003e基準\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"984:1-984:94\"\u003e\n\u003ctd data-sourcepos=\"984:2-984:23\"\u003eSE-ResNet-50\u003c/td\u003e\n\u003ctd data-sourcepos=\"984:25-984:40\"\u003e6.62%\u003c/td\u003e\n\u003ctd data-sourcepos=\"984:42-984:57\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"984:59-984:67\"\u003e3.87\u003c/td\u003e\n\u003ctd data-sourcepos=\"984:69-984:82\"\u003e27.5M\u003c/td\u003e\n\u003ctd data-sourcepos=\"984:84-984:93\"\u003e+0.86%\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"985:1-985:94\"\u003e\n\u003ctd data-sourcepos=\"985:2-985:23\"\u003eResNet-101\u003c/td\u003e\n\u003ctd data-sourcepos=\"985:25-985:40\"\u003e6.52%\u003c/td\u003e\n\u003ctd data-sourcepos=\"985:42-985:57\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"985:59-985:67\"\u003e7.58\u003c/td\u003e\n\u003ctd data-sourcepos=\"985:69-985:82\"\u003e45M\u003c/td\u003e\n\u003ctd data-sourcepos=\"985:84-985:93\"\u003e-\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"986:1-986:94\"\u003e\n\u003ctd data-sourcepos=\"986:2-986:23\"\u003eSE-ResNeXt-50\u003c/td\u003e\n\u003ctd data-sourcepos=\"986:25-986:40\"\u003e5.49%\u003c/td\u003e\n\u003ctd data-sourcepos=\"986:42-986:57\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"986:59-986:67\"\u003e4.25\u003c/td\u003e\n\u003ctd data-sourcepos=\"986:69-986:82\"\u003e25M\u003c/td\u003e\n\u003ctd data-sourcepos=\"986:84-986:93\"\u003e+0.41%\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"987:1-987:94\"\u003e\n\u003ctd data-sourcepos=\"987:2-987:23\"\u003eResNeXt-101\u003c/td\u003e\n\u003ctd data-sourcepos=\"987:25-987:40\"\u003e5.57%\u003c/td\u003e\n\u003ctd data-sourcepos=\"987:42-987:57\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"987:59-987:67\"\u003e8.01\u003c/td\u003e\n\u003ctd data-sourcepos=\"987:69-987:82\"\u003e44M\u003c/td\u003e\n\u003ctd data-sourcepos=\"987:84-987:93\"\u003e-\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"988:1-988:97\"\u003e\n\u003ctd data-sourcepos=\"988:2-988:23\"\u003eInception-ResNet-v2\u003c/td\u003e\n\u003ctd data-sourcepos=\"988:25-988:40\"\u003e5.21%\u003c/td\u003e\n\u003ctd data-sourcepos=\"988:42-988:57\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"988:59-988:67\"\u003e13.0\u003c/td\u003e\n\u003ctd data-sourcepos=\"988:69-988:82\"\u003e55M\u003c/td\u003e\n\u003ctd data-sourcepos=\"988:84-988:96\"\u003e基準\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"989:1-989:94\"\u003e\n\u003ctd data-sourcepos=\"989:2-989:25\"\u003eSE-Inception-ResNet-v2\u003c/td\u003e\n\u003ctd data-sourcepos=\"989:27-989:40\"\u003e4.79%\u003c/td\u003e\n\u003ctd data-sourcepos=\"989:42-989:57\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"989:59-989:67\"\u003e13.2\u003c/td\u003e\n\u003ctd data-sourcepos=\"989:69-989:82\"\u003e56.5M\u003c/td\u003e\n\u003ctd data-sourcepos=\"989:84-989:93\"\u003e+0.42%\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp data-sourcepos=\"991:1-992:6\"\u003e⑥ \u003cstrong\u003e上記⑤以外で表に出来そうな部分\u003c/strong\u003e\u003cbr\u003e\n無し\u003c/p\u003e\n\u003ch2 data-sourcepos=\"994:1-994:27\"\u003e\n\u003cspan id=\"52-scene-classification\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#52-scene-classification\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e5.2 Scene Classification\u003c/h2\u003e\n\u003cp data-sourcepos=\"996:1-1071:36\"\u003e\u003cstrong\u003e▼質問(人間)\u003c/strong\u003e\u003cbr\u003e\n(論文・記事の各章節を貼り付け)\u003cbr\u003e\n5.2 Scene Classification\u003cbr\u003e\nWe next conduct experiments on the Places365-Challenge\u003cbr\u003e\ndataset [75] for scene classification. This dataset comprises\u003cbr\u003e\n8 million training images and 36, 500 validation images\u003cbr\u003e\nacross 365 categories. Relative to classification, the task of\u003cbr\u003e\nscene understanding offers an alternative assessment of a\u003cbr\u003e\nmodel’s ability to generalise well and handle abstraction.\u003cbr\u003e\nThis is because it often requires the model to handle more\u003cbr\u003e\ncomplex data associations and to be robust to a greater level\u003cbr\u003e\nof appearance variation.\u003cbr\u003e\nWe opted to use ResNet-152 as a strong baseline to\u003cbr\u003e\nassess the effectiveness of SE blocks and carefully follow\u003cbr\u003e\nthe training and evaluation protocols described in [72]. In\u003cbr\u003e\nthese experiments, all models are trained from scratch. We\u003cbr\u003e\nreport the results in Table 6, comparing also with prior\u003cbr\u003e\nwork. We observe that SE-ResNet-152 (11.01% top-5 error)\u003cbr\u003e\nachieves a lower validation error than ResNet-152 (11.61%\u003cbr\u003e\ntop-5 error), providing evidence that SE blocks can also yield\u003cbr\u003e\nimprovements for scene classification. This SENet surpasses\u003cbr\u003e\nthe previous state-of-the-art model Places-365-CNN [72]\u003cbr\u003e\nwhich has a top-5 error of 11.48% on this task.\u003cbr\u003e\n5.3 Object Detection on COCO\u003cbr\u003e\nWe further assess the generalisation of SE blocks on the\u003cbr\u003e\ntask of object detection using the COCO dataset [76] which\u003cbr\u003e\ncomprises 80k training images and 40k validation images,\u003cbr\u003e\nfollowing the splits used in [13]. We use the Faster R-CNN\u003cbr\u003e\n[4] detection framework as the basis for evaluating our\u003cbr\u003e\nmodels and follow the basic implementation described in\u003cbr\u003e\n[13]. Our goal is to evaluate the effect of replacing the trunk\u003cbr\u003e\narchitecture (ResNet) in the object detector with SE-ResNet,\u003cbr\u003e\nso that any changes in performance can be attributed\u003cbr\u003e\nto better representations. Table 7 reports the validation\u003cbr\u003e\nset performance of the object detector using ResNet-50,\u003cbr\u003e\nResNet-101 and their SE counterparts as trunk architectures.\u003cbr\u003e\nSE-ResNet-50 outperforms ResNet-50 by 1.3% (a relative\u003cbr\u003e\n5.2% improvement) on COCO’s standard AP metric and by\u003cbr\u003e\n1.6% on AP@IoU=0.5. SE blocks also bring improvements\u003cbr\u003e\non the deeper ResNet-101 architecture achieving a 0.7%\u003cbr\u003e\nimprovement (or 2.6% relative improvement) on the AP\u003cbr\u003e\nmetric.\u003cbr\u003e\nIn summary, this set of experiments demonstrate that the\u003cbr\u003e\nimprovements induced by SE blocks can be realised across\u003cbr\u003e\na broad range of architectures, tasks and datasets.\u003cbr\u003e\n5.4 ILSVRC 2017 Classification Competition\u003cbr\u003e\nSENets formed the foundation of our submission to the\u003cbr\u003e\nILSVRC competition where we achieved first place. Our\u003cbr\u003e\nwinning entry comprised a small ensemble of SENets that\u003cbr\u003e\nemployed a standard multi-scale and multi-crop fusion\u003cbr\u003e\nstrategy to obtain a top-5 error of 2.251% on the test set.\u003cbr\u003e\nAs part of this submission, we constructed an additional\u003cbr\u003e\nmodel, SENet-154, by integrating SE blocks with a modified\u003cbr\u003e\nResNeXt [19] (the details of the architecture are provided\u003cbr\u003e\nin Appendix). We compare this model with prior work on\u003cbr\u003e\nthe ImageNet validation set in Table 8 using standard crop\u003cbr\u003e\nsizes (224 × 224 and 320 × 320). We observe that SENet-154\u003cbr\u003e\nachieves a top-1 error of 18.68% and a top-5 error of 4.47%\u003cbr\u003e\nusing a 224 × 224 centre crop evaluation, which represents\u003cbr\u003e\nthe strongest reported result.\u003cbr\u003e\nFollowing the challenge there has been a great deal of\u003cbr\u003e\nfurther progress on the ImageNet benchmark. For compar-\u003cbr\u003e\nison, we include the strongest results that we are currently\u003cbr\u003e\naware of among the both published and unpublished lit-\u003cbr\u003e\nerature in Table 9. The best performance using only Im-\u003cbr\u003e\nageNet data was recently reported by [79]. This method\u003cbr\u003e\nuses reinforcement learning to develop new polices for\u003cbr\u003e\ndata augmentation during training to improve the perfor-\u003cbr\u003e\nmance of the architecture proposed by [31]. The best overall\u003cbr\u003e\nperformance was reported by [80] using a ResNeXt-101\u003cbr\u003e\n32×48d architecture. This was achieved by pretraining their\u003cbr\u003e\nmodel on approximately one billion weakly labelled images\u003cbr\u003e\nand finetuning on ImageNet. The improvements yielded by\u003cbr\u003e\nmore sophisticated data augmentation [79] and extensive\u003cbr\u003e\npretraining [80] may be complementary to our proposed\u003cbr\u003e\nchanges to the network architecture.\u003c/p\u003e\n\u003cp data-sourcepos=\"1073:1-1079:123\"\u003e上記の内容に関して質問: (である調で記載)\u003cbr\u003e\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\u003cbr\u003e\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\u003cbr\u003e\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n⑤比較の記載があれば違いを\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003cbr\u003e\n⑥上記⑤以外で表に出来そうな部分があれば\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003c/p\u003e\n\u003cp data-sourcepos=\"1081:1-1083:187\"\u003e\u003cstrong\u003e▼生成AIの回答(ChatGPT-4o)\u003c/strong\u003e\u003cbr\u003e\n① \u003cstrong\u003e何についての記載か\u003c/strong\u003e\u003cbr\u003e\nこの節は、SEブロックの有効性をシーン分類、物体検出、ILSVRC 2017分類競技などの異なるタスクにおいて評価する実験に関する記載である。\u003c/p\u003e\n\u003cp data-sourcepos=\"1085:1-1085:10\"\u003e\u003cstrong\u003e要旨\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1086:1-1089:0\"\u003e\n\u003cli data-sourcepos=\"1086:1-1086:116\"\u003ePlaces365-Challengeデータセットを用いて、シーン分類におけるSEブロックの効果を評価。\u003c/li\u003e\n\u003cli data-sourcepos=\"1087:1-1087:101\"\u003eCOCOデータセットを使用し、物体検出タスクでのSEブロックの効果を検証。\u003c/li\u003e\n\u003cli data-sourcepos=\"1088:1-1089:0\"\u003eILSVRC 2017の分類競技におけるSENetの成功とその結果を報告。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1090:1-1090:61\"\u003e② \u003cstrong\u003e改良点・工夫点・テクニック等の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1091:1-1094:0\"\u003e\n\u003cli data-sourcepos=\"1091:1-1091:105\"\u003eResNet-152を基準に、SEブロックを導入したSE-ResNet-152のシーン分類能力を評価。\u003c/li\u003e\n\u003cli data-sourcepos=\"1092:1-1092:128\"\u003eFaster R-CNNフレームワークで、トランクアーキテクチャをSE-ResNetに置き換え、その効果を評価。\u003c/li\u003e\n\u003cli data-sourcepos=\"1093:1-1094:0\"\u003eILSVRC 2017の分類競技では、マルチスケールおよびマルチクロップ融合戦略を使用したSENetのエントリが使用された。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1095:1-1095:37\"\u003e③ \u003cstrong\u003e性能が向上した記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1096:1-1100:0\"\u003e\n\u003cli data-sourcepos=\"1096:1-1096:98\"\u003eSE-ResNet-152は、ResNet-152に対してトップ5エラー率が11.61%から11.01%に改善。\u003c/li\u003e\n\u003cli data-sourcepos=\"1097:1-1097:134\"\u003eCOCOデータセットでの物体検出タスクでは、SE-ResNet-50がResNet-50に対してAPで1.3%、AP@IoU=0.5で1.6%向上。\u003c/li\u003e\n\u003cli data-sourcepos=\"1098:1-1098:51\"\u003eSE-ResNet-101もAPで0.7%の改善を示した。\u003c/li\u003e\n\u003cli data-sourcepos=\"1099:1-1100:0\"\u003eILSVRC 2017でSENet-154が224×224のセンタークロップ評価でトップ1エラー率18.68%、トップ5エラー率4.47%を達成。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1101:1-1101:73\"\u003e④ \u003cstrong\u003eメカニズムの解明・なぜそうなるのか等の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1102:1-1103:0\"\u003e\n\u003cli data-sourcepos=\"1102:1-1103:0\"\u003e無し\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1104:1-1105:48\"\u003e⑤ \u003cstrong\u003e比較の記載\u003c/strong\u003e\u003cbr\u003e\n表で違いをまとめることができる。\u003c/p\u003e\n\u003ctable data-sourcepos=\"1107:1-1115:115\"\u003e\n\u003cthead\u003e\n\u003ctr data-sourcepos=\"1107:1-1107:140\"\u003e\n\u003cth data-sourcepos=\"1107:2-1107:31\"\u003eタスク\u003c/th\u003e\n\u003cth data-sourcepos=\"1107:33-1107:53\"\u003eモデル\u003c/th\u003e\n\u003cth data-sourcepos=\"1107:55-1107:78\"\u003eトップ5エラー率\u003c/th\u003e\n\u003cth data-sourcepos=\"1107:80-1107:103\"\u003eトップ1エラー率\u003c/th\u003e\n\u003cth data-sourcepos=\"1107:105-1107:111\"\u003eAP\u003c/th\u003e\n\u003cth data-sourcepos=\"1107:113-1107:124\"\u003eAP@IoU=0.5\u003c/th\u003e\n\u003cth data-sourcepos=\"1107:126-1107:139\"\u003e比較対象\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr data-sourcepos=\"1109:1-1109:123\"\u003e\n\u003ctd data-sourcepos=\"1109:2-1109:33\"\u003eシーン分類\u003c/td\u003e\n\u003ctd data-sourcepos=\"1109:35-1109:52\"\u003eResNet-152\u003c/td\u003e\n\u003ctd data-sourcepos=\"1109:54-1109:69\"\u003e11.61%\u003c/td\u003e\n\u003ctd data-sourcepos=\"1109:71-1109:86\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"1109:88-1109:94\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"1109:96-1109:108\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"1109:110-1109:122\"\u003e基準\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"1110:1-1110:120\"\u003e\n\u003ctd data-sourcepos=\"1110:2-1110:33\"\u003eシーン分類\u003c/td\u003e\n\u003ctd data-sourcepos=\"1110:35-1110:52\"\u003eSE-ResNet-152\u003c/td\u003e\n\u003ctd data-sourcepos=\"1110:54-1110:69\"\u003e11.01%\u003c/td\u003e\n\u003ctd data-sourcepos=\"1110:71-1110:86\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"1110:88-1110:94\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"1110:96-1110:108\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"1110:110-1110:119\"\u003e+0.60%\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"1111:1-1111:122\"\u003e\n\u003ctd data-sourcepos=\"1111:2-1111:32\"\u003e物体検出 (COCO)\u003c/td\u003e\n\u003ctd data-sourcepos=\"1111:34-1111:51\"\u003eResNet-50\u003c/td\u003e\n\u003ctd data-sourcepos=\"1111:53-1111:68\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"1111:70-1111:85\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"1111:87-1111:93\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"1111:95-1111:107\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"1111:109-1111:121\"\u003e基準\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"1112:1-1112:119\"\u003e\n\u003ctd data-sourcepos=\"1112:2-1112:32\"\u003e物体検出 (COCO)\u003c/td\u003e\n\u003ctd data-sourcepos=\"1112:34-1112:51\"\u003eSE-ResNet-50\u003c/td\u003e\n\u003ctd data-sourcepos=\"1112:53-1112:68\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"1112:70-1112:85\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"1112:87-1112:93\"\u003e+1.3%\u003c/td\u003e\n\u003ctd data-sourcepos=\"1112:95-1112:107\"\u003e+1.6%\u003c/td\u003e\n\u003ctd data-sourcepos=\"1112:109-1112:118\"\u003e-\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"1113:1-1113:122\"\u003e\n\u003ctd data-sourcepos=\"1113:2-1113:32\"\u003e物体検出 (COCO)\u003c/td\u003e\n\u003ctd data-sourcepos=\"1113:34-1113:51\"\u003eResNet-101\u003c/td\u003e\n\u003ctd data-sourcepos=\"1113:53-1113:68\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"1113:70-1113:85\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"1113:87-1113:93\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"1113:95-1113:107\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"1113:109-1113:121\"\u003e基準\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"1114:1-1114:119\"\u003e\n\u003ctd data-sourcepos=\"1114:2-1114:32\"\u003e物体検出 (COCO)\u003c/td\u003e\n\u003ctd data-sourcepos=\"1114:34-1114:51\"\u003eSE-ResNet-101\u003c/td\u003e\n\u003ctd data-sourcepos=\"1114:53-1114:68\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"1114:70-1114:85\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"1114:87-1114:93\"\u003e+0.7%\u003c/td\u003e\n\u003ctd data-sourcepos=\"1114:95-1114:107\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"1114:109-1114:118\"\u003e-\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"1115:1-1115:115\"\u003e\n\u003ctd data-sourcepos=\"1115:2-1115:28\"\u003eILSVRC 2017\u003c/td\u003e\n\u003ctd data-sourcepos=\"1115:30-1115:47\"\u003eSENet-154\u003c/td\u003e\n\u003ctd data-sourcepos=\"1115:49-1115:64\"\u003e4.47%\u003c/td\u003e\n\u003ctd data-sourcepos=\"1115:66-1115:81\"\u003e18.68%\u003c/td\u003e\n\u003ctd data-sourcepos=\"1115:83-1115:89\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"1115:91-1115:103\"\u003e-\u003c/td\u003e\n\u003ctd data-sourcepos=\"1115:105-1115:114\"\u003e-\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp data-sourcepos=\"1117:1-1118:6\"\u003e⑥ \u003cstrong\u003e上記⑤以外で表に出来そうな部分\u003c/strong\u003e\u003cbr\u003e\n無し\u003c/p\u003e\n\u003ch2 data-sourcepos=\"1120:1-1120:19\"\u003e\n\u003cspan id=\"6-ablation-study\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#6-ablation-study\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e6 ABLATION STUDY\u003c/h2\u003e\n\u003cp data-sourcepos=\"1122:1-1151:48\"\u003e\u003cstrong\u003e▼質問(人間)\u003c/strong\u003e\u003cbr\u003e\n(論文・記事の各章節を貼り付け)\u003cbr\u003e\n6 ABLATION STUDY\u003cbr\u003e\nIn this section we conduct ablation experiments to gain\u003cbr\u003e\na better understanding of the relative importance of com-\u003cbr\u003e\nponents in the SE block design. All ablation experiments\u003cbr\u003e\nare performed on the ImageNet dataset on a single ma-\u003cbr\u003e\nchine (with 8 GPUs). ResNet-50 is used as the backbone\u003cbr\u003e\narchitecture. The data augmentation strategy follows the\u003cbr\u003e\napproach described in Sec. 5.1. To allow us to study the\u003cbr\u003e\nupper limit of performance for each variant, the learning\u003cbr\u003e\nrate is initialised to 0.1 and training continues until the\u003cbr\u003e\nvalidation loss plateaus (rather than continuing for a fixed\u003cbr\u003e\nnumber of epochs). The learning rate is then reduced by a\u003cbr\u003e\nfactor of 10 and then this process is repeated (three times in\u003cbr\u003e\ntotal).\u003cbr\u003e\n6.1 Reduction ratio\u003cbr\u003e\nThe reduction ratio r introduced in Eqn. 5 is a hyperpa-\u003cbr\u003e\nrameter which allows us to vary the capacity and compu-\u003cbr\u003e\ntational cost of the SE blocks in the network. To investigate\u003cbr\u003e\nthe trade-off between performance and computational cost\u003cbr\u003e\nmediated by this hyperparameter, we conduct experiments\u003cbr\u003e\nwith SE-ResNet-50 for a range of different r values. The\u003cbr\u003e\ncomparison in Table 10 shows that performance does not\u003cbr\u003e\nimprove monotonically with increased capacity, suggesting\u003cbr\u003e\nthat with enough weights the SE block is able to overfit\u003cbr\u003e\nto the channel interdependencies of the training set. We\u003cbr\u003e\nfound that setting r = 16 achieved a good balance between\u003cbr\u003e\naccuracy and complexity and consequently, we used this\u003cbr\u003e\nvalue for all experiments reported in this work.\u003c/p\u003e\n\u003cp data-sourcepos=\"1153:1-1159:123\"\u003e上記の内容に関して質問: (である調で記載)\u003cbr\u003e\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\u003cbr\u003e\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\u003cbr\u003e\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n⑤比較の記載があれば違いを\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003cbr\u003e\n⑥上記⑤以外で表に出来そうな部分があれば\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003c/p\u003e\n\u003cp data-sourcepos=\"1161:1-1163:173\"\u003e\u003cstrong\u003e▼生成AIの回答(ChatGPT-4o)\u003c/strong\u003e\u003cbr\u003e\n① \u003cstrong\u003e何についての記載か\u003c/strong\u003e\u003cbr\u003e\nこの節は、SEブロック設計の各コンポーネントの相対的重要性を理解するために行ったアブレーション実験に関する記載である。\u003c/p\u003e\n\u003cp data-sourcepos=\"1165:1-1165:10\"\u003e\u003cstrong\u003e要旨\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1166:1-1169:0\"\u003e\n\u003cli data-sourcepos=\"1166:1-1166:129\"\u003eImageNetデータセット上でのアブレーション実験を実施し、SEブロックの設計要素の影響を評価。\u003c/li\u003e\n\u003cli data-sourcepos=\"1167:1-1167:71\"\u003eResNet-50をバックボーンアーキテクチャとして使用。\u003c/li\u003e\n\u003cli data-sourcepos=\"1168:1-1169:0\"\u003e主に減少率( r )が性能と計算コストに与える影響を調査。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1170:1-1170:61\"\u003e② \u003cstrong\u003e改良点・工夫点・テクニック等の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1171:1-1172:0\"\u003e\n\u003cli data-sourcepos=\"1171:1-1172:0\"\u003eSEブロックのハイパーパラメータである減少率( r )の最適化を行い、性能と計算コストのバランスを取ることに成功した。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1173:1-1173:37\"\u003e③ \u003cstrong\u003e性能が向上した記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1174:1-1176:0\"\u003e\n\u003cli data-sourcepos=\"1174:1-1174:113\"\u003e減少率( r )を16に設定することで、性能と計算コストのバランスが最適化された。\u003c/li\u003e\n\u003cli data-sourcepos=\"1175:1-1176:0\"\u003e具体的な性能向上の値は無し。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1177:1-1177:73\"\u003e④ \u003cstrong\u003eメカニズムの解明・なぜそうなるのか等の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1178:1-1179:0\"\u003e\n\u003cli data-sourcepos=\"1178:1-1179:0\"\u003eSEブロックがトレーニングセットのチャンネル依存関係に過適合する可能性があるため、性能が単調に向上しないことが示唆されている。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1180:1-1181:73\"\u003e⑤ \u003cstrong\u003e比較の記載\u003c/strong\u003e\u003cbr\u003e\n以下に減少率( r )による性能の違いを表でまとめる。\u003c/p\u003e\n\u003ctable data-sourcepos=\"1183:1-1186:96\"\u003e\n\u003cthead\u003e\n\u003ctr data-sourcepos=\"1183:1-1183:54\"\u003e\n\u003cth data-sourcepos=\"1183:2-1183:20\"\u003e減少率 ( r )\u003c/th\u003e\n\u003cth data-sourcepos=\"1183:22-1183:38\"\u003e性能の変化\u003c/th\u003e\n\u003cth data-sourcepos=\"1183:40-1183:53\"\u003eコメント\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr data-sourcepos=\"1185:1-1185:88\"\u003e\n\u003ctd data-sourcepos=\"1185:2-1185:15\"\u003e( r = 16 )\u003c/td\u003e\n\u003ctd data-sourcepos=\"1185:17-1185:42\"\u003e最適化されている\u003c/td\u003e\n\u003ctd data-sourcepos=\"1185:44-1185:87\"\u003e精度と複雑性のバランスが良い\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"1186:1-1186:96\"\u003e\n\u003ctd data-sourcepos=\"1186:2-1186:20\"\u003eその他の値\u003c/td\u003e\n\u003ctd data-sourcepos=\"1186:22-1186:62\"\u003e性能が向上しない場合もある\u003c/td\u003e\n\u003ctd data-sourcepos=\"1186:64-1186:95\"\u003e過適合の可能性がある\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp data-sourcepos=\"1188:1-1189:6\"\u003e⑥ \u003cstrong\u003e上記⑤以外で表に出来そうな部分\u003c/strong\u003e\u003cbr\u003e\n無し\u003c/p\u003e\n\u003ch2 data-sourcepos=\"1191:1-1191:23\"\u003e\n\u003cspan id=\"62-squeeze-operator\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#62-squeeze-operator\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e6.2 Squeeze Operator\u003c/h2\u003e\n\u003cp data-sourcepos=\"1193:1-1225:51\"\u003e\u003cstrong\u003e▼質問(人間)\u003c/strong\u003e\u003cbr\u003e\n(論文・記事の各章節を貼り付け)\u003cbr\u003e\n6.2 Squeeze Operator\u003cbr\u003e\nWe examine the significance of using global average pooling\u003cbr\u003e\nas opposed to global max pooling as our choice of squeeze\u003cbr\u003e\noperator (since this worked well, we did not consider more\u003cbr\u003e\nsophisticated alternatives). The results are reported in Ta-\u003cbr\u003e\nble 11. While both max and average pooling are effective,\u003cbr\u003e\naverage pooling achieves slightly better performance, jus-\u003cbr\u003e\ntifying its selection as the basis of the squeeze operation.\u003cbr\u003e\nHowever, we note that the performance of SE blocks is fairly\u003cbr\u003e\nrobust to the choice of specific aggregation operator.\u003cbr\u003e\n6.3 Excitation Operator\u003cbr\u003e\nWe next assess the choice of non-linearity for the excitation\u003cbr\u003e\nmechanism. We consider two further options: ReLU and\u003cbr\u003e\ntanh, and experiment with replacing the sigmoid with these\u003cbr\u003e\nalternative non-linearities. The results are reported in Ta-\u003cbr\u003e\nble 12. We see that exchanging the sigmoid for tanh slightly\u003cbr\u003e\nworsens performance, while using ReLU is dramatically\u003cbr\u003e\nworse and in fact causes the performance of SE-ResNet-50\u003cbr\u003e\nto drop below that of the ResNet-50 baseline. This suggests\u003cbr\u003e\nthat for the SE block to be effective, careful construction of\u003cbr\u003e\nthe excitation operator is important.\u003cbr\u003e\n6.4 Different stages\u003cbr\u003e\nWe explore the influence of SE blocks at different stages by\u003cbr\u003e\nintegrating SE blocks into ResNet-50, one stage at a time.\u003cbr\u003e\nSpecifically, we add SE blocks to the intermediate stages:\u003cbr\u003e\nstage 2, stage 3 and stage 4, and report the results in Ta-\u003cbr\u003e\nble 13. We observe that SE blocks bring performance benefits\u003cbr\u003e\nwhen introduced at each of these stages of the architecture.\u003cbr\u003e\nMoreover, the gains induced by SE blocks at different stages\u003cbr\u003e\nare complementary, in the sense that they can be combined\u003cbr\u003e\neffectively to further bolster network performance.\u003c/p\u003e\n\u003cp data-sourcepos=\"1227:1-1233:123\"\u003e上記の内容に関して質問: (である調で記載)\u003cbr\u003e\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\u003cbr\u003e\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\u003cbr\u003e\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n⑤比較の記載があれば違いを\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003cbr\u003e\n⑥上記⑤以外で表に出来そうな部分があれば\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003c/p\u003e\n\u003cp data-sourcepos=\"1235:1-1237:137\"\u003e\u003cstrong\u003e▼生成AIの回答(ChatGPT-4o)\u003c/strong\u003e\u003cbr\u003e\n① \u003cstrong\u003e何についての記載か\u003c/strong\u003e\u003cbr\u003e\nこの節は、SEブロックの各要素に関する詳細なアブレーションスタディの結果について記載している。\u003c/p\u003e\n\u003cp data-sourcepos=\"1239:1-1239:10\"\u003e\u003cstrong\u003e要旨\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1240:1-1243:0\"\u003e\n\u003cli data-sourcepos=\"1240:1-1240:88\"\u003eGlobal average poolingとglobal max poolingの比較（Squeeze Operatorの選択）。\u003c/li\u003e\n\u003cli data-sourcepos=\"1241:1-1241:96\"\u003eExcitation Operatorとしての非線形性の選択肢（sigmoid、ReLU、tanh）の評価。\u003c/li\u003e\n\u003cli data-sourcepos=\"1242:1-1243:0\"\u003eSEブロックをResNet-50の異なるステージに導入した場合の影響の調査。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1244:1-1244:61\"\u003e② \u003cstrong\u003e改良点・工夫点・テクニック等の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1245:1-1247:0\"\u003e\n\u003cli data-sourcepos=\"1245:1-1245:36\"\u003eGlobal average poolingの採用。\u003c/li\u003e\n\u003cli data-sourcepos=\"1246:1-1247:0\"\u003eSigmoidをExcitation Operatorとして選択。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1248:1-1248:37\"\u003e③ \u003cstrong\u003e性能が向上した記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1249:1-1252:0\"\u003e\n\u003cli data-sourcepos=\"1249:1-1249:93\"\u003eGlobal average poolingがglobal max poolingよりもわずかに優れた性能を示す。\u003c/li\u003e\n\u003cli data-sourcepos=\"1250:1-1250:105\"\u003eSigmoidが最も効果的なExcitation Operatorであり、tanhやReLUと比較して性能が向上。\u003c/li\u003e\n\u003cli data-sourcepos=\"1251:1-1252:0\"\u003eSEブロックをResNet-50の各ステージに導入することで、どのステージでも性能向上が見られる。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1253:1-1253:73\"\u003e④ \u003cstrong\u003eメカニズムの解明・なぜそうなるのか等の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1254:1-1255:0\"\u003e\n\u003cli data-sourcepos=\"1254:1-1255:0\"\u003eSEブロックは、squeezeとexcitationの各段階での適切な選択により、ネットワークの性能を効果的に強化できることが示されている。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1256:1-1256:23\"\u003e⑤ \u003cstrong\u003e比較の記載\u003c/strong\u003e\u003c/p\u003e\n\u003ctable data-sourcepos=\"1258:1-1262:139\"\u003e\n\u003cthead\u003e\n\u003ctr data-sourcepos=\"1258:1-1258:116\"\u003e\n\u003cth data-sourcepos=\"1258:2-1258:39\"\u003e比較項目\u003c/th\u003e\n\u003cth data-sourcepos=\"1258:41-1258:73\"\u003e結果\u003c/th\u003e\n\u003cth data-sourcepos=\"1258:75-1258:115\"\u003eコメント\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr data-sourcepos=\"1260:1-1260:117\"\u003e\n\u003ctd data-sourcepos=\"1260:2-1260:33\"\u003eSqueeze Operator\u003c/td\u003e\n\u003ctd data-sourcepos=\"1260:35-1260:66\"\u003eAverage poolingが最良\u003c/td\u003e\n\u003ctd data-sourcepos=\"1260:68-1260:116\"\u003eMax poolingも有効だが、やや劣る\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"1261:1-1261:122\"\u003e\n\u003ctd data-sourcepos=\"1261:2-1261:33\"\u003eExcitation Operator\u003c/td\u003e\n\u003ctd data-sourcepos=\"1261:35-1261:67\"\u003eSigmoidが最良\u003c/td\u003e\n\u003ctd data-sourcepos=\"1261:69-1261:121\"\u003eTanhはわずかに悪化、ReLUは著しく悪化\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"1262:1-1262:139\"\u003e\n\u003ctd data-sourcepos=\"1262:2-1262:45\"\u003eSEブロックの導入ステージ\u003c/td\u003e\n\u003ctd data-sourcepos=\"1262:47-1262:86\"\u003e各ステージで性能向上\u003c/td\u003e\n\u003ctd data-sourcepos=\"1262:88-1262:138\"\u003eどのステージでも導入が有効\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp data-sourcepos=\"1264:1-1265:6\"\u003e⑥ \u003cstrong\u003e上記⑤以外で表に出来そうな部分\u003c/strong\u003e\u003cbr\u003e\n無し\u003c/p\u003e\n\u003ch2 data-sourcepos=\"1267:1-1267:27\"\u003e\n\u003cspan id=\"65-integration-strategy\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#65-integration-strategy\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e6.5 Integration strategy\u003c/h2\u003e\n\u003cp data-sourcepos=\"1269:1-1299:55\"\u003e\u003cstrong\u003e▼質問(人間)\u003c/strong\u003e\u003cbr\u003e\n(論文・記事の各章節を貼り付け)\u003cbr\u003e\n6.5 Integration strategy\u003cbr\u003e\nFinally, we perform an ablation study to assess the influence\u003cbr\u003e\nof the location of the SE block when integrating it into exist-\u003cbr\u003e\ning architectures. In addition to the proposed SE design, we\u003cbr\u003e\nconsider three variants: (1) SE-PRE block, in which the SE\u003cbr\u003e\nblock is moved before the residual unit; (2) SE-POST block,\u003cbr\u003e\nin which the SE unit is moved after the summation with the\u003cbr\u003e\nidentity branch and (3) SE-Identity block, in which the SE\u003cbr\u003e\nunit is placed on the identity connection in parallel to the\u003cbr\u003e\nresidual unit. These variants are illustrated in Figure 5 and\u003cbr\u003e\nthe performance of each variant is reported in Table 14. We\u003cbr\u003e\nobserve that the SE-PRE, SE-Identity and proposed SE block\u003cbr\u003e\neach perform similarly well, while usage of the SE-POST\u003cbr\u003e\nblock leads to a drop in performance. This experiment\u003cbr\u003e\nsuggests that the performance improvements produced by\u003cbr\u003e\nSE units are fairly robust to their location, provided that\u003cbr\u003e\nthey are applied prior to branch aggregation.\u003cbr\u003e\nIn the experiments above, each SE block was placed\u003cbr\u003e\noutside the structure of a residual unit. We also construct\u003cbr\u003e\na variant of the design which moves the SE block inside\u003cbr\u003e\nthe residual unit, placing it directly after the 3 × 3 convo-\u003cbr\u003e\nlutional layer. Since the 3 × 3 convolutional layer possesses\u003cbr\u003e\nfewer channels, the number of parameters introduced by the\u003cbr\u003e\ncorresponding SE block is also reduced. The comparison in\u003cbr\u003e\nTable 15 shows that the SE 3×3 variant achieves comparable\u003cbr\u003e\nclassification accuracy with fewer parameters than the stan-\u003cbr\u003e\ndard SE block. Although it is beyond the scope of this work,\u003cbr\u003e\nwe anticipate that further efficiency gains will be achievable\u003cbr\u003e\nby tailoring SE block usage for specific architectures.\u003c/p\u003e\n\u003cp data-sourcepos=\"1301:1-1307:123\"\u003e上記の内容に関して質問: (である調で記載)\u003cbr\u003e\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\u003cbr\u003e\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\u003cbr\u003e\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n⑤比較の記載があれば違いを\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003cbr\u003e\n⑥上記⑤以外で表に出来そうな部分があれば\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003c/p\u003e\n\u003cp data-sourcepos=\"1309:1-1311:185\"\u003e\u003cstrong\u003e▼生成AIの回答(ChatGPT-4o)\u003c/strong\u003e\u003cbr\u003e\n① \u003cstrong\u003e何についての記載か\u003c/strong\u003e\u003cbr\u003e\nこの節は、SEブロックを既存のアーキテクチャに統合する際の位置による影響を評価したアブレーションスタディについて記載している。\u003c/p\u003e\n\u003cp data-sourcepos=\"1313:1-1313:10\"\u003e\u003cstrong\u003e要旨\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1314:1-1318:0\"\u003e\n\u003cli data-sourcepos=\"1314:1-1314:110\"\u003eSEブロックの位置を変更した4つのバリアント（SE-PRE、SE-POST、SE-Identity）を検討。\u003c/li\u003e\n\u003cli data-sourcepos=\"1315:1-1315:126\"\u003e各バリアントの性能を比較し、SE-PRE、SE-Identity、提案されたSEブロックは良好な性能を示す。\u003c/li\u003e\n\u003cli data-sourcepos=\"1316:1-1316:81\"\u003eSE-POSTブロックは性能低下を引き起こすことが確認された。\u003c/li\u003e\n\u003cli data-sourcepos=\"1317:1-1318:0\"\u003eSEブロックを3×3の畳み込み層の後に配置するバリアントが紹介され、標準のSEブロックに比べてパラメータ数が減少しながらも同等の分類精度を達成することが示された。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1319:1-1319:61\"\u003e② \u003cstrong\u003e改良点・工夫点・テクニック等の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1320:1-1322:0\"\u003e\n\u003cli data-sourcepos=\"1320:1-1320:127\"\u003eSEブロックを残差ユニットの前後や並行に配置することで、性能に与える影響を検証した点。\u003c/li\u003e\n\u003cli data-sourcepos=\"1321:1-1322:0\"\u003eSEブロックを3×3の畳み込み層内に配置することで、パラメータ数を削減しつつ性能を維持する設計が提案された。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1323:1-1323:37\"\u003e③ \u003cstrong\u003e性能が向上した記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1324:1-1325:0\"\u003e\n\u003cli data-sourcepos=\"1324:1-1325:0\"\u003eSEブロックを3×3の畳み込み層の後に配置するバリアントが、パラメータ数を減少させながらも標準のSEブロックと同等の分類精度を達成したが、具体的な性能数値は示されていない。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1326:1-1326:73\"\u003e④ \u003cstrong\u003eメカニズムの解明・なぜそうなるのか等の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1327:1-1329:0\"\u003e\n\u003cli data-sourcepos=\"1327:1-1327:183\"\u003eSEユニットの位置が残差ユニットの集約前であれば性能向上が見られ、これはSEユニットが情報の流れを妨げないことを示唆している。\u003c/li\u003e\n\u003cli data-sourcepos=\"1328:1-1329:0\"\u003eSEブロックを3×3の畳み込み層内に配置することで、パラメータ数を削減できる一方、性能を維持できることが示されている。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1330:1-1330:23\"\u003e⑤ \u003cstrong\u003e比較の記載\u003c/strong\u003e\u003c/p\u003e\n\u003ctable data-sourcepos=\"1332:1-1337:115\"\u003e\n\u003cthead\u003e\n\u003ctr data-sourcepos=\"1332:1-1332:115\"\u003e\n\u003cth data-sourcepos=\"1332:2-1332:39\"\u003eSEブロックの位置\u003c/th\u003e\n\u003cth data-sourcepos=\"1332:41-1332:73\"\u003e性能\u003c/th\u003e\n\u003cth data-sourcepos=\"1332:75-1332:114\"\u003eコメント\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr data-sourcepos=\"1334:1-1334:118\"\u003e\n\u003ctd data-sourcepos=\"1334:2-1334:29\"\u003eSE-PRE\u003c/td\u003e\n\u003ctd data-sourcepos=\"1334:31-1334:65\"\u003e良好な性能\u003c/td\u003e\n\u003ctd data-sourcepos=\"1334:67-1334:117\"\u003e提案されたSEブロックと同様の性能\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"1335:1-1335:117\"\u003e\n\u003ctd data-sourcepos=\"1335:2-1335:29\"\u003eSE-POST\u003c/td\u003e\n\u003ctd data-sourcepos=\"1335:31-1335:63\"\u003e性能低下\u003c/td\u003e\n\u003ctd data-sourcepos=\"1335:65-1335:116\"\u003e情報の流れに悪影響を与える可能性\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"1336:1-1336:118\"\u003e\n\u003ctd data-sourcepos=\"1336:2-1336:29\"\u003eSE-Identity\u003c/td\u003e\n\u003ctd data-sourcepos=\"1336:31-1336:65\"\u003e良好な性能\u003c/td\u003e\n\u003ctd data-sourcepos=\"1336:67-1336:117\"\u003e提案されたSEブロックと同様の性能\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"1337:1-1337:115\"\u003e\n\u003ctd data-sourcepos=\"1337:2-1337:29\"\u003eSE 3×3\u003c/td\u003e\n\u003ctd data-sourcepos=\"1337:31-1337:66\"\u003e同等の分類精度\u003c/td\u003e\n\u003ctd data-sourcepos=\"1337:68-1337:114\"\u003eパラメータ数が少なくなる\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp data-sourcepos=\"1339:1-1340:6\"\u003e⑥ \u003cstrong\u003e上記⑤以外で表に出来そうな部分があれば\u003c/strong\u003e表で\u003cstrong\u003eまとめて下さい。\u003c/strong\u003e\u003cbr\u003e\n無し\u003c/p\u003e\n\u003ch2 data-sourcepos=\"1342:1-1342:22\"\u003e\n\u003cspan id=\"7-role-of-se-blocks\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#7-role-of-se-blocks\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e7 ROLE OF SE BLOCKS\u003c/h2\u003e\n\u003cp data-sourcepos=\"1344:1-1379:20\"\u003e\u003cstrong\u003e▼質問(人間)\u003c/strong\u003e\u003cbr\u003e\n(論文・記事の各章節を貼り付け)\u003cbr\u003e\n7 ROLE OF SE BLOCKS\u003cbr\u003e\nAlthough the proposed SE block has been shown to improve\u003cbr\u003e\nnetwork performance on multiple visual tasks, we would\u003cbr\u003e\nalso like to understand the role of the squeeze operation and\u003cbr\u003e\nhow the excitation mechanism operates in practice. Unfortu-\u003cbr\u003e\nnately, a rigorous theoretical analysis of the representations\u003cbr\u003e\nlearned by deep neural networks remains challenging. We\u003cbr\u003e\ntherefore take an empirical approach to examining the role\u003cbr\u003e\nplayed by the SE block with the goal of attaining at least a\u003cbr\u003e\nprimitive understanding of its practical function.\u003cbr\u003e\n7.1 Effect of Squeeze\u003cbr\u003e\nTo assess whether the global embedding produced by the\u003cbr\u003e\nsqueeze operation plays an important role in performance,\u003cbr\u003e\nwe experiment with a variant of the SE block that adds an\u003cbr\u003e\nequal number of parameters, but does not perform global\u003cbr\u003e\naverage pooling. Specifically, we remove the pooling op-\u003cbr\u003e\neration and replace the two FC layers with corresponding\u003cbr\u003e\n1 × 1 convolutions with identical channel dimensions in\u003cbr\u003e\nthe excitation operator, namely NoSqueeze, where the ex-\u003cbr\u003e\ncitation output maintains the spatial dimensions as input.\u003cbr\u003e\nIn contrast to the SE block, these point-wise convolutions\u003cbr\u003e\ncan only remap the channels as a function of the output\u003cbr\u003e\nof a local operator. While in practice, the later layers of a\u003cbr\u003e\ndeep network will typically possess a (theoretical) global\u003cbr\u003e\nreceptive field, global embeddings are no longer directly\u003cbr\u003e\naccessible throughout the network in the NoSqueeze variant.\u003cbr\u003e\nThe accuracy and computational complexity of the both\u003cbr\u003e\nmodels are compared to a standard ResNet-50 model in\u003cbr\u003e\nTable 16. We observe that the use of global information has\u003cbr\u003e\na significant influence on the model performance, underlin-\u003cbr\u003e\ning the importance of the squeeze operation. Moreover, in\u003cbr\u003e\ncomparison to the NoSqueeze design, the SE block allows\u003cbr\u003e\nthis global information to be used in a computationally\u003cbr\u003e\nparsimonious manner.\u003c/p\u003e\n\u003cp data-sourcepos=\"1381:1-1387:123\"\u003e上記の内容に関して質問: (である調で記載)\u003cbr\u003e\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\u003cbr\u003e\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\u003cbr\u003e\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n⑤比較の記載があれば違いを\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003cbr\u003e\n⑥上記⑤以外で表に出来そうな部分があれば\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003c/p\u003e\n\u003cp data-sourcepos=\"1389:1-1391:203\"\u003e\u003cstrong\u003e▼生成AIの回答(ChatGPT-4o)\u003c/strong\u003e\u003cbr\u003e\n① \u003cstrong\u003e何についての記載か\u003c/strong\u003e\u003cbr\u003e\nこの節は、SEブロックの役割、特にスクイーズ操作とエキサイテーションメカニズムが実際にどのように機能しているかについての考察を行っている。\u003c/p\u003e\n\u003cp data-sourcepos=\"1393:1-1393:10\"\u003e\u003cstrong\u003e要旨\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1394:1-1397:0\"\u003e\n\u003cli data-sourcepos=\"1394:1-1394:161\"\u003eスクイーズ操作が性能に及ぼす影響を評価するため、スクイーズ操作を除いたNoSqueezeバリアントと比較実験を行った。\u003c/li\u003e\n\u003cli data-sourcepos=\"1395:1-1395:98\"\u003eグローバル情報がモデルの性能に重要な影響を与えていることを確認。\u003c/li\u003e\n\u003cli data-sourcepos=\"1396:1-1397:0\"\u003eSEブロックは、計算効率を維持しながらグローバル情報を利用できる設計であることを示している。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1398:1-1398:61\"\u003e② \u003cstrong\u003e改良点・工夫点・テクニック等の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1399:1-1400:0\"\u003e\n\u003cli data-sourcepos=\"1399:1-1400:0\"\u003eスクイーズ操作を排除したNoSqueezeバリアントを提案し、グローバル情報の有無による性能の違いを検証した。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1401:1-1401:37\"\u003e③ \u003cstrong\u003e性能が向上した記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1402:1-1403:0\"\u003e\n\u003cli data-sourcepos=\"1402:1-1403:0\"\u003eスクイーズ操作を含むSEブロックは、NoSqueezeバリアントと比較して、性能が向上しているが、具体的な数値は示されていない。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1404:1-1404:73\"\u003e④ \u003cstrong\u003eメカニズムの解明・なぜそうなるのか等の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1405:1-1406:0\"\u003e\n\u003cli data-sourcepos=\"1405:1-1406:0\"\u003eスクイーズ操作によるグローバル情報が、ネットワーク全体で重要な役割を果たしており、その結果、モデルの性能が向上することが示されている。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1407:1-1407:23\"\u003e⑤ \u003cstrong\u003e比較の記載\u003c/strong\u003e\u003c/p\u003e\n\u003ctable data-sourcepos=\"1409:1-1413:78\"\u003e\n\u003cthead\u003e\n\u003ctr data-sourcepos=\"1409:1-1409:88\"\u003e\n\u003cth data-sourcepos=\"1409:2-1409:22\"\u003eモデル\u003c/th\u003e\n\u003cth data-sourcepos=\"1409:24-1409:55\"\u003eグローバル情報の使用\u003c/th\u003e\n\u003cth data-sourcepos=\"1409:57-1409:73\"\u003e計算効率\u003c/th\u003e\n\u003cth data-sourcepos=\"1409:75-1409:87\"\u003e性能\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr data-sourcepos=\"1411:1-1411:79\"\u003e\n\u003ctd data-sourcepos=\"1411:2-1411:20\"\u003e標準ResNet-50\u003c/td\u003e\n\u003ctd data-sourcepos=\"1411:22-1411:45\"\u003eなし\u003c/td\u003e\n\u003ctd data-sourcepos=\"1411:47-1411:61\"\u003e高い\u003c/td\u003e\n\u003ctd data-sourcepos=\"1411:63-1411:78\"\u003e基準性能\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"1412:1-1412:73\"\u003e\n\u003ctd data-sourcepos=\"1412:2-1412:18\"\u003eNoSqueeze\u003c/td\u003e\n\u003ctd data-sourcepos=\"1412:20-1412:43\"\u003eなし\u003c/td\u003e\n\u003ctd data-sourcepos=\"1412:45-1412:58\"\u003e低い\u003c/td\u003e\n\u003ctd data-sourcepos=\"1412:60-1412:72\"\u003e低下\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"1413:1-1413:78\"\u003e\n\u003ctd data-sourcepos=\"1413:2-1413:23\"\u003eSEブロック\u003c/td\u003e\n\u003ctd data-sourcepos=\"1413:25-1413:48\"\u003eあり\u003c/td\u003e\n\u003ctd data-sourcepos=\"1413:50-1413:63\"\u003e高い\u003c/td\u003e\n\u003ctd data-sourcepos=\"1413:65-1413:77\"\u003e向上\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp data-sourcepos=\"1415:1-1416:6\"\u003e⑥ \u003cstrong\u003e上記⑤以外で表に出来そうな部分があれば\u003c/strong\u003e表で\u003cstrong\u003eまとめて下さい。\u003c/strong\u003e\u003cbr\u003e\n無し\u003c/p\u003e\n\u003ch2 data-sourcepos=\"1418:1-1418:25\"\u003e\n\u003cspan id=\"72-role-of-excitation\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#72-role-of-excitation\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e7.2 Role of Excitation\u003c/h2\u003e\n\u003cp data-sourcepos=\"1420:1-1484:37\"\u003e\u003cstrong\u003e▼質問(人間)\u003c/strong\u003e\u003cbr\u003e\n(論文・記事の各章節を貼り付け)\u003cbr\u003e\n7.2 Role of Excitation\u003cbr\u003e\nTo provide a clearer picture of the function of the excitation\u003cbr\u003e\noperator in SE blocks, in this section we study example\u003cbr\u003e\nactivations from the SE-ResNet-50 model and examine their\u003cbr\u003e\ndistribution with respect to different classes and different\u003cbr\u003e\ninput images at various depths in the network. In particular,\u003cbr\u003e\nwe would like to understand how excitations vary across\u003cbr\u003e\nimages of different classes, and across images within a class.\u003cbr\u003e\nWe first consider the distribution of excitations for dif-\u003cbr\u003e\nferent classes. Specifically, we sample four classes from the\u003cbr\u003e\nImageNet dataset that exhibit semantic and appearance di-\u003cbr\u003e\nversity, namely goldfish, pug, plane and cliff (example images\u003cbr\u003e\nfrom these classes are shown in Figure 6). We then draw\u003cbr\u003e\nfifty samples for each class from the validation set and\u003cbr\u003e\ncompute the average activations for fifty uniformly sampled\u003cbr\u003e\nchannels in the last SE block of each stage (immediately\u003cbr\u003e\nprior to downsampling) and plot their distribution in Fig. 7.\u003cbr\u003e\nFor reference, we also plot the distribution of the mean\u003cbr\u003e\nactivations across all of the 1000 classes.\u003cbr\u003e\nWe make the following three observations about the\u003cbr\u003e\nrole of the excitation operation. First, the distribution across\u003cbr\u003e\ndifferent classes is very similar at the earlier layers of the\u003cbr\u003e\nnetwork, e.g. SE 2 3. This suggests that the importance of\u003cbr\u003e\nfeature channels is likely to be shared by different classes\u003cbr\u003e\nin the early stages. The second observation is that at greater\u003cbr\u003e\ndepth, the value of each channel becomes much more class-\u003cbr\u003e\nspecific as different classes exhibit different preferences to\u003cbr\u003e\nthe discriminative value of features, e.g. SE 4 6 and SE 5 1.\u003cbr\u003e\nThese observations are consistent with findings in previous\u003cbr\u003e\nwork [81], [82], namely that earlier layer features are typi-\u003cbr\u003e\ncally more general (e.g. class agnostic in the context of the\u003cbr\u003e\nclassification task) while later layer features exhibit greater\u003cbr\u003e\nlevels of specificity [83].\u003cbr\u003e\nNext, we observe a somewhat different phenomena in\u003cbr\u003e\nthe last stage of the network. SE 5 2 exhibits an interesting\u003cbr\u003e\ntendency towards a saturated state in which most of the\u003cbr\u003e\nactivations are close to one and the remainder is close to\u003cbr\u003e\nzero. At the point at which all activations take the value one,\u003cbr\u003e\nan SE block reduces to the identity operator. At the end of\u003cbr\u003e\nthe network in the SE 5 3 (which is immediately followed\u003cbr\u003e\nby global pooling prior before classifiers), a similar pattern\u003cbr\u003e\nemerges over different classes, up to a slight change in scale\u003cbr\u003e\n(which could be tuned by the classifiers). This suggests that\u003cbr\u003e\nSE 5 2 and SE 5 3 are less important than previous blocks\u003cbr\u003e\nin providing recalibration to the network. This finding is\u003cbr\u003e\nconsistent with the result of the empirical investigation in\u003cbr\u003e\nSec. 4 which demonstrated that the additional parameter\u003cbr\u003e\ncount could be significantly reduced by removing the SE\u003cbr\u003e\nblocks for the last stage with only a marginal loss of perfor-\u003cbr\u003e\nmance.\u003cbr\u003e\nFinally, we show the mean and standard deviations of\u003cbr\u003e\nthe activations for image instances within the same class for\u003cbr\u003e\ntwo sample classes (goldfish and plane) in Fig. 8. We observe a\u003cbr\u003e\ntrend consistent with the inter-class visualisation, indicating\u003cbr\u003e\nthat the dynamic behaviour of SE blocks varies over both\u003cbr\u003e\nclasses and instances within a class. Particularly in the later\u003cbr\u003e\nlayers of the network where there is considerable diversity\u003cbr\u003e\nof representation within a single class, the network learns\u003cbr\u003e\nto take advantage of feature recalibration to improve its\u003cbr\u003e\ndiscriminative performance. In summary, SE blocks produce\u003cbr\u003e\ninstance-specific responses which nevertheless function to\u003cbr\u003e\nsupport the increasingly class-specific needs of the model at\u003cbr\u003e\ndifferent layers in the architecture.\u003c/p\u003e\n\u003cp data-sourcepos=\"1486:1-1492:123\"\u003e上記の内容に関して質問: (である調で記載)\u003cbr\u003e\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\u003cbr\u003e\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\u003cbr\u003e\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n⑤比較の記載があれば違いを\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003cbr\u003e\n⑥上記⑤以外で表に出来そうな部分があれば\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003c/p\u003e\n\u003cp data-sourcepos=\"1494:1-1496:204\"\u003e\u003cstrong\u003e▼生成AIの回答(ChatGPT-4o)\u003c/strong\u003e\u003cbr\u003e\n① \u003cstrong\u003e何についての記載か\u003c/strong\u003e\u003cbr\u003e\nこの節は、SEブロックにおけるエキサイテーション（Excitation）操作の役割を解明し、その機能をより明確にするための実験とその結果を説明している。\u003c/p\u003e\n\u003cp data-sourcepos=\"1498:1-1498:10\"\u003e\u003cstrong\u003e要旨\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1499:1-1503:0\"\u003e\n\u003cli data-sourcepos=\"1499:1-1499:157\"\u003eSEブロックのエキサイテーション操作が異なるクラスや異なる入力画像に対してどのように分布しているかを調査。\u003c/li\u003e\n\u003cli data-sourcepos=\"1500:1-1500:98\"\u003eネットワークの深さによるエキサイテーションのクラス依存性を検討。\u003c/li\u003e\n\u003cli data-sourcepos=\"1501:1-1501:134\"\u003eネットワークの最後の段階では、エキサイテーションが飽和状態に近づく傾向があることを観察。\u003c/li\u003e\n\u003cli data-sourcepos=\"1502:1-1503:0\"\u003eSEブロックがクラス固有の要求を満たすためにインスタンスごとの応答を生成していることを確認。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1504:1-1504:61\"\u003e② \u003cstrong\u003e改良点・工夫点・テクニック等の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1505:1-1506:0\"\u003e\n\u003cli data-sourcepos=\"1505:1-1506:0\"\u003eクラス間およびクラス内でのエキサイテーション分布を比較するための具体的な実験を実施し、エキサイテーション操作の重要性を検証した。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1507:1-1507:37\"\u003e③ \u003cstrong\u003e性能が向上した記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1508:1-1509:0\"\u003e\n\u003cli data-sourcepos=\"1508:1-1509:0\"\u003e性能の向上に関する具体的な数値は記載されていないが、エキサイテーション操作がネットワークの識別性能を改善する役割を果たしていることが示唆されている。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1510:1-1510:73\"\u003e④ \u003cstrong\u003eメカニズムの解明・なぜそうなるのか等の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1511:1-1513:0\"\u003e\n\u003cli data-sourcepos=\"1511:1-1511:254\"\u003eネットワークの初期層では、異なるクラス間でのエキサイテーション分布が類似しているが、後半の層になるとクラスごとにエキサイテーションの値が特異的になることが示されている。\u003c/li\u003e\n\u003cli data-sourcepos=\"1512:1-1513:0\"\u003e最後の段階では、エキサイテーションが飽和状態に近づき、SEブロックがアイデンティティ演算子として機能する傾向がある。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1514:1-1514:23\"\u003e⑤ \u003cstrong\u003e比較の記載\u003c/strong\u003e\u003c/p\u003e\n\u003ctable data-sourcepos=\"1516:1-1520:131\"\u003e\n\u003cthead\u003e\n\u003ctr data-sourcepos=\"1516:1-1516:127\"\u003e\n\u003cth data-sourcepos=\"1516:2-1516:24\"\u003eネットワーク層\u003c/th\u003e\n\u003cth data-sourcepos=\"1516:26-1516:79\"\u003eエキサイテーションの分布特性\u003c/th\u003e\n\u003cth data-sourcepos=\"1516:81-1516:101\"\u003eクラス依存性\u003c/th\u003e\n\u003cth data-sourcepos=\"1516:103-1516:126\"\u003e重要度\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr data-sourcepos=\"1518:1-1518:126\"\u003e\n\u003ctd data-sourcepos=\"1518:2-1518:21\"\u003e初期層 (SE 2 3)\u003c/td\u003e\n\u003ctd data-sourcepos=\"1518:23-1518:77\"\u003e異なるクラス間で類似した分布\u003c/td\u003e\n\u003ctd data-sourcepos=\"1518:79-1518:94\"\u003e低い\u003c/td\u003e\n\u003ctd data-sourcepos=\"1518:96-1518:125\"\u003e一般的な特徴を抽出\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"1519:1-1519:143\"\u003e\n\u003ctd data-sourcepos=\"1519:2-1519:29\"\u003e中間層 (SE 4 6, SE 5 1)\u003c/td\u003e\n\u003ctd data-sourcepos=\"1519:31-1519:89\"\u003eクラスごとに特異的なエキサイテーション\u003c/td\u003e\n\u003ctd data-sourcepos=\"1519:91-1519:106\"\u003e高い\u003c/td\u003e\n\u003ctd data-sourcepos=\"1519:108-1519:142\"\u003eクラス固有の特徴を強調\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"1520:1-1520:131\"\u003e\n\u003ctd data-sourcepos=\"1520:2-1520:29\"\u003e最終層 (SE 5 2, SE 5 3)\u003c/td\u003e\n\u003ctd data-sourcepos=\"1520:31-1520:85\"\u003eエキサイテーションが飽和に近づく\u003c/td\u003e\n\u003ctd data-sourcepos=\"1520:87-1520:102\"\u003e低い\u003c/td\u003e\n\u003ctd data-sourcepos=\"1520:104-1520:130\"\u003e重要度は低い\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp data-sourcepos=\"1522:1-1523:6\"\u003e⑥ \u003cstrong\u003e上記⑤以外で表に出来そうな部分があれば\u003c/strong\u003e表で\u003cstrong\u003eまとめて下さい。\u003c/strong\u003e\u003cbr\u003e\n無し\u003c/p\u003e\n\u003ch2 data-sourcepos=\"1525:1-1525:15\"\u003e\n\u003cspan id=\"8-conclusion\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#8-conclusion\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e8 CONCLUSION\u003c/h2\u003e\n\u003cp data-sourcepos=\"1527:1-1542:12\"\u003e\u003cstrong\u003e▼質問(人間)\u003c/strong\u003e\u003cbr\u003e\n(論文・記事の各章節を貼り付け)\u003cbr\u003e\n8 CONCLUSION\u003cbr\u003e\nIn this paper we proposed the SE block, an architectural\u003cbr\u003e\nunit designed to improve the representational power of a\u003cbr\u003e\nnetwork by enabling it to perform dynamic channel-wise\u003cbr\u003e\nfeature recalibration. A wide range of experiments show\u003cbr\u003e\nthe effectiveness of SENets, which achieve state-of-the-art\u003cbr\u003e\nperformance across multiple datasets and tasks. In addition,\u003cbr\u003e\nSE blocks shed some light on the inability of previous\u003cbr\u003e\narchitectures to adequately model channel-wise feature de-\u003cbr\u003e\npendencies. We hope this insight may prove useful for other\u003cbr\u003e\ntasks requiring strong discriminative features. Finally, the\u003cbr\u003e\nfeature importance values produced by SE blocks may be\u003cbr\u003e\nof use for other tasks such as network pruning for model\u003cbr\u003e\ncompression.\u003c/p\u003e\n\u003cp data-sourcepos=\"1544:1-1550:123\"\u003e上記の内容に関して質問: (である調で記載)\u003cbr\u003e\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\u003cbr\u003e\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\u003cbr\u003e\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n⑤比較の記載があれば違いを\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003cbr\u003e\n⑥上記⑤以外で表に出来そうな部分があれば\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003c/p\u003e\n\u003cp data-sourcepos=\"1552:1-1554:132\"\u003e\u003cstrong\u003e▼生成AIの回答(ChatGPT-4o)\u003c/strong\u003e\u003cbr\u003e\n① \u003cstrong\u003e何についての記載か\u003c/strong\u003e\u003cbr\u003e\nこの節は、SEブロック（Squeeze-and-Excitation Block）の提案とその効果についての結論をまとめている。\u003c/p\u003e\n\u003cp data-sourcepos=\"1556:1-1556:10\"\u003e\u003cstrong\u003e要旨\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1557:1-1561:0\"\u003e\n\u003cli data-sourcepos=\"1557:1-1557:199\"\u003eSEブロックは、チャネルごとの特徴再キャリブレーションを動的に行うことで、ネットワークの表現力を向上させることを目的として設計された。\u003c/li\u003e\n\u003cli data-sourcepos=\"1558:1-1558:146\"\u003e多岐にわたる実験により、SENetsが複数のデータセットやタスクで最先端の性能を達成することが示された。\u003c/li\u003e\n\u003cli data-sourcepos=\"1559:1-1559:166\"\u003eSEブロックは、従来のアーキテクチャがチャネルごとの特徴依存関係を十分にモデル化できていないことを明らかにした。\u003c/li\u003e\n\u003cli data-sourcepos=\"1560:1-1561:0\"\u003eSEブロックによって生成される特徴の重要度は、モデル圧縮のためのネットワークプルーニングなど、他のタスクにも有用である可能性がある。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1562:1-1562:61\"\u003e② \u003cstrong\u003e改良点・工夫点・テクニック等の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1563:1-1564:0\"\u003e\n\u003cli data-sourcepos=\"1563:1-1564:0\"\u003eSEブロックの設計自体が改良点であり、動的チャネルワイズ特徴再キャリブレーションという新しいアプローチを導入している。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1565:1-1565:37\"\u003e③ \u003cstrong\u003e性能が向上した記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1566:1-1567:0\"\u003e\n\u003cli data-sourcepos=\"1566:1-1567:0\"\u003eSENetsは、複数のデータセットとタスクにおいて最先端の性能を達成したと記載されているが、具体的な数値は示されていない。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1568:1-1568:73\"\u003e④ \u003cstrong\u003eメカニズムの解明・なぜそうなるのか等の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1569:1-1570:0\"\u003e\n\u003cli data-sourcepos=\"1569:1-1570:0\"\u003eSEブロックは、従来のアーキテクチャがチャネルごとの特徴依存関係を十分にモデル化できていなかったことを明らかにし、その問題を解決するためのアプローチを提供している。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1571:1-1571:25\"\u003e⑤ \u003cstrong\u003e比較の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1572:1-1573:0\"\u003e\n\u003cli data-sourcepos=\"1572:1-1573:0\"\u003e無し\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1574:1-1574:101\"\u003e⑥ \u003cstrong\u003e上記⑤以外で表に出来そうな部分があれば\u003c/strong\u003e表で\u003cstrong\u003eまとめて下さい。\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1575:1-1576:0\"\u003e\n\u003cli data-sourcepos=\"1575:1-1576:0\"\u003e無し\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 data-sourcepos=\"1577:1-1577:33\"\u003e\n\u003cspan id=\"appendix-details-of-senet-154\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#appendix-details-of-senet-154\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eAPPENDIX: DETAILS OF SENET-154\u003c/h2\u003e\n\u003cp data-sourcepos=\"1579:1-1602:55\"\u003e\u003cstrong\u003e▼質問(人間)\u003c/strong\u003e\u003cbr\u003e\n(論文・記事の各章節を貼り付け)\u003cbr\u003e\nAPPENDIX: DETAILS OF SENET-154\u003cbr\u003e\nSENet-154 is constructed by incorporating SE blocks into a\u003cbr\u003e\nmodified version of the 64×4d ResNeXt-152 which extends\u003cbr\u003e\nthe original ResNeXt-101 [19] by adopting the block stack-\u003cbr\u003e\ning strategy of ResNet-152 [13]. Further differences to the\u003cbr\u003e\ndesign and training of this model (beyond the use of SE\u003cbr\u003e\nblocks) are as follows: (a) The number of the first 1 × 1\u003cbr\u003e\nconvolutional channels for each bottleneck building block\u003cbr\u003e\nwas halved to reduce the computational cost of the model\u003cbr\u003e\nwith a minimal decrease in performance. (b) The first 7 × 7\u003cbr\u003e\nconvolutional layer was replaced with three consecutive\u003cbr\u003e\n3 × 3 convolutional layers. (c) The 1 × 1 down-sampling\u003cbr\u003e\nprojection with stride-2 convolution was replaced with a\u003cbr\u003e\n3 × 3 stride-2 convolution to preserve information. (d) A\u003cbr\u003e\ndropout layer (with a dropout ratio of 0.2) was inserted\u003cbr\u003e\nbefore the classification layer to reduce overfitting. (e) Label-\u003cbr\u003e\nsmoothing regularisation (as introduced in [20]) was used\u003cbr\u003e\nduring training. (f) The parameters of all BN layers were\u003cbr\u003e\nfrozen for the last few training epochs to ensure consistency\u003cbr\u003e\nbetween training and testing. (g) Training was performed\u003cbr\u003e\nwith 8 servers (64 GPUs) in parallel to enable large batch\u003cbr\u003e\nsizes (2048). The initial learning rate was set to 1.0.\u003c/p\u003e\n\u003cp data-sourcepos=\"1604:1-1610:123\"\u003e上記の内容に関して質問: (である調で記載)\u003cbr\u003e\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\u003cbr\u003e\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\u003cbr\u003e\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\u003cbr\u003e\n⑤比較の記載があれば違いを\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003cbr\u003e\n⑥上記⑤以外で表に出来そうな部分があれば\u003cstrong\u003e表で\u003c/strong\u003eまとめて下さい。(無ければ無しと記載)\u003c/p\u003e\n\u003cp data-sourcepos=\"1612:1-1614:138\"\u003e\u003cstrong\u003e▼生成AIの回答(ChatGPT-4o)\u003c/strong\u003e\u003cbr\u003e\n① \u003cstrong\u003e何についての記載か\u003c/strong\u003e\u003cbr\u003e\nこの節は、SENet-154という特定のネットワークアーキテクチャの設計とその詳細について記載している。\u003c/p\u003e\n\u003cp data-sourcepos=\"1616:1-1616:10\"\u003e\u003cstrong\u003e要旨\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1617:1-1620:0\"\u003e\n\u003cli data-sourcepos=\"1617:1-1617:136\"\u003eSENet-154は、64×4d ResNeXt-152をベースに、SEブロックを組み込んだネットワークアーキテクチャである。\u003c/li\u003e\n\u003cli data-sourcepos=\"1618:1-1618:101\"\u003eResNeXt-101を拡張し、ResNet-152のブロックスタッキング戦略を採用している。\u003c/li\u003e\n\u003cli data-sourcepos=\"1619:1-1620:0\"\u003eモデルの設計とトレーニングにおいて、いくつかの改良と変更が行われている。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1621:1-1621:59\"\u003e② \u003cstrong\u003e改良点・工夫点・テクニック等の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1622:1-1629:0\"\u003e\n\u003cli data-sourcepos=\"1622:1-1622:256\"\u003e\n\u003cstrong\u003e1 × 1 畳み込みチャネルの減少\u003c/strong\u003e: ボトルネックビルディングブロックの最初の1 × 1 畳み込みチャネル数を半分にし、計算コストを削減しつつ性能の低下を最小限に抑える工夫をしている。\u003c/li\u003e\n\u003cli data-sourcepos=\"1623:1-1623:148\"\u003e\n\u003cstrong\u003e畳み込みレイヤーの変更\u003c/strong\u003e: 最初の7 × 7 畳み込み層を3 × 3 畳み込み層に変更し、情報の保持を図っている。\u003c/li\u003e\n\u003cli data-sourcepos=\"1624:1-1624:243\"\u003e\n\u003cstrong\u003eダウンサンプリングプロジェクションの変更\u003c/strong\u003e: ストライド2の1 × 1 ダウンサンプリングプロジェクションを、3 × 3 のストライド2畳み込みに置き換え、情報の保持を図っている。\u003c/li\u003e\n\u003cli data-sourcepos=\"1625:1-1625:170\"\u003e\n\u003cstrong\u003eドロップアウト層の追加\u003c/strong\u003e: 過学習を防ぐために、分類層の前にドロップアウト層（ドロップアウト率0.2）を追加している。\u003c/li\u003e\n\u003cli data-sourcepos=\"1626:1-1626:137\"\u003e\n\u003cstrong\u003eラベルスムージング正則化の使用\u003c/strong\u003e: トレーニング中にラベルスムージング正則化を使用している。\u003c/li\u003e\n\u003cli data-sourcepos=\"1627:1-1627:264\"\u003e\n\u003cstrong\u003eバッチノーマライゼーション（BN）パラメータの固定\u003c/strong\u003e: 最後のトレーニングエポックでは、トレーニングとテストの一貫性を確保するために、すべてのBNレイヤーのパラメータを固定している。\u003c/li\u003e\n\u003cli data-sourcepos=\"1628:1-1629:0\"\u003e\n\u003cstrong\u003e並列トレーニング\u003c/strong\u003e: 大規模なバッチサイズ（2048）を可能にするために、8台のサーバー（64 GPU）を使用して並列トレーニングを行っている。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1630:1-1630:35\"\u003e③ \u003cstrong\u003e性能が向上した記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1631:1-1632:0\"\u003e\n\u003cli data-sourcepos=\"1631:1-1632:0\"\u003e無し\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1633:1-1633:71\"\u003e④ \u003cstrong\u003eメカニズムの解明・なぜそうなるのか等の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1634:1-1635:0\"\u003e\n\u003cli data-sourcepos=\"1634:1-1635:0\"\u003e無し\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1636:1-1636:23\"\u003e⑤ \u003cstrong\u003e比較の記載\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1637:1-1638:0\"\u003e\n\u003cli data-sourcepos=\"1637:1-1638:0\"\u003e無し\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"1639:1-1639:53\"\u003e⑥ \u003cstrong\u003e上記⑤以外で表に出来そうな部分\u003c/strong\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"1640:1-1642:0\"\u003e\n\u003cli data-sourcepos=\"1640:1-1642:0\"\u003e無し\u003c/li\u003e\n\u003c/ul\u003e\n",
    "body": "# はじめに\n\n生成AIを用いてSE blockの論文「Squeeze-and-Excitation Networks」の内容を(なるべく)把握してみました。\n\n以降で、ChatGPTに聞いてみた例を記載します。\n\n## 同類の記事\n\nhttps://qiita.com/dl_from_scratch/items/f6e6a27fc4752e28dd5a\n\nhttps://qiita.com/dl_from_scratch/items/42196ffd7740ed44209f\n\n## 対象の論文\n\n論文: (SE blockに関する論文)\n\n[1709.01507] Squeeze-and-Excitation Networks\nhttps://arxiv.org/abs/1709.01507\n(PDF: https://arxiv.org/pdf/1709.01507v3)\n\n## 質問時の各章節の区切り部分\n\n論文の中にある各章節を、下記のように区切って、部分毎に生成AIに内容を質問していきます。\n\n* Abstract\n* \\---\n* 1 INTRODUCTION\n* \\---\n* 2 RELATED WORK\n* \\---\n* 3 SQUEEZE-AND-EXCITATION BLOCKS\n* \\---\n* 3.1 Squeeze: Global Information Embedding\n* \\---\n* 3.2 Excitation: Adaptive Recalibration\n* \\---\n* 3.3 Instantiations\n* \\---\n* 4 MODEL AND COMPUTATIONAL COMPLEXITY\n* \\---\n* 5 EXPERIMENTS\n* 5.1 Image Classification\n* \\---\n* 5.2 Scene Classification\n* 5.3 Object Detection on COCO\n* 5.4 ILSVRC 2017 Classification Competition\n* \\---\n* 6 ABLATION STUDY\n* 6.1 Reduction ratio\n* \\---\n* 6.2 Squeeze Operator\n* 6.3 Excitation Operator\n* 6.4 Different stages\n* \\---\n* 6.5 Integration strategy\n* \\---\n* 7 ROLE OF SE BLOCKS\n* 7.1 Effect of Squeeze\n* \\---\n* 7.2 Role of Excitation\n* \\---\n* 8 CONCLUSION\n* \\---\n* APPENDIX: DETAILS OF SENET-154\n\n## 生成AIへの質問方法\n\n生成AIを活用して、知りたい記事・論文の1節分(適度な長さ)のテキストをコピー＆ペーストして、その下に**質問内容を「①～ ②～ …」と番号付きで書いて**、生成AIに渡せば、全質問に一発で的確に回答してくれるので、非常に良好でした。記事全体を読む必要なく、知りたい点の情報だけを収集できます。\n\n**生成AIへの質問例:**\n```\n(論文・記事の各章節を貼り付け)\n\n上記の内容に関して質問: (である調で記載)\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\n⑤比較の記載があれば違いを**表で**まとめて下さい。(無ければ無しと記載)\n⑥上記⑤以外で表に出来そうな部分があれば**表で**まとめて下さい。(無ければ無しと記載)\n```\n\n質問内容は、記事・論文を読んでいて**いつも知りたいと思う点**(**改良点・工夫点・テクニック・メカニズムの解明**)にしています。また、表で比較した方が素早く把握できるので、可能であれば**記事を表に変換する**ようにしています。\n\n論文・記事を貼り付けるテキストの長さは、**1節分程度の量に**とどめた方が、良い回答が得られました。生成AIの回答の文量が多くなってくると、回答が長くなり過ぎないように、生成AIが勝手に(適度に)端折り始めてしまい、重要な点が回答から抜けてしまう可能性が高まります。\n\n## 注意点\n\n論文のテキスト内容だけを貼り付けて、生成AIに質問しています。論文の中の図・表の部分は貼り付けていません。図・表の内容は生成AIの回答には含まれず、別途論文を見る必要があります。\n\n# 生成AIに質問\n\n以降で、ChatGPTに実際に聞いてみた例を記載します。\n\n## 生成AIへの質問＆回答の全容\n\n生成AIへの質問＆回答の全容・詳細:\n\nhttps://chatgpt.com/share/77aea68d-52c6-499a-a07b-760431628034\n\n## Abstract\n\n**▼質問(人間)**\n(論文・記事の各章節を貼り付け)\nAbstract—The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to\nconstruct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad\nrange of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of\na CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel\nrelationship and propose a novel architectural unit, which we term the “Squeeze-and-Excitation” (SE) block, that adaptively recalibrates\nchannel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be\nstacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate\nthat SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at minimal additional computational\ncost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and\nreduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ∼25%.\n\n上記の内容に関して質問: (である調で記載)\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\n⑤比較の記載があれば違いを**表で**まとめて下さい。(無ければ無しと記載)\n⑥上記⑤以外で表に出来そうな部分があれば**表で**まとめて下さい。(無ければ無しと記載)\n\n**▼生成AIの回答(ChatGPT-4o)**\n① **何についての記載か**  \nこの論文は、畳み込みニューラルネットワーク (CNN) のチャネル間の依存関係を明示的にモデル化する新しいアーキテクチャユニット「Squeeze-and-Excitation (SE) ブロック」について述べている。\n\n**要旨**\n- CNNのチャネル間の依存関係を再校正するSEブロックを提案。\n- SEブロックは、CNNの表現力を向上させるための新しい手法である。\n- SEブロックは、既存の最先端CNNにおいて、わずかな追加の計算コストで大幅な性能向上をもたらす。\n- ILSVRC 2017分類コンペで1位を獲得し、top-5エラー率を2.251%に削減。\n\n② **改良点・工夫点・テクニック等の記載**  \nこの論文では、チャネル間の依存関係を明示的にモデル化し、チャネルごとの特徴応答を適応的に再校正するSEブロックを提案している。これは、従来のCNNが空間的な情報に重点を置いていたのに対し、チャネル情報の重要性を強調するものである。\n\n③ **性能が向上した記載**  \nILSVRC 2017分類コンペで、提案されたSENetアーキテクチャはtop-5エラー率を2.251%にまで削減し、2016年の勝利エントリを相対的に約25%上回ったとされる。\n\n④ **メカニズムの解明・なぜそうなるのか等の記載**  \n無し\n\n⑤ **比較の記載**  \n無し\n\n⑥ **上記⑤以外で表に出来そうな部分**  \n無し\n\n## 1 INTRODUCTION\n\n**▼質問(人間)**\n(論文・記事の各章節を貼り付け)\n1 INTRODUCTION\nCONVOLUTIONAL neural networks (CNNs) have proven\nto be useful models for tackling a wide range of\nvisual tasks [1]–[4]. At each convolutional layer in the\nnetwork, a collection of filters expresses neighbourhood\nspatial connectivity patterns along input channels—fusing\nspatial and channel-wise information together within local\nreceptive fields. By interleaving a series of convolutional\nlayers with non-linear activation functions and downsam-\npling operators, CNNs are able to produce robust represen-\ntations that capture hierarchical patterns and attain global\ntheoretical receptive fields. Recent research has shown that\nthese representations can be strengthened by integrating\nlearning mechanisms into the network that help capture\nspatial correlations between features. One such approach,\npopularised by the Inception family of architectures [5], [6],\nincorporates multi-scale processes into network modules to\nachieve improved performance. Further work has sought to\nbetter model spatial dependencies [7], [8] and incorporate\nspatial attention into the structure of the network [9].\nIn this paper, we investigate a different aspect of network\ndesign - the relationship between channels. We introduce\na new architectural unit, which we term the Squeeze-and-\nExcitation (SE) block, with the goal of improving the quality\nof representations produced by a network by explicitly\nmodelling the interdependencies between the channels of\nits convolutional features. To this end, we propose a mecha-\nnism that allows the network to perform feature recalibration,\nthrough which it can learn to use global information to\nselectively emphasise informative features and suppress less\nuseful ones.\nThe structure of the SE building block is depicted in\nFig. 1. For any given transformation Ftr : X 7 → U,\nX ∈ RH′×W ′×C′\n, U ∈ RH×W ×C , (e.g. a convolution), we\ncan construct a corresponding SE block to perform feature\nrecalibration. The features U are first passed through a\nsqueeze operation, which produces a channel descriptor by\naggregating feature maps across their spatial dimensions\n(H × W ). The function of this descriptor is to produce\nan embedding of the global distribution of channel-wise\nfeature responses, allowing information from the global\nreceptive field of the network to be used by all its layers. The\naggregation is followed by an excitation operation, which\ntakes the form of a simple self-gating mechanism that takes\nthe embedding as input and produces a collection of per-\nchannel modulation weights. These weights are applied to\nthe feature maps U to generate the output of the SE block\nwhich can be fed directly into subsequent layers of the\nnetwork.\nIt is possible to construct an SE network (SENet) by\nsimply stacking a collection of SE blocks. Moreover, these\nSE blocks can also be used as a drop-in replacement for\nthe original block at a range of depths in the network\narchitecture (Sec. 6.4). While the template for the building\nblock is generic, the role it performs at different depths\ndiffers throughout the network. In earlier layers, it excites\ninformative features in a class-agnostic manner, strengthen-\ning the shared low-level representations. In later layers, the\nSE blocks become increasingly specialised, and respond to\ndifferent inputs in a highly class-specific manner (Sec. 7.2).\nAs a consequence, the benefits of the feature recalibration\nperformed by SE blocks can be accumulated through the\nnetwork.\nThe design and development of new CNN architectures\nis a difficult engineering task, typically requiring the se-\nlection of many new hyperparameters and layer configura-\ntions. By contrast, the structure of the SE block is simple and\ncan be used directly in existing state-of-the-art architectures\nby replacing components with their SE counterparts, where\nthe performance can be effectively enhanced. SE blocks are\nalso computationally lightweight and impose only a slight\nincrease in model complexity and computational burden.\nTo provide evidence for these claims, in Sec. 4 we de-\nvelop several SENets and conduct an extensive evaluation\non the ImageNet 2012 dataset [10]. We also present results\nbeyond ImageNet that indicate that the benefits of our\napproach are not restricted to a specific dataset or task.\nBy making use of SENets, we ranked first in the ILSVRC\n2017 classification competition. Our best model ensemble\nachieves a 2.251% top-5 error on the test set1. This repre-\nsents roughly a 25% relative improvement when compared\nto the winner entry of the previous year (top-5 error of\n2.991%).\n\n上記の内容に関して質問: (である調で記載)\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\n⑤比較の記載があれば違いを**表で**まとめて下さい。(無ければ無しと記載)\n⑥上記⑤以外で表に出来そうな部分があれば**表で**まとめて下さい。(無ければ無しと記載)\n\n**▼生成AIの回答(ChatGPT-4o)**\n① **何についての記載か**  \nこの論文は、畳み込みニューラルネットワーク（CNN）のチャネル間の依存関係をモデル化し、ネットワークが生成する表現の質を向上させる「Squeeze-and-Excitation (SE) ブロック」について述べている。\n\n**要旨**\n- CNNのチャネル間の依存関係を明示的にモデル化することで、表現の質を向上させる新しいアーキテクチャ「SEブロック」を提案。\n- SEブロックは、グローバルな情報を活用して有用な特徴を強調し、あまり有用でない特徴を抑制するメカニズムを持つ。\n- SEブロックは既存の最先端アーキテクチャに直接適用可能で、性能向上をもたらす。\n- SEブロックを使用したSENetは、ILSVRC 2017分類コンペで1位を獲得し、top-5エラー率を2.251%にまで削減。\n\n② **改良点・工夫点・テクニック等の記載**  \nこの論文では、CNNのチャネル間の依存関係を明示的にモデル化する「Squeeze-and-Excitation (SE) ブロック」を提案している。SEブロックは、グローバルなチャネル特徴の分布を埋め込むための「squeeze」操作と、それに基づいてチャネルごとの重みを生成する「excitation」操作の2つのステップで構成される。また、SEブロックは計算コストが軽く、既存のアーキテクチャに容易に統合できる点が特徴である。\n\n③ **性能が向上した記載**  \nILSVRC 2017分類コンペで、提案されたSENetアーキテクチャはtop-5エラー率を2.251%に削減し、前年度の勝利エントリ（2.991%）を約25%上回る性能向上を達成した。\n\n④ **メカニズムの解明・なぜそうなるのか等の記載**  \nSEブロックは、チャネル間の依存関係を再校正することで、ネットワークが有用な特徴を強調し、不要な特徴を抑制する能力を向上させる。\n\n⑤ **比較の記載**  \n無し\n\n⑥ **上記⑤以外で表に出来そうな部分**  \n無し\n\n## 2 RELATED WORK\n\n**▼質問(人間)**\n(論文・記事の各章節を貼り付け)\n2 RELATED WORK\nDeeper architectures. VGGNets [11] and Inception mod-\nels [5] showed that increasing the depth of a network could\nsignificantly increase the quality of representations that\nit was capable of learning. By regulating the distribution\nof the inputs to each layer, Batch Normalization (BN) [6]\nadded stability to the learning process in deep networks\nand produced smoother optimisation surfaces [12]. Building\non these works, ResNets demonstrated that it was pos-\nsible to learn considerably deeper and stronger networks\nthrough the use of identity-based skip connections [13], [14].\nHighway networks [15] introduced a gating mechanism to\nregulate the flow of information along shortcut connections.\nFollowing these works, there have been further reformula-\ntions of the connections between network layers [16], [17],\nwhich show promising improvements to the learning and\nrepresentational properties of deep networks.\nAn alternative, but closely related line of research has\nfocused on methods to improve the functional form of\nthe computational elements contained within a network.\nGrouped convolutions have proven to be a popular ap-\nproach for increasing the cardinality of learned transforma-\ntions [18], [19]. More flexible compositions of operators can\nbe achieved with multi-branch convolutions [5], [6], [20],\n[21], which can be viewed as a natural extension of the\ngrouping operator. In prior work, cross-channel correlations\nare typically mapped as new combinations of features, ei-\nther independently of spatial structure [22], [23] or jointly\nby using standard convolutional filters [24] with 1 × 1\nconvolutions. Much of this research has concentrated on the\nobjective of reducing model and computational complexity,\nreflecting an assumption that channel relationships can be\nformulated as a composition of instance-agnostic functions\nwith local receptive fields. In contrast, we claim that provid-\ning the unit with a mechanism to explicitly model dynamic,\nnon-linear dependencies between channels using global in-\nformation can ease the learning process, and significantly\nenhance the representational power of the network.\nAlgorithmic Architecture Search. Alongside the works\ndescribed above, there is also a rich history of research\nthat aims to forgo manual architecture design and instead\nseeks to learn the structure of the network automatically.\nMuch of the early work in this domain was conducted in\nthe neuro-evolution community, which established methods\nfor searching across network topologies with evolutionary\nmethods [25], [26]. While often computationally demand-\ning, evolutionary search has had notable successes which\ninclude finding good memory cells for sequence models\n[27], [28] and learning sophisticated architectures for large-\nscale image classification [29]–[31]. With the goal of reduc-\ning the computational burden of these methods, efficient\nalternatives to this approach have been proposed based on\nLamarckian inheritance [32] and differentiable architecture\nsearch [33].\nBy formulating architecture search as hyperparameter\noptimisation, random search [34] and other more sophis-\nticated model-based optimisation techniques [35], [36] can\nalso be used to tackle the problem. Topology selection\nas a path through a fabric of possible designs [37] and\ndirect architecture prediction [38], [39] have been proposed\nas additional viable architecture search tools. Particularly\nstrong results have been achieved with techniques from\nreinforcement learning [40]–[44]. SE blocks can be used as\natomic building blocks for these search algorithms, and\nwere demonstrated to be highly effective in this capacity\nin concurrent work [45].\nAttention and gating mechanisms. Attention can be in-\nterpreted as a means of biasing the allocation of available\ncomputational resources towards the most informative com-\nponents of a signal [46]–[51]. Attention mechanisms have\ndemonstrated their utility across many tasks including se-\nquence learning [52], [53], localisation and understanding in\nimages [9], [54], image captioning [55], [56] and lip reading\n[57]. In these applications, it can be incorporated as an oper-\nator following one or more layers representing higher-level\nabstractions for adaptation between modalities. Concurrent\nwork provides an interesting study into the combined use of\nspatial and channel attention around convolutional [58] and\nbottleneck units [59]. Wang et al. [60] introduced a powerful\ntrunk-and-mask attention mechanism based on hourglass\nmodules [8] that is inserted between the intermediate stages\nof deep residual networks. By contrast, our proposed SE\nblock comprises a lightweight gating mechanism which\nfocuses on enhancing the representational power of the\nnetwork by modelling channel-wise relationships in a com-\nputationally efficient manner.\n\n上記の内容に関して質問: (である調で記載)\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\n⑤比較の記載があれば違いを**表で**まとめて下さい。(無ければ無しと記載)\n⑥上記⑤以外で表に出来そうな部分があれば**表で**まとめて下さい。(無ければ無しと記載)\n\n**▼生成AIの回答(ChatGPT-4o)**\n① **何についての記載か**  \nこの章は、畳み込みニューラルネットワーク（CNN）の関連研究について述べており、特に深層アーキテクチャ、アルゴリズム的なアーキテクチャ探索、注意メカニズムなどに焦点を当てている。\n\n**要旨**\n- VGGNetsやInceptionモデルなど、ネットワークの深さを増すことにより表現力を向上させる手法について議論。\n- ResNetsなど、ネットワーク層間の接続を再構築する手法が、深層ネットワークの学習を改善することを示している。\n- グループ化された畳み込みやマルチブランチ畳み込みのような手法が、チャネル間の相関を捉えるために提案されている。\n- アーキテクチャ探索のための進化的手法や強化学習を用いた自動化手法についての研究が進んでいる。\n- 注意メカニズムやゲーティングメカニズムが、信号の最も情報量の多い部分に計算リソースを集中させる方法として有用であることが示されている。\n\n② **改良点・工夫点・テクニック等の記載**  \nこの論文では、チャネル間の動的で非線形な依存関係をグローバルな情報を用いて明示的にモデル化するメカニズムを提案し、これによりネットワークの表現力を大幅に強化することを主張している。また、SEブロックは注意メカニズムやゲーティングメカニズムの一環として、計算効率を保ちながらチャネル間の関係をモデル化するための軽量なゲーティングメカニズムを提供している。\n\n③ **性能が向上した記載**  \n無し\n\n④ **メカニズムの解明・なぜそうなるのか等の記載**  \nSEブロックは、注意メカニズムやゲーティングメカニズムの一部として、信号の最も情報量の多い部分に計算リソースを集中させることで、ネットワークの表現力を向上させる。\n\n⑤ **比較の記載**  \n無し\n\n⑥ **上記⑤以外で表に出来そうな部分**  \n無し\n\n**▼質問(人間)**\n関連研究について表にまとめられないか?\n\n**▼生成AIの回答(ChatGPT-4o)**\n前述の関連研究については、以下のように表にまとめることが可能である。\n\n| **カテゴリー**               | **主な研究**                           | **内容**                                                                                                     | **特記事項**                                     |\n|------------------------------|-----------------------------------------|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------|\n| **深層アーキテクチャ**        | VGGNets, Inception, ResNets            | ネットワークの深さを増すことで表現力を向上。ResNetsはスキップ接続を使用して学習を容易に。                          | ResNetsは学習の安定性を改善する。                |\n| **グループ化された畳み込み**  | Grouped Convolutions                   | 畳み込み演算をグループ化することで、変換のカーディナリティを増加。                                                  | モデルと計算の複雑さを低減する手法。            |\n| **マルチブランチ畳み込み**    | Inception, ResNeXt                     | 複数のブランチを使用した柔軟なオペレーターの組み合わせによって、空間構造とチャネル間の相関を同時にモデリング。           | モデルの表現力を向上させる。                    |\n| **アーキテクチャ探索**        | Evolutionary Methods, Reinforcement Learning | 手動での設計を避け、進化的手法や強化学習によってネットワークの構造を自動的に学習。                                 | 高度なネットワーク構造を自動で発見。            |\n| **注意メカニズム**            | Attention, Trunk-and-Mask              | 信号の最も情報量の多い部分に計算リソースを集中させることで、ネットワークの表現力を強化。                               | SEブロックも注意メカニズムの一種として機能。    |\n| **ゲーティングメカニズム**    | Highway Networks, SE Blocks            | 情報の流れを調整するためのメカニズム。SEブロックはチャネル間の依存関係をモデル化し、ネットワークの表現力を向上させる。 | 計算効率が高く、既存アーキテクチャに適用可能。 |\n\nこの表により、各研究のカテゴリーと主な内容、および特記事項が整理され、関連研究の全体像を理解しやすくなっている。\n\n## 3 SQUEEZE-AND-EXCITATION BLOCKS\n\n**▼質問(人間)**\n(論文・記事の各章節を貼り付け)\n3 SQUEEZE-AND-EXCITATION BLOCKS\nThe Squeeze-and-Excitation block is a computational unit\nwhich can be constructed for any given transformation\nU = Ftr (X), X ∈ RH′×W ′×C′\n, U ∈ RH×W ×C . For sim-\nplicity, in the notation that follows we take Ftr to be a\nconvolutional operator. Let V = [v1, v2, . . . , vC ] denote\nthe learned set of filter kernels, where vc refers to the\nparameters of the c-th filter. We can then write the outputs\nof Ftr as U = [u1, u2, . . . , uC ], where\nuc = vc ∗ X =\nC′\n∑\ns=1\nvs\nc ∗ xs. (1)\nHere ∗ denotes convolution, vc = [v1\nc , v2\nc , . . . , vC′\nc ], X =\n[x1, x2, . . . , xC′\n] (to simplify the notation, bias terms are\nomitted) and vs\nc is a 2D spatial kernel representing a single\nchannel of vc that acts on the corresponding channel of X.\nSince the output is produced by a summation through all\nchannels, channel dependencies are implicitly embedded in\nvc, but are entangled with the local spatial correlation cap-\ntured by the filters. As a consequence, the channel relation-\nships modelled by convolution are inherently local. Since\nour goal is to ensure that the network is able to increase\nits sensitivity to informative features so that they can be\nexploited by subsequent transformations most effectively,\nwe would like to provide it with access to global infor-\nmation. We propose to achieve this by explicitly modelling\nchannel interdependencies to recalibrate filter responses in\ntwo steps, squeeze and excitation, before they are fed into the\nnext transformation, described next. A diagram illustrating\nthe structure of an SE block is shown in Fig. 1.\n\n上記の内容に関して質問: (である調で記載)\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\n⑤比較の記載があれば違いを**表で**まとめて下さい。(無ければ無しと記載)\n⑥上記⑤以外で表に出来そうな部分があれば**表で**まとめて下さい。(無ければ無しと記載)\n\n**▼生成AIの回答(ChatGPT-4o)**\n① **何についての記載か**  \nこの節は、Squeeze-and-Excitation（SE）ブロックの設計と機能に関する説明である。SEブロックは、CNNの変換操作に対してチャネル間の依存関係を明示的にモデル化し、フィルタの応答を再キャリブレーションするための計算単位である。\n\n**要旨**\n- SEブロックは、任意の変換操作に対して構築可能な計算単位である。\n- 変換操作によって生成されるフィルタ応答を、チャネル間の依存関係を明示的にモデル化することで再キャリブレーションする。\n- SEブロックは、2つのステップ（スクイーズとエキサイト）でフィルタ応答を再キャリブレーションする。\n\n② **改良点・工夫点・テクニック等の記載**  \nSEブロックの主要な改良点は、チャネル間の依存関係を明示的にモデル化することである。具体的には、以下の工夫がなされている:\n- **スクイーズ（Squeeze）**: フィルタ応答を空間次元で集約し、チャネルごとのグローバルなコンテキスト情報を取得する。\n- **エキサイト（Excitation）**: スクイーズされたグローバル情報を基に、各チャネルの重要度を決定するための自己ゲーティング機構を導入。\n\n③ **性能が向上した記載**  \n性能向上に関する具体値はこの節には記載されていないが、SEブロックはフィルタの応答を再キャリブレーションすることで、ネットワークの感度を高めることができると述べられている。\n\n④ **メカニズムの解明・なぜそうなるのか等の記載**  \nSEブロックは、チャネル間の依存関係が畳み込み操作で局所的にしかモデル化されていないという問題を解決するために設計された。スクイーズとエキサイトの2つのステップを通じて、チャネル間の依存関係をグローバルなスケールで明示的にモデル化し、ネットワークが有用な特徴に対してより敏感に反応できるようにしている。\n\n⑤ **比較の記載**  \n無し\n\n⑥ **上記⑤以外で表に出来そうな部分**  \n表にする部分は特に無し\n\n## 3.1 Squeeze: Global Information Embedding\n\n**▼質問(人間)**\n(論文・記事の各章節を貼り付け)\n3.1 Squeeze: Global Information Embedding\nIn order to tackle the issue of exploiting channel depen-\ndencies, we first consider the signal to each channel in the\noutput features. Each of the learned filters operates with\na local receptive field and consequently each unit of the\ntransformation output U is unable to exploit contextual\ninformation outside of this region.\nTo mitigate this problem, we propose to squeeze global\nspatial information into a channel descriptor. This is\nachieved by using global average pooling to generate\nchannel-wise statistics. Formally, a statistic z ∈ RC is gener-\nated by shrinking U through its spatial dimensions H × W ,\nsuch that the c-th element of z is calculated by:\nzc = Fsq (uc) = 1\nH × W\nH∑\ni=1\nW∑\nj=1\nuc(i, j). (2)\nDiscussion. The output of the transformation U can be\ninterpreted as a collection of the local descriptors whose\nstatistics are expressive for the whole image. Exploiting\nsuch information is prevalent in prior feature engineering\nwork [61]–[63]. We opt for the simplest aggregation tech-\nnique, global average pooling, noting that more sophisti-\ncated strategies could be employed here as well.\n\n上記の内容に関して質問: (である調で記載)\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\n⑤比較の記載があれば違いを**表で**まとめて下さい。(無ければ無しと記載)\n⑥上記⑤以外で表に出来そうな部分があれば**表で**まとめて下さい。(無ければ無しと記載)\n\n**▼生成AIの回答(ChatGPT-4o)**\n① **何についての記載か**  \nこの節は、Squeeze-and-Excitation（SE）ブロックの「Squeeze」操作について説明している。Squeeze操作は、チャネル依存関係を活用するために、グローバルな空間情報をチャネル記述子に圧縮するプロセスである。\n\n**要旨**\n- 各チャネルの信号が局所的な受容野に制約されている問題を解決するために、グローバルな空間情報をチャネル記述子に圧縮する。\n- グローバル平均プーリングを使用してチャネルごとの統計を生成し、各チャネルの要素を圧縮する。\n\n② **改良点・工夫点・テクニック等の記載**  \n- Squeeze操作では、グローバルな空間情報を効果的に活用するために、グローバル平均プーリングを使用している。これにより、各チャネルの統計が生成され、ネットワークが局所的な受容野を超えた情報を利用できるようにしている。\n\n③ **性能が向上した記載**  \n性能向上に関する具体値は記載されていない。\n\n④ **メカニズムの解明・なぜそうなるのか等の記載**  \nグローバル平均プーリングによって生成されたチャネル記述子は、局所的な記述子の集合として解釈される。これにより、ネットワークが画像全体の情報を利用できるようになり、より表現力の高い特徴を得ることができる。\n\n⑤ **比較の記載**  \n無し\n\n⑥ **上記⑤以外で表に出来そうな部分**  \n表にする部分は特に無し\n\n## 3.2 Excitation: Adaptive Recalibration\n\n**▼質問(人間)**\n(論文・記事の各章節を貼り付け)\n3.2 Excitation: Adaptive Recalibration\nTo make use of the information aggregated in the squeeze\noperation, we follow it with a second operation which aims\nto fully capture channel-wise dependencies. To fulfil this\nobjective, the function must meet two criteria: first, it must\nbe flexible (in particular, it must be capable of learning\na nonlinear interaction between channels) and second, it\nmust learn a non-mutually-exclusive relationship since we\nwould like to ensure that multiple channels are allowed to\nbe emphasised (rather than enforcing a one-hot activation).\nTo meet these criteria, we opt to employ a simple gating\nmechanism with a sigmoid activation:\ns = Fex(z, W) = σ(g(z, W)) = σ(W2δ(W1z)), (3)\nwhere δ refers to the ReLU [64] function, W1 ∈ R C\nr ×C and\nW2 ∈ RC× C\nr . To limit model complexity and aid general-\nisation, we parameterise the gating mechanism by forming\na bottleneck with two fully connected (FC) layers around\nthe non-linearity, i.e. a dimensionality-reduction layer with\nparameters W1 and reduction ratio r (this parameter choice\nis discussed in Sec. 6.1), a ReLU and then a dimensionality-\nincreasing layer with parameters W2. The final output of\nthe block is obtained by rescaling the transformation output\nU with the activations:\n˜xc = Fscale(uc, sc) = sc · uc, (4)\nwhere ˜X = [˜x1, ˜x2, . . . , ˜xC ] and Fscale(uc, sc) refers to\nchannel-wise multiplication between the scalar sc and the\nfeature map uc ∈ RH×W .\nDiscussion. The excitation operator maps the input-\nspecific descriptor z to a set of channel specific weights.\nIn this regard, SE blocks intrinsically introduce dynamics\nconditioned on the input, helping to boost feature discrim-\ninability.\n\n上記の内容に関して質問: (である調で記載)\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\n⑤比較の記載があれば違いを**表で**まとめて下さい。(無ければ無しと記載)\n⑥上記⑤以外で表に出来そうな部分があれば**表で**まとめて下さい。(無ければ無しと記載)\n\n**▼生成AIの回答(ChatGPT-4o)**\n① **何についての記載か**  \nこの節は、Squeeze-and-Excitation（SE）ブロックの「Excitation」操作について説明している。Excitation操作は、チャネル間の依存関係を捉え、各チャネルの重要度を適応的に再調整するプロセスである。\n\n**要旨**\n- Squeeze操作で集約された情報を活用し、チャネル間の依存関係を捉えるためにExcitation操作を行う。\n- チャネルごとの依存関係を学習し、複数のチャネルが強調されるようにするために、シグモイド活性化を伴う単純なゲーティングメカニズムを使用する。\n- ゲーティングメカニズムはボトルネック構造を持ち、モデルの複雑さを制限しつつ、非線形なチャネル間の相互作用を学習する。\n\n② **改良点・工夫点・テクニック等の記載**  \n- シンプルなゲーティングメカニズムを使用し、ReLUを使った非線形性とボトルネック構造（2層の全結合層）を導入することで、モデルの複雑さを抑えつつ、チャネル間の依存関係を効果的に捉えている。\n\n③ **性能が向上した記載**  \n性能向上に関する具体値は記載されていない。\n\n④ **メカニズムの解明・なぜそうなるのか等の記載**  \nExcitation操作によって、入力に依存したチャネルごとの重みが生成される。これにより、SEブロックは入力に応じた動的な反応を引き起こし、特徴の識別力を高める。\n\n⑤ **比較の記載**  \n無し\n\n⑥ **上記⑤以外で表に出来そうな部分**  \n表にする部分は特に無し\n\n## 3.3 Instantiations\n\n**▼質問(人間)**\n(論文・記事の各章節を貼り付け)\n3.3 Instantiations\nThe SE block can be integrated into standard architectures\nsuch as VGGNet [11] by insertion after the non-linearity\nfollowing each convolution. Moreover, the flexibility of the\nSE block means that it can be directly applied to transforma-\ntions beyond standard convolutions. To illustrate this point,\nwe develop SENets by incorporating SE blocks into several\nexamples of more complex architectures, described next.\nWe first consider the construction of SE blocks for Incep-\ntion networks [5]. Here, we simply take the transformation\nFtr to be an entire Inception module (see Fig. 2) and by mak-\ning this change for each such module in the architecture, we\nobtain an SE-Inception network. SE blocks can also be used\ndirectly with residual networks (Fig. 3 depicts the schema\nof an SE-ResNet module). Here, the SE block transformation\nFtr is taken to be the non-identity branch of a residual\nmodule. Squeeze and Excitation both act before summation\nwith the identity branch. Further variants that integrate SE\nblocks with ResNeXt [19], Inception-ResNet [21], MobileNet\n[65] and ShuffleNet [66] can be constructed by following\nsimilar schemes (Sec. 5.1). For concrete examples of SENet\narchitectures, a detailed description of SE-ResNet-50 and\nSE-ResNeXt-50 is given in Table 1.\nOne consequence of the flexible nature of the SE block\nis that there are several viable ways in which it could\nbe integrated into these architectures. Therefore, to assess\nsensitivity to the integration strategy used to incorporate SE\nblocks into a network architecture, we also provide ablation\nexperiments exploring different designs for block inclusion\nin Sec. 6.5.\n\n上記の内容に関して質問: (である調で記載)\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\n⑤比較の記載があれば違いを**表で**まとめて下さい。(無ければ無しと記載)\n⑥上記⑤以外で表に出来そうな部分があれば**表で**まとめて下さい。(無ければ無しと記載)\n\n**▼生成AIの回答(ChatGPT-4o)**\n① **何についての記載か**  \nこの節は、Squeeze-and-Excitation（SE）ブロックの実装に関する記載である。SEブロックがどのように標準的なニューラルネットワークアーキテクチャ（例：VGGNet、Inception、ResNetなど）に統合されるかを説明し、特定のアーキテクチャにおけるSEブロックの適用例について述べている。\n\n**要旨**\n- SEブロックは、VGGNetやInceptionなどの標準的なアーキテクチャに簡単に統合できる。\n- InceptionネットワークやResNetにSEブロックを組み込んだSE-Inceptionネットワーク、SE-ResNetなどの具体例が説明されている。\n- 他の複雑なアーキテクチャ（ResNeXt、Inception-ResNet、MobileNet、ShuffleNetなど）への応用も可能。\n- SEブロックを統合するための異なるデザインの影響を調べるためのアブレーション実験が実施されている。\n\n② **改良点・工夫点・テクニック等の記載**  \n- SEブロックを既存の様々なアーキテクチャに統合する際に、その柔軟性を活かして異なるデザインを適用できる点が工夫されている。\n- 異なるアーキテクチャに対して、SEブロックの統合位置や手法を調整することが可能である。\n\n③ **性能が向上した記載**  \n具体値は記載されていないが、SEブロックを適用することで各アーキテクチャの性能向上が期待されるとされている。\n\n④ **メカニズムの解明・なぜそうなるのか等の記載**  \n無し\n\n⑤ **比較の記載**  \n無し\n\n⑥ **上記⑤以外で表に出来そうな部分**  \n表にできそうな部分として、各アーキテクチャ（VGGNet、Inception、ResNetなど）におけるSEブロックの適用例や統合方法を表形式でまとめることができる。\n\n| アーキテクチャ | SEブロックの適用方法 |\n| -------------- | --------------------- |\n| VGGNet         | 各畳み込み層後の非線形性の後に挿入 |\n| Inception      | 各Inceptionモジュール全体に適用        |\n| ResNet         | 非アイデンティティブランチに適用し、アイデンティティブランチとの和の前に適用 |\n| ResNeXt        | ResNetと同様の手法で適用            |\n| Inception-ResNet | InceptionおよびResNetの組み合わせモジュールに適用 |\n| MobileNet      | MobileNetモジュールに適用         |\n| ShuffleNet     | ShuffleNetモジュールに適用         |\n\n## 4 MODEL AND COMPUTATIONAL COMPLEXITY\n\n**▼質問(人間)**\n(論文・記事の各章節を貼り付け)\n4 MODEL AND COMPUTATIONAL COMPLEXITY\nFor the proposed SE block design to be of practical use, it\nmust offer a good trade-off between improved performance\nand increased model complexity. We set the reduction ratio\nr (introduced in Sec. 3.2) to 16 in all experiments, except\nwhere stated otherwise (an ablation study of this design\ndecision is provided in Sec. 6.1). To illustrate the compu-\ntational burden associated with the module, we consider\na comparison between ResNet-50 and SE-ResNet-50 as an\nexample. ResNet-50 requires ∼3.86 GFLOPs in a single\nforward pass for a 224 × 224 pixel input image. Each SE\nblock makes use of a global average pooling operation in\nthe squeeze phase and two small fully connected layers in\nthe excitation phase, followed by an inexpensive channel-\nwise scaling operation. In aggregate, SE-ResNet-50 requires\n∼3.87 GFLOPs, corresponding to a 0.26% relative increase\nover the original ResNet-50. In exchange for this slight addi-\ntional computational burden, the accuracy of SE-ResNet-50\nsurpasses that of ResNet-50 and indeed, approaches that\nof a deeper ResNet-101 network requiring ∼7.58 GFLOPs\n(Table 2).\nIn practical terms, a single pass forwards and backwards\nthrough ResNet-50 takes 190 ms, compared to 209 ms for\nSE-ResNet-50 with a training minibatch of 256 images (both\ntimings are performed on a server with 8 NVIDIA Titan X\nGPUs). We suggest that this represents a reasonable runtime\noverhead, which may be further reduced as global pooling\nand small inner-product operations receive further opti-\nmisation in popular GPU libraries. Due to its importance\nfor embedded device applications, we further benchmark\nCPU inference time for each model: for a 224 × 224 pixel\ninput image, ResNet-50 takes 164 ms in comparison to 167\nms for SE-ResNet-50. We believe that the small additional\ncomputational cost incurred by the SE block is justified by\nits contribution to model performance.\nWe next consider the additional parameters introduced\nby the proposed SE block. These additional parameters\nresult solely from the two fully-connected layers of the\ngating mechanism and therefore constitute a small fraction\nof the total network capacity. Concretely, the total number of\nadditional parameters introduced by the proposed approach\nis given by:\n2\nr\nS∑\ns=1\nNs · Cs2, (5)\nwhere r denotes the reduction ratio, S refers to the number\nof stages (a stage refers to the collection of blocks operating\non feature maps of a common spatial dimension), Cs de-\nnotes the dimension of the output channels and Ns denotes\nthe number of repeated blocks for stage s. SE-ResNet-50\nintroduces ∼2.5 million additional parameters beyond the\n∼25 million parameters required by ResNet-50, correspond-\ning to a ∼10% increase. In practice, the majority of these\nparameters come from the final stage of the network, where\nthe excitation operation is performed across the greatest\nnumber of channels. However, we found that this compara-\ntively costly final stage of SE blocks could be removed at\nonly a small cost in performance (\u003c0.1% top-5 error on\nImageNet) reducing the relative parameter increase to ∼4%,\nwhich may prove useful in cases where parameter usage is\na key consideration (see Sec. 7.2 for further discussion).\n\n上記の内容に関して質問: (である調で記載)\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\n⑤比較の記載があれば違いを**表で**まとめて下さい。(無ければ無しと記載)\n⑥上記⑤以外で表に出来そうな部分があれば**表で**まとめて下さい。(無ければ無しと記載)\n\n**▼生成AIの回答(ChatGPT-4o)**\n① **何についての記載か**  \nこの節は、提案されたSEブロックのモデルおよび計算複雑性に関する記載である。SEブロックがどの程度の計算負荷とモデル複雑性を追加し、それが性能向上にどのように寄与するかについて述べている。\n\n**要旨**\n- SEブロックの追加による計算量の増加は僅かであるが、モデルの性能向上には大きく寄与する。\n- SE-ResNet-50では、ResNet-50に対して約0.26%の計算量増加であり、ResNet-101と同等の精度に近づく。\n- パラメータの増加は主に完全結合層から生じ、全体のモデル容量に対する影響は比較的小さい。\n- CPUおよびGPUでの推論時間やトレーニング時間のオーバーヘッドは許容範囲内である。\n\n② **改良点・工夫点・テクニック等の記載**  \n- 完全結合層によるパラメータの増加があるが、特定のステージでのSEブロックを省略することで、精度にほとんど影響を与えずにパラメータ増加を約4%に抑えられる工夫が記載されている。\n\n③ **性能が向上した記載**  \n- SE-ResNet-50は、約0.26%の計算量増加でResNet-50よりも高い精度を達成しており、ResNet-101と同等の性能に近づく。\n- ResNet-50とSE-ResNet-50の比較において、推論時間はGPUで約19 msの増加、CPUで約3 msの増加が見られるが、精度向上に対して妥当なトレードオフであるとされている。\n\n④ **メカニズムの解明・なぜそうなるのか等の記載**  \n- SEブロックが追加する計算負荷の大部分は完全結合層に起因しているが、これがモデルの性能向上に寄与するため、わずかなオーバーヘッドが許容される。\n\n⑤ **比較の記載**  \nResNet-50とSE-ResNet-50の違いについて表でまとめることができる。\n\n| モデル          | 計算量 (GFLOPs) | 推論時間 (GPU) | 推論時間 (CPU) | パラメータ数増加 | 精度向上 |\n| -------------- | ---------------- | --------------- | --------------- | ---------------- | -------- |\n| ResNet-50      | ∼3.86            | 190 ms          | 164 ms          | -                | -        |\n| SE-ResNet-50   | ∼3.87 (+0.26%)   | 209 ms (+19 ms) | 167 ms (+3 ms)  | ∼10% (約2.5M)    | ResNet-101に接近 |\n\n⑥ **上記⑤以外で表に出来そうな部分**  \n無し\n\n## 5 EXPERIMENTS\n\n**▼質問(人間)**\n(論文・記事の各章節を貼り付け)\n5 EXPERIMENTS\nIn this section, we conduct experiments to investigate the\neffectiveness of SE blocks across a range of tasks, datasets\nand model architectures.\n5.1 Image Classification\nTo evaluate the influence of SE blocks, we first perform\nexperiments on the ImageNet 2012 dataset [10] which\ncomprises 1.28 million training images and 50K validation\nimages from 1000 different classes. We train networks on\nthe training set and report the top-1 and top-5 error on the\nvalidation set.\nEach original network architecture and its correspond-\ning SE counterpart are trained with identical optimisa-\ntion schemes. We follow standard practices and perform\ndata augmentation with random cropping [5] to a size of\n224 × 224 pixels (or 299 × 299 for Inception-ResNet-v2 [21]\nand SE-Inception-ResNet-v2) and perform random horizon-\ntal flipping. Each input image is normalised through mean\nRGB-channel subtraction. We adopt the data balancing strat-\negy described in [67] for minibatch sampling. All models\nare trained on our distributed learning system ROCS which\nis designed to handle efficient parallel training of large\nnetworks. Optimisation is performed using synchronous\nSGD with momentum 0.9 and a minibatch size of 1024.\nThe initial learning rate is set to 0.6 and decreased by a\nfactor of 10 every 30 epochs. All models are trained for 100\nepochs from scratch, using the weight initialisation strategy\ndescribed in [68].\nWhen evaluating the models we apply centre-cropping\nso that 224 × 224 pixels are cropped from each image, after\nits shorter edge is first resized to 256 (299 × 299 from\neach image whose shorter edge is first resized to 352 for\nInception-ResNet-v2 and SE-Inception-ResNet-v2).\nNetwork depth. We begin by comparing SE-ResNet against\nResNet architectures with different depths and report the\nresults in Table 2. We observe that SE blocks consistently\nimprove performance across different depths with an ex-\ntremely small increase in computational complexity. Re-\nmarkably, SE-ResNet-50 achieves a single-crop top-5 valida-\ntion error of 6.62%, exceeding ResNet-50 (7.48%) by 0.86%\nand approaching the performance achieved by the much\ndeeper ResNet-101 network (6.52% top-5 error) with only\nhalf of the total computational burden (3.87 GFLOPs vs.\n7.58 GFLOPs). This pattern is repeated at greater depth,\nwhere SE-ResNet-101 (6.07% top-5 error) not only matches,\nbut outperforms the deeper ResNet-152 network (6.34%\ntop-5 error) by 0.27%. While it should be noted that the SE\nblocks themselves add depth, they do so in an extremely\ncomputationally efficient manner and yield good returns\neven at the point at which extending the depth of the base\narchitecture achieves diminishing returns. Moreover, we see\nthat the gains are consistent across a range of different\nnetwork depths, suggesting that the improvements induced\nby SE blocks may be complementary to those obtained by\nsimply increasing the depth of the base architecture.\nIntegration with modern architectures. We next study the\neffect of integrating SE blocks with two further state-of-\nthe-art architectures, Inception-ResNet-v2 [21] and ResNeXt\n(using the setting of 32 × 4d) [19], both of which introduce\nadditional computational building blocks into the base net-\nwork. We construct SENet equivalents of these networks,\nSE-Inception-ResNet-v2 and SE-ResNeXt (the configuration\nof SE-ResNeXt-50 is given in Table 1) and report results\nin Table 2. As with the previous experiments, we observe\nsignificant performance improvements induced by the in-\ntroduction of SE blocks into both architectures. In partic-\nular, SE-ResNeXt-50 has a top-5 error of 5.49% which is\nsuperior to both its direct counterpart ResNeXt-50 (5.90%\ntop-5 error) as well as the deeper ResNeXt-101 (5.57% top-5\nerror), a model which has almost twice the total number of\nparameters and computational overhead. We note a slight\ndifference in performance between our re-implementation\nof Inception-ResNet-v2 and the result reported in [21]. How-\never, we observe a similar trend with regard to the effect of\nSE blocks, finding that SE-Inception-ResNet-v2 (4.79% top-5\nerror) outperforms our reimplemented Inception-ResNet-v2\nbaseline (5.21% top-5 error) by 0.42% (a relative improve-\nment of 8.1%) as well as the reported result in [21]. The\ntraining curves for the baseline architectures ResNet-50,\nResNet-152, ResNeXt-50 and BN-Inception, and their re-\nspective SE counterparts are depicted in Fig. 4. We observe\nthat SE blocks yield a steady improvement throughout the\noptimisation procedure. Moreover, this trend is consistent\nacross each of the families of state-of-the-art architectures\nconsidered as baselines.\nWe also assess the effect of SE blocks when operating\non non-residual networks by conducting experiments with\nthe VGG-16 [11] and BN-Inception architecture [6]. To fa-\ncilitate the training of VGG-16 from scratch, we add Batch\nNormalization layers after each convolution. As with the\nprevious models, we use identical training schemes for both\nVGG-16 and SE-VGG-16. The results of the comparison are\nshown in Table 2. Similarly to the results reported for the\nresidual baseline architectures, we observe that SE blocks\nbring improvements in performance.\nMobile setting. Finally, we consider two representative\narchitectures from the class of mobile-optimised networks,\nMobileNet [65] and ShuffleNet [66]. For these experiments,\nwe used a minibatch size of 256 and a weight decay of\n4 × 10−5. We trained the models across 8 GPUs using SGD\nwith momentum (set to 0.9) and an initial learning rate\nof 0.1 which was reduced by a factor of 10 each time the\nvalidation loss plateaued (rather than using a fixed-length\nschedule). The total training process required ∼ 400 epochs\n(we found that this approach enabled us to reproduce the\nbaseline performance of [66]). The results reported in Table 3\nshow that SE blocks consistently improve the accuracy by a\nlarge margin at a minimal increase in computational cost.\nAdditional datasets. We next investigate whether the bene-\nfits of SE blocks generalise to datasets beyond ImageNet. We\nperform experiments with several popular baseline archi-\ntectures and techniques (ResNet-110 [14], ResNet-164 [14],\nWideResNet-16-8 [69], Shake-Shake [70] and Cutout [71]) on\nthe CIFAR-10 and CIFAR-100 datasets [73]. These comprise\na collection of 50k training and 10k test 32 × 32 pixel RGB\nimages, labelled with 10 and 100 classes respectively. The\nintegration of SE blocks into these networks follows the\nsame approach that was described in Sec. 3.3. Each baseline\nand its SENet counterpart are trained with a standard data\naugmentation strategy [24], [74]. During training, images\nare randomly horizontally flipped and zero-padded on each\nside with four pixels before taking a random 32 × 32 crop.\nMean and standard deviation normalisation is also applied.\nThe setting of the training strategy and other hyperparam-\neters (e.g. minibatch size, initial learning rate, number of\nepochs, weight decay) match those suggested by the authors\nof each model. We report the performance of each baseline\nand its SENet counterpart on CIFAR-10 in Table 4 and per-\nformance on CIFAR-100 in Table 5. We observe that in every\ncomparison SENets outperform the baseline architectures,\nsuggesting that the benefits of SE blocks are not confined to\nthe ImageNet dataset.\n\n上記の内容に関して質問: (である調で記載)\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\n⑤比較の記載があれば違いを**表で**まとめて下さい。(無ければ無しと記載)\n⑥上記⑤以外で表に出来そうな部分があれば**表で**まとめて下さい。(無ければ無しと記載)\n\n**▼生成AIの回答(ChatGPT-4o)**\n① **何についての記載か**  \nこの節は、SEブロックの有効性を様々なタスク、データセット、モデルアーキテクチャにわたって評価するための実験に関する記載である。\n\n**要旨**\n- ImageNet 2012データセットを用いた画像分類タスクで、SEブロックがモデル性能に与える影響を評価。\n- ResNetをはじめとする各種モデルアーキテクチャにSEブロックを組み込んだ際の性能向上を検証。\n- モバイル向けのアーキテクチャやCIFAR-10、CIFAR-100などの追加データセットに対しても、SEブロックの有効性を確認。\n\n② **改良点・工夫点・テクニック等の記載**  \n- SEブロックを様々なアーキテクチャに統合し、それらの効果を評価する実験が行われている。\n- モバイルネットワークでは、検証損失のプラトーに応じて学習率を調整する手法が使用されている。\n\n③ **性能が向上した記載**  \n- SE-ResNet-50は、ResNet-50に対してトップ5エラー率が7.48%から6.62%に改善（+0.86%）し、ResNet-101に迫る性能を示した。\n- SE-ResNeXt-50は、ResNeXt-50に対してトップ5エラー率が5.90%から5.49%に改善し、ResNeXt-101（5.57%）を上回った。\n- SE-Inception-ResNet-v2は、Inception-ResNet-v2に対してトップ5エラー率が5.21%から4.79%に改善した。\n- CIFAR-10およびCIFAR-100においても、SEブロックを組み込んだモデルがベースラインを上回る性能を示した。\n\n④ **メカニズムの解明・なぜそうなるのか等の記載**  \n- SEブロックがモデルの深さを増す代わりに、計算効率よく性能を向上させることが示されている。\n\n⑤ **比較の記載**  \nResNetやその他のモデルアーキテクチャにSEブロックを組み込んだ際の性能向上を表でまとめることができる。\n\n| モデル                | トップ5エラー率 | トップ1エラー率 | GFLOPs | パラメータ数 | 比較対象 |\n| -------------------- | -------------- | -------------- | ------- | ------------ | -------- |\n| ResNet-50            | 7.48%          | -              | 3.86    | 25M          | 基準      |\n| SE-ResNet-50         | 6.62%          | -              | 3.87    | 27.5M        | +0.86%   |\n| ResNet-101           | 6.52%          | -              | 7.58    | 45M          | -        |\n| SE-ResNeXt-50        | 5.49%          | -              | 4.25    | 25M          | +0.41%   |\n| ResNeXt-101          | 5.57%          | -              | 8.01    | 44M          | -        |\n| Inception-ResNet-v2  | 5.21%          | -              | 13.0    | 55M          | 基準      |\n| SE-Inception-ResNet-v2 | 4.79%        | -              | 13.2    | 56.5M        | +0.42%   |\n\n⑥ **上記⑤以外で表に出来そうな部分**  \n無し\n\n## 5.2 Scene Classification\n\n**▼質問(人間)**\n(論文・記事の各章節を貼り付け)\n5.2 Scene Classification\nWe next conduct experiments on the Places365-Challenge\ndataset [75] for scene classification. This dataset comprises\n8 million training images and 36, 500 validation images\nacross 365 categories. Relative to classification, the task of\nscene understanding offers an alternative assessment of a\nmodel’s ability to generalise well and handle abstraction.\nThis is because it often requires the model to handle more\ncomplex data associations and to be robust to a greater level\nof appearance variation.\nWe opted to use ResNet-152 as a strong baseline to\nassess the effectiveness of SE blocks and carefully follow\nthe training and evaluation protocols described in [72]. In\nthese experiments, all models are trained from scratch. We\nreport the results in Table 6, comparing also with prior\nwork. We observe that SE-ResNet-152 (11.01% top-5 error)\nachieves a lower validation error than ResNet-152 (11.61%\ntop-5 error), providing evidence that SE blocks can also yield\nimprovements for scene classification. This SENet surpasses\nthe previous state-of-the-art model Places-365-CNN [72]\nwhich has a top-5 error of 11.48% on this task.\n5.3 Object Detection on COCO\nWe further assess the generalisation of SE blocks on the\ntask of object detection using the COCO dataset [76] which\ncomprises 80k training images and 40k validation images,\nfollowing the splits used in [13]. We use the Faster R-CNN\n[4] detection framework as the basis for evaluating our\nmodels and follow the basic implementation described in\n[13]. Our goal is to evaluate the effect of replacing the trunk\narchitecture (ResNet) in the object detector with SE-ResNet,\nso that any changes in performance can be attributed\nto better representations. Table 7 reports the validation\nset performance of the object detector using ResNet-50,\nResNet-101 and their SE counterparts as trunk architectures.\nSE-ResNet-50 outperforms ResNet-50 by 1.3% (a relative\n5.2% improvement) on COCO’s standard AP metric and by\n1.6% on AP@IoU=0.5. SE blocks also bring improvements\non the deeper ResNet-101 architecture achieving a 0.7%\nimprovement (or 2.6% relative improvement) on the AP\nmetric.\nIn summary, this set of experiments demonstrate that the\nimprovements induced by SE blocks can be realised across\na broad range of architectures, tasks and datasets.\n5.4 ILSVRC 2017 Classification Competition\nSENets formed the foundation of our submission to the\nILSVRC competition where we achieved first place. Our\nwinning entry comprised a small ensemble of SENets that\nemployed a standard multi-scale and multi-crop fusion\nstrategy to obtain a top-5 error of 2.251% on the test set.\nAs part of this submission, we constructed an additional\nmodel, SENet-154, by integrating SE blocks with a modified\nResNeXt [19] (the details of the architecture are provided\nin Appendix). We compare this model with prior work on\nthe ImageNet validation set in Table 8 using standard crop\nsizes (224 × 224 and 320 × 320). We observe that SENet-154\nachieves a top-1 error of 18.68% and a top-5 error of 4.47%\nusing a 224 × 224 centre crop evaluation, which represents\nthe strongest reported result.\nFollowing the challenge there has been a great deal of\nfurther progress on the ImageNet benchmark. For compar-\nison, we include the strongest results that we are currently\naware of among the both published and unpublished lit-\nerature in Table 9. The best performance using only Im-\nageNet data was recently reported by [79]. This method\nuses reinforcement learning to develop new polices for\ndata augmentation during training to improve the perfor-\nmance of the architecture proposed by [31]. The best overall\nperformance was reported by [80] using a ResNeXt-101\n32×48d architecture. This was achieved by pretraining their\nmodel on approximately one billion weakly labelled images\nand finetuning on ImageNet. The improvements yielded by\nmore sophisticated data augmentation [79] and extensive\npretraining [80] may be complementary to our proposed\nchanges to the network architecture.\n\n上記の内容に関して質問: (である調で記載)\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\n⑤比較の記載があれば違いを**表で**まとめて下さい。(無ければ無しと記載)\n⑥上記⑤以外で表に出来そうな部分があれば**表で**まとめて下さい。(無ければ無しと記載)\n\n**▼生成AIの回答(ChatGPT-4o)**\n① **何についての記載か**  \nこの節は、SEブロックの有効性をシーン分類、物体検出、ILSVRC 2017分類競技などの異なるタスクにおいて評価する実験に関する記載である。\n\n**要旨**\n- Places365-Challengeデータセットを用いて、シーン分類におけるSEブロックの効果を評価。\n- COCOデータセットを使用し、物体検出タスクでのSEブロックの効果を検証。\n- ILSVRC 2017の分類競技におけるSENetの成功とその結果を報告。\n\n② **改良点・工夫点・テクニック等の記載**  \n- ResNet-152を基準に、SEブロックを導入したSE-ResNet-152のシーン分類能力を評価。\n- Faster R-CNNフレームワークで、トランクアーキテクチャをSE-ResNetに置き換え、その効果を評価。\n- ILSVRC 2017の分類競技では、マルチスケールおよびマルチクロップ融合戦略を使用したSENetのエントリが使用された。\n\n③ **性能が向上した記載**  \n- SE-ResNet-152は、ResNet-152に対してトップ5エラー率が11.61%から11.01%に改善。\n- COCOデータセットでの物体検出タスクでは、SE-ResNet-50がResNet-50に対してAPで1.3%、AP@IoU=0.5で1.6%向上。\n- SE-ResNet-101もAPで0.7%の改善を示した。\n- ILSVRC 2017でSENet-154が224×224のセンタークロップ評価でトップ1エラー率18.68%、トップ5エラー率4.47%を達成。\n\n④ **メカニズムの解明・なぜそうなるのか等の記載**  \n- 無し\n\n⑤ **比較の記載**  \n表で違いをまとめることができる。\n\n| タスク                    | モデル           | トップ5エラー率 | トップ1エラー率 | AP    | AP@IoU=0.5 | 比較対象 |\n| ------------------------- | ---------------- | -------------- | -------------- | ----- | ----------- | -------- |\n| シーン分類                | ResNet-152       | 11.61%         | -              | -     | -           | 基準      |\n| シーン分類                | SE-ResNet-152    | 11.01%         | -              | -     | -           | +0.60%   |\n| 物体検出 (COCO)           | ResNet-50        | -              | -              | -     | -           | 基準      |\n| 物体検出 (COCO)           | SE-ResNet-50     | -              | -              | +1.3% | +1.6%       | -        |\n| 物体検出 (COCO)           | ResNet-101       | -              | -              | -     | -           | 基準      |\n| 物体検出 (COCO)           | SE-ResNet-101    | -              | -              | +0.7% | -           | -        |\n| ILSVRC 2017               | SENet-154        | 4.47%          | 18.68%         | -     | -           | -        |\n\n⑥ **上記⑤以外で表に出来そうな部分**  \n無し\n\n## 6 ABLATION STUDY\n\n**▼質問(人間)**\n(論文・記事の各章節を貼り付け)\n6 ABLATION STUDY\nIn this section we conduct ablation experiments to gain\na better understanding of the relative importance of com-\nponents in the SE block design. All ablation experiments\nare performed on the ImageNet dataset on a single ma-\nchine (with 8 GPUs). ResNet-50 is used as the backbone\narchitecture. The data augmentation strategy follows the\napproach described in Sec. 5.1. To allow us to study the\nupper limit of performance for each variant, the learning\nrate is initialised to 0.1 and training continues until the\nvalidation loss plateaus (rather than continuing for a fixed\nnumber of epochs). The learning rate is then reduced by a\nfactor of 10 and then this process is repeated (three times in\ntotal).\n6.1 Reduction ratio\nThe reduction ratio r introduced in Eqn. 5 is a hyperpa-\nrameter which allows us to vary the capacity and compu-\ntational cost of the SE blocks in the network. To investigate\nthe trade-off between performance and computational cost\nmediated by this hyperparameter, we conduct experiments\nwith SE-ResNet-50 for a range of different r values. The\ncomparison in Table 10 shows that performance does not\nimprove monotonically with increased capacity, suggesting\nthat with enough weights the SE block is able to overfit\nto the channel interdependencies of the training set. We\nfound that setting r = 16 achieved a good balance between\naccuracy and complexity and consequently, we used this\nvalue for all experiments reported in this work.\n\n上記の内容に関して質問: (である調で記載)\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\n⑤比較の記載があれば違いを**表で**まとめて下さい。(無ければ無しと記載)\n⑥上記⑤以外で表に出来そうな部分があれば**表で**まとめて下さい。(無ければ無しと記載)\n\n**▼生成AIの回答(ChatGPT-4o)**\n① **何についての記載か**  \nこの節は、SEブロック設計の各コンポーネントの相対的重要性を理解するために行ったアブレーション実験に関する記載である。\n\n**要旨**\n- ImageNetデータセット上でのアブレーション実験を実施し、SEブロックの設計要素の影響を評価。\n- ResNet-50をバックボーンアーキテクチャとして使用。\n- 主に減少率\\( r \\)が性能と計算コストに与える影響を調査。\n\n② **改良点・工夫点・テクニック等の記載**  \n- SEブロックのハイパーパラメータである減少率\\( r \\)の最適化を行い、性能と計算コストのバランスを取ることに成功した。\n\n③ **性能が向上した記載**  \n- 減少率\\( r \\)を16に設定することで、性能と計算コストのバランスが最適化された。\n- 具体的な性能向上の値は無し。\n\n④ **メカニズムの解明・なぜそうなるのか等の記載**  \n- SEブロックがトレーニングセットのチャンネル依存関係に過適合する可能性があるため、性能が単調に向上しないことが示唆されている。\n\n⑤ **比較の記載**  \n以下に減少率\\( r \\)による性能の違いを表でまとめる。\n\n| 減少率 \\( r \\) | 性能の変化 | コメント |\n| ----------- | -------- | -------- |\n| \\( r = 16 \\) | 最適化されている | 精度と複雑性のバランスが良い |\n| その他の値   | 性能が向上しない場合もある | 過適合の可能性がある |\n\n⑥ **上記⑤以外で表に出来そうな部分**  \n無し\n\n## 6.2 Squeeze Operator\n\n**▼質問(人間)**\n(論文・記事の各章節を貼り付け)\n6.2 Squeeze Operator\nWe examine the significance of using global average pooling\nas opposed to global max pooling as our choice of squeeze\noperator (since this worked well, we did not consider more\nsophisticated alternatives). The results are reported in Ta-\nble 11. While both max and average pooling are effective,\naverage pooling achieves slightly better performance, jus-\ntifying its selection as the basis of the squeeze operation.\nHowever, we note that the performance of SE blocks is fairly\nrobust to the choice of specific aggregation operator.\n6.3 Excitation Operator\nWe next assess the choice of non-linearity for the excitation\nmechanism. We consider two further options: ReLU and\ntanh, and experiment with replacing the sigmoid with these\nalternative non-linearities. The results are reported in Ta-\nble 12. We see that exchanging the sigmoid for tanh slightly\nworsens performance, while using ReLU is dramatically\nworse and in fact causes the performance of SE-ResNet-50\nto drop below that of the ResNet-50 baseline. This suggests\nthat for the SE block to be effective, careful construction of\nthe excitation operator is important.\n6.4 Different stages\nWe explore the influence of SE blocks at different stages by\nintegrating SE blocks into ResNet-50, one stage at a time.\nSpecifically, we add SE blocks to the intermediate stages:\nstage 2, stage 3 and stage 4, and report the results in Ta-\nble 13. We observe that SE blocks bring performance benefits\nwhen introduced at each of these stages of the architecture.\nMoreover, the gains induced by SE blocks at different stages\nare complementary, in the sense that they can be combined\neffectively to further bolster network performance.\n\n上記の内容に関して質問: (である調で記載)\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\n⑤比較の記載があれば違いを**表で**まとめて下さい。(無ければ無しと記載)\n⑥上記⑤以外で表に出来そうな部分があれば**表で**まとめて下さい。(無ければ無しと記載)\n\n**▼生成AIの回答(ChatGPT-4o)**\n① **何についての記載か**  \nこの節は、SEブロックの各要素に関する詳細なアブレーションスタディの結果について記載している。\n\n**要旨**\n- Global average poolingとglobal max poolingの比較（Squeeze Operatorの選択）。\n- Excitation Operatorとしての非線形性の選択肢（sigmoid、ReLU、tanh）の評価。\n- SEブロックをResNet-50の異なるステージに導入した場合の影響の調査。\n\n② **改良点・工夫点・テクニック等の記載**  \n- Global average poolingの採用。\n- SigmoidをExcitation Operatorとして選択。\n\n③ **性能が向上した記載**  \n- Global average poolingがglobal max poolingよりもわずかに優れた性能を示す。\n- Sigmoidが最も効果的なExcitation Operatorであり、tanhやReLUと比較して性能が向上。\n- SEブロックをResNet-50の各ステージに導入することで、どのステージでも性能向上が見られる。\n\n④ **メカニズムの解明・なぜそうなるのか等の記載**  \n- SEブロックは、squeezeとexcitationの各段階での適切な選択により、ネットワークの性能を効果的に強化できることが示されている。\n\n⑤ **比較の記載**\n\n| 比較項目                         | 結果                          | コメント                            |\n| ------------------------------ | --------------------------- | ----------------------------------- |\n| Squeeze Operator               | Average poolingが最良       | Max poolingも有効だが、やや劣る       |\n| Excitation Operator            | Sigmoidが最良                | Tanhはわずかに悪化、ReLUは著しく悪化  |\n| SEブロックの導入ステージ        | 各ステージで性能向上         | どのステージでも導入が有効           |\n\n⑥ **上記⑤以外で表に出来そうな部分**  \n無し\n\n## 6.5 Integration strategy\n\n**▼質問(人間)**\n(論文・記事の各章節を貼り付け)\n6.5 Integration strategy\nFinally, we perform an ablation study to assess the influence\nof the location of the SE block when integrating it into exist-\ning architectures. In addition to the proposed SE design, we\nconsider three variants: (1) SE-PRE block, in which the SE\nblock is moved before the residual unit; (2) SE-POST block,\nin which the SE unit is moved after the summation with the\nidentity branch and (3) SE-Identity block, in which the SE\nunit is placed on the identity connection in parallel to the\nresidual unit. These variants are illustrated in Figure 5 and\nthe performance of each variant is reported in Table 14. We\nobserve that the SE-PRE, SE-Identity and proposed SE block\neach perform similarly well, while usage of the SE-POST\nblock leads to a drop in performance. This experiment\nsuggests that the performance improvements produced by\nSE units are fairly robust to their location, provided that\nthey are applied prior to branch aggregation.\nIn the experiments above, each SE block was placed\noutside the structure of a residual unit. We also construct\na variant of the design which moves the SE block inside\nthe residual unit, placing it directly after the 3 × 3 convo-\nlutional layer. Since the 3 × 3 convolutional layer possesses\nfewer channels, the number of parameters introduced by the\ncorresponding SE block is also reduced. The comparison in\nTable 15 shows that the SE 3×3 variant achieves comparable\nclassification accuracy with fewer parameters than the stan-\ndard SE block. Although it is beyond the scope of this work,\nwe anticipate that further efficiency gains will be achievable\nby tailoring SE block usage for specific architectures.\n\n上記の内容に関して質問: (である調で記載)\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\n⑤比較の記載があれば違いを**表で**まとめて下さい。(無ければ無しと記載)\n⑥上記⑤以外で表に出来そうな部分があれば**表で**まとめて下さい。(無ければ無しと記載)\n\n**▼生成AIの回答(ChatGPT-4o)**\n① **何についての記載か**  \nこの節は、SEブロックを既存のアーキテクチャに統合する際の位置による影響を評価したアブレーションスタディについて記載している。\n\n**要旨**\n- SEブロックの位置を変更した4つのバリアント（SE-PRE、SE-POST、SE-Identity）を検討。\n- 各バリアントの性能を比較し、SE-PRE、SE-Identity、提案されたSEブロックは良好な性能を示す。\n- SE-POSTブロックは性能低下を引き起こすことが確認された。\n- SEブロックを3×3の畳み込み層の後に配置するバリアントが紹介され、標準のSEブロックに比べてパラメータ数が減少しながらも同等の分類精度を達成することが示された。\n\n② **改良点・工夫点・テクニック等の記載**  \n- SEブロックを残差ユニットの前後や並行に配置することで、性能に与える影響を検証した点。\n- SEブロックを3×3の畳み込み層内に配置することで、パラメータ数を削減しつつ性能を維持する設計が提案された。\n\n③ **性能が向上した記載**  \n- SEブロックを3×3の畳み込み層の後に配置するバリアントが、パラメータ数を減少させながらも標準のSEブロックと同等の分類精度を達成したが、具体的な性能数値は示されていない。\n\n④ **メカニズムの解明・なぜそうなるのか等の記載**  \n- SEユニットの位置が残差ユニットの集約前であれば性能向上が見られ、これはSEユニットが情報の流れを妨げないことを示唆している。\n- SEブロックを3×3の畳み込み層内に配置することで、パラメータ数を削減できる一方、性能を維持できることが示されている。\n\n⑤ **比較の記載**\n\n| SEブロックの位置              | 性能                          | コメント                           |\n| -------------------------- | --------------------------- | ---------------------------------- |\n| SE-PRE                     | 良好な性能                   | 提案されたSEブロックと同様の性能   |\n| SE-POST                    | 性能低下                    | 情報の流れに悪影響を与える可能性   |\n| SE-Identity                | 良好な性能                   | 提案されたSEブロックと同様の性能   |\n| SE 3×3                    | 同等の分類精度              | パラメータ数が少なくなる          |\n\n⑥ **上記⑤以外で表に出来そうな部分があれば**表で**まとめて下さい。**  \n無し\n\n## 7 ROLE OF SE BLOCKS\n\n**▼質問(人間)**\n(論文・記事の各章節を貼り付け)\n7 ROLE OF SE BLOCKS\nAlthough the proposed SE block has been shown to improve\nnetwork performance on multiple visual tasks, we would\nalso like to understand the role of the squeeze operation and\nhow the excitation mechanism operates in practice. Unfortu-\nnately, a rigorous theoretical analysis of the representations\nlearned by deep neural networks remains challenging. We\ntherefore take an empirical approach to examining the role\nplayed by the SE block with the goal of attaining at least a\nprimitive understanding of its practical function.\n7.1 Effect of Squeeze\nTo assess whether the global embedding produced by the\nsqueeze operation plays an important role in performance,\nwe experiment with a variant of the SE block that adds an\nequal number of parameters, but does not perform global\naverage pooling. Specifically, we remove the pooling op-\neration and replace the two FC layers with corresponding\n1 × 1 convolutions with identical channel dimensions in\nthe excitation operator, namely NoSqueeze, where the ex-\ncitation output maintains the spatial dimensions as input.\nIn contrast to the SE block, these point-wise convolutions\ncan only remap the channels as a function of the output\nof a local operator. While in practice, the later layers of a\ndeep network will typically possess a (theoretical) global\nreceptive field, global embeddings are no longer directly\naccessible throughout the network in the NoSqueeze variant.\nThe accuracy and computational complexity of the both\nmodels are compared to a standard ResNet-50 model in\nTable 16. We observe that the use of global information has\na significant influence on the model performance, underlin-\ning the importance of the squeeze operation. Moreover, in\ncomparison to the NoSqueeze design, the SE block allows\nthis global information to be used in a computationally\nparsimonious manner.\n\n上記の内容に関して質問: (である調で記載)\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\n⑤比較の記載があれば違いを**表で**まとめて下さい。(無ければ無しと記載)\n⑥上記⑤以外で表に出来そうな部分があれば**表で**まとめて下さい。(無ければ無しと記載)\n\n**▼生成AIの回答(ChatGPT-4o)**\n① **何についての記載か**  \nこの節は、SEブロックの役割、特にスクイーズ操作とエキサイテーションメカニズムが実際にどのように機能しているかについての考察を行っている。\n\n**要旨**\n- スクイーズ操作が性能に及ぼす影響を評価するため、スクイーズ操作を除いたNoSqueezeバリアントと比較実験を行った。\n- グローバル情報がモデルの性能に重要な影響を与えていることを確認。\n- SEブロックは、計算効率を維持しながらグローバル情報を利用できる設計であることを示している。\n\n② **改良点・工夫点・テクニック等の記載**  \n- スクイーズ操作を排除したNoSqueezeバリアントを提案し、グローバル情報の有無による性能の違いを検証した。\n\n③ **性能が向上した記載**  \n- スクイーズ操作を含むSEブロックは、NoSqueezeバリアントと比較して、性能が向上しているが、具体的な数値は示されていない。\n\n④ **メカニズムの解明・なぜそうなるのか等の記載**  \n- スクイーズ操作によるグローバル情報が、ネットワーク全体で重要な役割を果たしており、その結果、モデルの性能が向上することが示されている。\n\n⑤ **比較の記載**\n\n| モデル           | グローバル情報の使用 | 計算効率    | 性能      |\n| --------------- | ------------------- | --------- | -------- |\n| 標準ResNet-50   | なし                 | 高い        | 基準性能   |\n| NoSqueeze       | なし                 | 低い       | 低下      |\n| SEブロック       | あり                 | 高い       | 向上      |\n\n⑥ **上記⑤以外で表に出来そうな部分があれば**表で**まとめて下さい。**  \n無し\n\n## 7.2 Role of Excitation\n\n**▼質問(人間)**\n(論文・記事の各章節を貼り付け)\n7.2 Role of Excitation\nTo provide a clearer picture of the function of the excitation\noperator in SE blocks, in this section we study example\nactivations from the SE-ResNet-50 model and examine their\ndistribution with respect to different classes and different\ninput images at various depths in the network. In particular,\nwe would like to understand how excitations vary across\nimages of different classes, and across images within a class.\nWe first consider the distribution of excitations for dif-\nferent classes. Specifically, we sample four classes from the\nImageNet dataset that exhibit semantic and appearance di-\nversity, namely goldfish, pug, plane and cliff (example images\nfrom these classes are shown in Figure 6). We then draw\nfifty samples for each class from the validation set and\ncompute the average activations for fifty uniformly sampled\nchannels in the last SE block of each stage (immediately\nprior to downsampling) and plot their distribution in Fig. 7.\nFor reference, we also plot the distribution of the mean\nactivations across all of the 1000 classes.\nWe make the following three observations about the\nrole of the excitation operation. First, the distribution across\ndifferent classes is very similar at the earlier layers of the\nnetwork, e.g. SE 2 3. This suggests that the importance of\nfeature channels is likely to be shared by different classes\nin the early stages. The second observation is that at greater\ndepth, the value of each channel becomes much more class-\nspecific as different classes exhibit different preferences to\nthe discriminative value of features, e.g. SE 4 6 and SE 5 1.\nThese observations are consistent with findings in previous\nwork [81], [82], namely that earlier layer features are typi-\ncally more general (e.g. class agnostic in the context of the\nclassification task) while later layer features exhibit greater\nlevels of specificity [83].\nNext, we observe a somewhat different phenomena in\nthe last stage of the network. SE 5 2 exhibits an interesting\ntendency towards a saturated state in which most of the\nactivations are close to one and the remainder is close to\nzero. At the point at which all activations take the value one,\nan SE block reduces to the identity operator. At the end of\nthe network in the SE 5 3 (which is immediately followed\nby global pooling prior before classifiers), a similar pattern\nemerges over different classes, up to a slight change in scale\n(which could be tuned by the classifiers). This suggests that\nSE 5 2 and SE 5 3 are less important than previous blocks\nin providing recalibration to the network. This finding is\nconsistent with the result of the empirical investigation in\nSec. 4 which demonstrated that the additional parameter\ncount could be significantly reduced by removing the SE\nblocks for the last stage with only a marginal loss of perfor-\nmance.\nFinally, we show the mean and standard deviations of\nthe activations for image instances within the same class for\ntwo sample classes (goldfish and plane) in Fig. 8. We observe a\ntrend consistent with the inter-class visualisation, indicating\nthat the dynamic behaviour of SE blocks varies over both\nclasses and instances within a class. Particularly in the later\nlayers of the network where there is considerable diversity\nof representation within a single class, the network learns\nto take advantage of feature recalibration to improve its\ndiscriminative performance. In summary, SE blocks produce\ninstance-specific responses which nevertheless function to\nsupport the increasingly class-specific needs of the model at\ndifferent layers in the architecture.\n\n上記の内容に関して質問: (である調で記載)\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\n⑤比較の記載があれば違いを**表で**まとめて下さい。(無ければ無しと記載)\n⑥上記⑤以外で表に出来そうな部分があれば**表で**まとめて下さい。(無ければ無しと記載)\n\n**▼生成AIの回答(ChatGPT-4o)**\n① **何についての記載か**  \nこの節は、SEブロックにおけるエキサイテーション（Excitation）操作の役割を解明し、その機能をより明確にするための実験とその結果を説明している。\n\n**要旨**\n- SEブロックのエキサイテーション操作が異なるクラスや異なる入力画像に対してどのように分布しているかを調査。\n- ネットワークの深さによるエキサイテーションのクラス依存性を検討。\n- ネットワークの最後の段階では、エキサイテーションが飽和状態に近づく傾向があることを観察。\n- SEブロックがクラス固有の要求を満たすためにインスタンスごとの応答を生成していることを確認。\n\n② **改良点・工夫点・テクニック等の記載**  \n- クラス間およびクラス内でのエキサイテーション分布を比較するための具体的な実験を実施し、エキサイテーション操作の重要性を検証した。\n\n③ **性能が向上した記載**  \n- 性能の向上に関する具体的な数値は記載されていないが、エキサイテーション操作がネットワークの識別性能を改善する役割を果たしていることが示唆されている。\n\n④ **メカニズムの解明・なぜそうなるのか等の記載**  \n- ネットワークの初期層では、異なるクラス間でのエキサイテーション分布が類似しているが、後半の層になるとクラスごとにエキサイテーションの値が特異的になることが示されている。\n- 最後の段階では、エキサイテーションが飽和状態に近づき、SEブロックがアイデンティティ演算子として機能する傾向がある。\n\n⑤ **比較の記載**\n\n| ネットワーク層 | エキサイテーションの分布特性           | クラス依存性  | 重要度              |\n| ------------- | ----------------------------------- | ----------- | ------------------- |\n| 初期層 (SE 2 3) | 異なるクラス間で類似した分布            | 低い         | 一般的な特徴を抽出  |\n| 中間層 (SE 4 6, SE 5 1) | クラスごとに特異的なエキサイテーション | 高い         | クラス固有の特徴を強調 |\n| 最終層 (SE 5 2, SE 5 3) | エキサイテーションが飽和に近づく      | 低い         | 重要度は低い        |\n\n⑥ **上記⑤以外で表に出来そうな部分があれば**表で**まとめて下さい。**  \n無し\n\n## 8 CONCLUSION\n\n**▼質問(人間)**\n(論文・記事の各章節を貼り付け)\n8 CONCLUSION\nIn this paper we proposed the SE block, an architectural\nunit designed to improve the representational power of a\nnetwork by enabling it to perform dynamic channel-wise\nfeature recalibration. A wide range of experiments show\nthe effectiveness of SENets, which achieve state-of-the-art\nperformance across multiple datasets and tasks. In addition,\nSE blocks shed some light on the inability of previous\narchitectures to adequately model channel-wise feature de-\npendencies. We hope this insight may prove useful for other\ntasks requiring strong discriminative features. Finally, the\nfeature importance values produced by SE blocks may be\nof use for other tasks such as network pruning for model\ncompression.\n\n上記の内容に関して質問: (である調で記載)\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\n⑤比較の記載があれば違いを**表で**まとめて下さい。(無ければ無しと記載)\n⑥上記⑤以外で表に出来そうな部分があれば**表で**まとめて下さい。(無ければ無しと記載)\n\n**▼生成AIの回答(ChatGPT-4o)**\n① **何についての記載か**  \nこの節は、SEブロック（Squeeze-and-Excitation Block）の提案とその効果についての結論をまとめている。\n\n**要旨**\n- SEブロックは、チャネルごとの特徴再キャリブレーションを動的に行うことで、ネットワークの表現力を向上させることを目的として設計された。\n- 多岐にわたる実験により、SENetsが複数のデータセットやタスクで最先端の性能を達成することが示された。\n- SEブロックは、従来のアーキテクチャがチャネルごとの特徴依存関係を十分にモデル化できていないことを明らかにした。\n- SEブロックによって生成される特徴の重要度は、モデル圧縮のためのネットワークプルーニングなど、他のタスクにも有用である可能性がある。\n\n② **改良点・工夫点・テクニック等の記載**  \n- SEブロックの設計自体が改良点であり、動的チャネルワイズ特徴再キャリブレーションという新しいアプローチを導入している。\n\n③ **性能が向上した記載**  \n- SENetsは、複数のデータセットとタスクにおいて最先端の性能を達成したと記載されているが、具体的な数値は示されていない。\n\n④ **メカニズムの解明・なぜそうなるのか等の記載**  \n- SEブロックは、従来のアーキテクチャがチャネルごとの特徴依存関係を十分にモデル化できていなかったことを明らかにし、その問題を解決するためのアプローチを提供している。\n\n⑤ **比較の記載**  \n- 無し\n\n⑥ **上記⑤以外で表に出来そうな部分があれば**表で**まとめて下さい。**  \n- 無し\n\n## APPENDIX: DETAILS OF SENET-154\n\n**▼質問(人間)**\n(論文・記事の各章節を貼り付け)\nAPPENDIX: DETAILS OF SENET-154\nSENet-154 is constructed by incorporating SE blocks into a\nmodified version of the 64×4d ResNeXt-152 which extends\nthe original ResNeXt-101 [19] by adopting the block stack-\ning strategy of ResNet-152 [13]. Further differences to the\ndesign and training of this model (beyond the use of SE\nblocks) are as follows: (a) The number of the first 1 × 1\nconvolutional channels for each bottleneck building block\nwas halved to reduce the computational cost of the model\nwith a minimal decrease in performance. (b) The first 7 × 7\nconvolutional layer was replaced with three consecutive\n3 × 3 convolutional layers. (c) The 1 × 1 down-sampling\nprojection with stride-2 convolution was replaced with a\n3 × 3 stride-2 convolution to preserve information. (d) A\ndropout layer (with a dropout ratio of 0.2) was inserted\nbefore the classification layer to reduce overfitting. (e) Label-\nsmoothing regularisation (as introduced in [20]) was used\nduring training. (f) The parameters of all BN layers were\nfrozen for the last few training epochs to ensure consistency\nbetween training and testing. (g) Training was performed\nwith 8 servers (64 GPUs) in parallel to enable large batch\nsizes (2048). The initial learning rate was set to 1.0.\n\n上記の内容に関して質問: (である調で記載)\n①何についての記載か? + 要旨は何? (要旨は箇条書きで記載)\n②改良点・工夫点・テクニック等の記載があれば説明下さい。(無ければ無しと記載)\n③性能が向上した記載があれば説明下さい。(具体値があれば併記、無ければ無しと記載)\n④メカニズムの解明・なぜそうなるのか等の記載があれば説明下さい。(無ければ無しと記載)\n⑤比較の記載があれば違いを**表で**まとめて下さい。(無ければ無しと記載)\n⑥上記⑤以外で表に出来そうな部分があれば**表で**まとめて下さい。(無ければ無しと記載)\n\n**▼生成AIの回答(ChatGPT-4o)**\n① **何についての記載か**  \nこの節は、SENet-154という特定のネットワークアーキテクチャの設計とその詳細について記載している。\n\n**要旨**\n- SENet-154は、64×4d ResNeXt-152をベースに、SEブロックを組み込んだネットワークアーキテクチャである。\n- ResNeXt-101を拡張し、ResNet-152のブロックスタッキング戦略を採用している。\n- モデルの設計とトレーニングにおいて、いくつかの改良と変更が行われている。\n\n② **改良点・工夫点・テクニック等の記載**\n- **1 × 1 畳み込みチャネルの減少**: ボトルネックビルディングブロックの最初の1 × 1 畳み込みチャネル数を半分にし、計算コストを削減しつつ性能の低下を最小限に抑える工夫をしている。\n- **畳み込みレイヤーの変更**: 最初の7 × 7 畳み込み層を3 × 3 畳み込み層に変更し、情報の保持を図っている。\n- **ダウンサンプリングプロジェクションの変更**: ストライド2の1 × 1 ダウンサンプリングプロジェクションを、3 × 3 のストライド2畳み込みに置き換え、情報の保持を図っている。\n- **ドロップアウト層の追加**: 過学習を防ぐために、分類層の前にドロップアウト層（ドロップアウト率0.2）を追加している。\n- **ラベルスムージング正則化の使用**: トレーニング中にラベルスムージング正則化を使用している。\n- **バッチノーマライゼーション（BN）パラメータの固定**: 最後のトレーニングエポックでは、トレーニングとテストの一貫性を確保するために、すべてのBNレイヤーのパラメータを固定している。\n- **並列トレーニング**: 大規模なバッチサイズ（2048）を可能にするために、8台のサーバー（64 GPU）を使用して並列トレーニングを行っている。\n\n③ **性能が向上した記載**\n- 無し\n\n④ **メカニズムの解明・なぜそうなるのか等の記載**\n- 無し\n\n⑤ **比較の記載**\n- 無し\n\n⑥ **上記⑤以外で表に出来そうな部分**\n- 無し\n\n\n",
    "coediting": false,
    "comments_count": 0,
    "created_at": "2024-08-22T13:05:04+09:00",
    "group": null,
    "id": "cf5bb954396595c4a340",
    "likes_count": 1,
    "private": false,
    "reactions_count": 0,
    "stocks_count": 0,
    "tags": [
      {
        "name": "機械学習",
        "versions": []
      },
      {
        "name": "プロンプト",
        "versions": []
      },
      {
        "name": "論文読み",
        "versions": []
      },
      {
        "name": "生成AI",
        "versions": []
      },
      {
        "name": "ChatGPT",
        "versions": []
      }
    ],
    "title": "生成AIを用いてSE blockの論文「Squeeze-and-Excitation Networks」を読んでみた (番号付き質問で内容把握)",
    "updated_at": "2024-08-22T13:06:39+09:00",
    "url": "https://qiita.com/dl_from_scratch/items/cf5bb954396595c4a340",
    "user": {
      "description": "",
      "facebook_id": "",
      "followees_count": 1,
      "followers_count": 9,
      "github_login_name": "DL-from-Scratch",
      "id": "dl_from_scratch",
      "items_count": 140,
      "linkedin_id": "",
      "location": "",
      "name": "DLビギナー",
      "organization": "",
      "permanent_id": 606924,
      "profile_image_url": "https://s3-ap-northeast-1.amazonaws.com/qiita-image-store/0/606924/8dcd1062a198667f417a820ffc0ac757d0cad019/large.png?1584881570",
      "team_only": false,
      "twitter_screen_name": null,
      "website_url": "https://twitter.com/dl_from_scra2"
    },
    "page_views_count": null,
    "team_membership": null,
    "organization_url_name": null,
    "slide": false
  },
  {
    "rendered_body": "\u003ch3 data-sourcepos=\"1:1-1:56\"\u003e\n\u003cspan id=\"1-nanoエディタの基本操作macの場合\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#1-nano%E3%82%A8%E3%83%87%E3%82%A3%E3%82%BF%E3%81%AE%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cmac%E3%81%AE%E5%A0%B4%E5%90%88\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e1. nanoエディタの基本操作（Macの場合）\u003c/h3\u003e\n\u003cp data-sourcepos=\"3:1-3:157\"\u003enanoは初心者にも使いやすいテキストエディタで、画面の下部に操作方法が表示されるため直感的に操作が可能です。\u003c/p\u003e\n\u003ch4 data-sourcepos=\"5:1-5:26\"\u003e\n\u003cspan id=\"ファイルを開く\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%82%92%E9%96%8B%E3%81%8F\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eファイルを開く\u003c/h4\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"bash\" data-sourcepos=\"6:1-8:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003enano ファイル名\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"9:1-9:4\"\u003e例:\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"bash\" data-sourcepos=\"10:1-12:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003enano /private/etc/hosts\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003ch4 data-sourcepos=\"14:1-14:17\"\u003e\n\u003cspan id=\"基本操作\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e基本操作\u003c/h4\u003e\n\u003cul data-sourcepos=\"15:1-32:0\"\u003e\n\u003cli data-sourcepos=\"15:1-17:2\"\u003e\n\u003cp data-sourcepos=\"15:3-15:74\"\u003e\u003cstrong\u003e保存する\u003c/strong\u003e: \u003ccode\u003eCtrl + O\u003c/code\u003e（「Write Out」）、Enterキーで確定\u003c/p\u003e\n\u003cul data-sourcepos=\"16:3-17:2\"\u003e\n\u003cli data-sourcepos=\"16:3-17:2\"\u003e画面下部に「File Name to Write:」と表示されるのでEnterキーを押して保存を確定します。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"18:1-20:0\"\u003e\n\u003cp data-sourcepos=\"18:3-18:46\"\u003e\u003cstrong\u003e終了する\u003c/strong\u003e: \u003ccode\u003eCtrl + X\u003c/code\u003e（「Exit」）\u003c/p\u003e\n\u003cul data-sourcepos=\"19:3-20:0\"\u003e\n\u003cli data-sourcepos=\"19:3-20:0\"\u003e変更がある場合、「Save modified buffer?」と表示されるので、\u003ccode\u003eY\u003c/code\u003e（Yes）を押して保存するか、\u003ccode\u003eN\u003c/code\u003e（No）を押して保存せずに終了します。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"21:1-23:0\"\u003e\n\u003cp data-sourcepos=\"21:3-21:50\"\u003e\u003cstrong\u003e検索する\u003c/strong\u003e: \u003ccode\u003eCtrl + W\u003c/code\u003e（「Where Is」）\u003c/p\u003e\n\u003cul data-sourcepos=\"22:3-23:0\"\u003e\n\u003cli data-sourcepos=\"22:3-23:0\"\u003e画面下部に「Search:」と表示されるので、検索したい文字列を入力してEnterキーを押します。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"24:1-26:0\"\u003e\n\u003cp data-sourcepos=\"24:3-24:45\"\u003e\u003cstrong\u003eカット（切り取り）\u003c/strong\u003e: \u003ccode\u003eCtrl + K\u003c/code\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"25:3-26:0\"\u003e\n\u003cli data-sourcepos=\"25:3-26:0\"\u003eカーソルがある行全体をカットします。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"27:1-29:0\"\u003e\n\u003cp data-sourcepos=\"27:3-27:30\"\u003e\u003cstrong\u003eペースト\u003c/strong\u003e: \u003ccode\u003eCtrl + U\u003c/code\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"28:3-29:0\"\u003e\n\u003cli data-sourcepos=\"28:3-29:0\"\u003eカットした内容をカーソル位置にペーストします。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"30:1-32:0\"\u003e\n\u003cp data-sourcepos=\"30:3-30:36\"\u003e\u003cstrong\u003eヘルプを表示\u003c/strong\u003e: \u003ccode\u003eCtrl + G\u003c/code\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"31:3-32:0\"\u003e\n\u003cli data-sourcepos=\"31:3-32:0\"\u003enanoの操作方法に関するヘルプが表示されます。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 data-sourcepos=\"33:1-33:55\"\u003e\n\u003cspan id=\"1-vimエディタの基本操作macの場合\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#1-vim%E3%82%A8%E3%83%87%E3%82%A3%E3%82%BF%E3%81%AE%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9Cmac%E3%81%AE%E5%A0%B4%E5%90%88\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e1. vimエディタの基本操作（Macの場合）\u003c/h3\u003e\n\u003cp data-sourcepos=\"35:1-35:264\"\u003evimはより強力でカスタマイズ性が高いエディタですが、操作に慣れるまで時間がかかることがあります。vimには「通常モード」と「挿入モード」があり、モードに応じてキーの動作が異なります。\u003c/p\u003e\n\u003ch4 data-sourcepos=\"37:1-37:26\"\u003e\n\u003cspan id=\"ファイルを開く-1\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%82%92%E9%96%8B%E3%81%8F-1\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eファイルを開く\u003c/h4\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"bash\" data-sourcepos=\"38:1-40:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003evim ファイル名\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"41:1-41:4\"\u003e例:\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"bash\" data-sourcepos=\"42:1-44:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003evim /private/etc/hosts\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003ch4 data-sourcepos=\"46:1-46:17\"\u003e\n\u003cspan id=\"基本操作-1\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C-1\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e基本操作\u003c/h4\u003e\n\u003cul data-sourcepos=\"48:1-71:0\"\u003e\n\u003cli data-sourcepos=\"48:1-50:0\"\u003e\n\u003cp data-sourcepos=\"48:3-48:35\"\u003e\u003cstrong\u003e挿入モードに入る\u003c/strong\u003e: \u003ccode\u003ei\u003c/code\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"49:3-50:0\"\u003e\n\u003cli data-sourcepos=\"49:3-50:0\"\u003eこれでテキストの入力が可能になります。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"51:1-53:0\"\u003e\n\u003cp data-sourcepos=\"51:3-51:64\"\u003e\u003cstrong\u003e挿入モードを終了し通常モードに戻る\u003c/strong\u003e: \u003ccode\u003eEsc\u003c/code\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"52:3-53:0\"\u003e\n\u003cli data-sourcepos=\"52:3-53:0\"\u003e挿入モードから通常モードに戻ります。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"54:1-56:0\"\u003e\n\u003cp data-sourcepos=\"54:3-54:37\"\u003e\u003cstrong\u003e保存して終了する\u003c/strong\u003e: \u003ccode\u003e:wq\u003c/code\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"55:3-56:0\"\u003e\n\u003cli data-sourcepos=\"55:3-56:0\"\u003e通常モードで \u003ccode\u003e:\u003c/code\u003e を押してコマンドラインモードに入り、\u003ccode\u003ewq\u003c/code\u003e と入力してEnterキーを押します。\u003ccode\u003ew\u003c/code\u003eは「write（保存）」、\u003ccode\u003eq\u003c/code\u003eは「quit（終了）」の意味です。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"57:1-59:0\"\u003e\n\u003cp data-sourcepos=\"57:3-57:40\"\u003e\u003cstrong\u003e保存せずに終了する\u003c/strong\u003e: \u003ccode\u003e:q!\u003c/code\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"58:3-59:0\"\u003e\n\u003cli data-sourcepos=\"58:3-59:0\"\u003e通常モードで \u003ccode\u003e:\u003c/code\u003e を押してコマンドラインモードに入り、\u003ccode\u003eq!\u003c/code\u003e と入力してEnterキーを押します。\u003ccode\u003e!\u003c/code\u003eは強制終了の意味です。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"60:1-62:0\"\u003e\n\u003cp data-sourcepos=\"60:3-60:38\"\u003e\u003cstrong\u003e検索する\u003c/strong\u003e: \u003ccode\u003e/検索文字列\u003c/code\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"61:3-62:0\"\u003e\n\u003cli data-sourcepos=\"61:3-62:0\"\u003e通常モードで \u003ccode\u003e/\u003c/code\u003e を押してから検索したい文字列を入力し、Enterキーを押します。次の一致箇所に移動するには \u003ccode\u003en\u003c/code\u003e キーを押します。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"63:1-65:0\"\u003e\n\u003cp data-sourcepos=\"63:3-63:27\"\u003e\u003cstrong\u003eコピーする\u003c/strong\u003e: \u003ccode\u003eyy\u003c/code\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"64:3-65:0\"\u003e\n\u003cli data-sourcepos=\"64:3-65:0\"\u003e通常モードで \u003ccode\u003eyy\u003c/code\u003e を押すと、カーソルがある行全体をコピーします。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"66:1-68:0\"\u003e\n\u003cp data-sourcepos=\"66:3-66:29\"\u003e\u003cstrong\u003eペーストする\u003c/strong\u003e: \u003ccode\u003ep\u003c/code\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"67:3-68:0\"\u003e\n\u003cli data-sourcepos=\"67:3-68:0\"\u003e通常モードで \u003ccode\u003ep\u003c/code\u003e を押すと、コピーした内容をカーソルの次の行にペーストします。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"69:1-71:0\"\u003e\n\u003cp data-sourcepos=\"69:3-69:39\"\u003e\u003cstrong\u003eカット（切り取り）\u003c/strong\u003e: \u003ccode\u003edd\u003c/code\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"70:3-71:0\"\u003e\n\u003cli data-sourcepos=\"70:3-71:0\"\u003e通常モードで \u003ccode\u003edd\u003c/code\u003e を押すと、カーソルがある行全体をカットします。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"72:1-72:293\"\u003e申し訳ありません、説明が少し混乱させてしまったかもしれません。モードについての説明は主に\u003ccode\u003evim\u003c/code\u003eエディタに関するものです。\u003ccode\u003enano\u003c/code\u003eエディタにはモードという概念はなく、すべての操作が同じ画面から行われます。\u003c/p\u003e\n\u003ch3 data-sourcepos=\"74:1-74:46\"\u003e\n\u003cspan id=\"3-vimエディタのモードについて\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#3-vim%E3%82%A8%E3%83%87%E3%82%A3%E3%82%BF%E3%81%AE%E3%83%A2%E3%83%BC%E3%83%89%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e3. vimエディタのモードについて\u003c/h3\u003e\n\u003ch4 data-sourcepos=\"76:1-76:20\"\u003e\n\u003cspan id=\"通常モード\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E9%80%9A%E5%B8%B8%E3%83%A2%E3%83%BC%E3%83%89\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e通常モード\u003c/h4\u003e\n\u003cul data-sourcepos=\"77:1-84:0\"\u003e\n\u003cli data-sourcepos=\"77:1-77:146\"\u003e\n\u003cstrong\u003e説明\u003c/strong\u003e: vimを起動したときに最初に入るモードです。テキストの編集や移動などのコマンドを入力します。\u003c/li\u003e\n\u003cli data-sourcepos=\"78:1-84:0\"\u003e\n\u003cstrong\u003e主なコマンド\u003c/strong\u003e:\n\u003cul data-sourcepos=\"79:3-84:0\"\u003e\n\u003cli data-sourcepos=\"79:3-79:22\"\u003e\n\u003ccode\u003edd\u003c/code\u003e: 行を削除\u003c/li\u003e\n\u003cli data-sourcepos=\"80:3-80:25\"\u003e\n\u003ccode\u003eyy\u003c/code\u003e: 行をコピー\u003c/li\u003e\n\u003cli data-sourcepos=\"81:3-81:21\"\u003e\n\u003ccode\u003ep\u003c/code\u003e: ペースト\u003c/li\u003e\n\u003cli data-sourcepos=\"82:3-82:48\"\u003e\n\u003ccode\u003e:\u003c/code\u003e: コマンドラインモードに移行\u003c/li\u003e\n\u003cli data-sourcepos=\"83:3-84:0\"\u003e\n\u003ccode\u003e/\u003c/code\u003e: 検索モードに移行\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 data-sourcepos=\"85:1-85:20\"\u003e\n\u003cspan id=\"挿入モード\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E6%8C%BF%E5%85%A5%E3%83%A2%E3%83%BC%E3%83%89\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e挿入モード\u003c/h4\u003e\n\u003cul data-sourcepos=\"86:1-91:0\"\u003e\n\u003cli data-sourcepos=\"86:1-86:140\"\u003e\n\u003cstrong\u003e説明\u003c/strong\u003e: テキストを直接入力するモードです。通常モードから\u003ccode\u003ei\u003c/code\u003eキーを押すとこのモードに入ります。\u003c/li\u003e\n\u003cli data-sourcepos=\"87:1-91:0\"\u003e\n\u003cstrong\u003e主なコマンド\u003c/strong\u003e:\n\u003cul data-sourcepos=\"88:3-91:0\"\u003e\n\u003cli data-sourcepos=\"88:3-88:54\"\u003e\n\u003ccode\u003ei\u003c/code\u003e: カーソルの位置にテキストを挿入\u003c/li\u003e\n\u003cli data-sourcepos=\"89:3-89:60\"\u003e\n\u003ccode\u003ea\u003c/code\u003e: カーソルの次の位置にテキストを挿入\u003c/li\u003e\n\u003cli data-sourcepos=\"90:3-91:0\"\u003e\n\u003ccode\u003eEsc\u003c/code\u003e: 挿入モードを終了し、通常モードに戻る\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 data-sourcepos=\"92:1-92:35\"\u003e\n\u003cspan id=\"コマンドラインモード\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%82%B3%E3%83%9E%E3%83%B3%E3%83%89%E3%83%A9%E3%82%A4%E3%83%B3%E3%83%A2%E3%83%BC%E3%83%89\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eコマンドラインモード\u003c/h4\u003e\n\u003cul data-sourcepos=\"93:1-99:0\"\u003e\n\u003cli data-sourcepos=\"93:1-93:152\"\u003e\n\u003cstrong\u003e説明\u003c/strong\u003e: 通常モードで\u003ccode\u003e:\u003c/code\u003eを押すとこのモードに入ります。ファイルの保存や終了などのコマンドを入力します。\u003c/li\u003e\n\u003cli data-sourcepos=\"94:1-99:0\"\u003e\n\u003cstrong\u003e主なコマンド\u003c/strong\u003e:\n\u003cul data-sourcepos=\"95:3-99:0\"\u003e\n\u003cli data-sourcepos=\"95:3-95:31\"\u003e\n\u003ccode\u003e:w\u003c/code\u003e: ファイルを保存\u003c/li\u003e\n\u003cli data-sourcepos=\"96:3-96:22\"\u003e\n\u003ccode\u003e:q\u003c/code\u003e: vimを終了\u003c/li\u003e\n\u003cli data-sourcepos=\"97:3-97:29\"\u003e\n\u003ccode\u003e:wq\u003c/code\u003e: 保存して終了\u003c/li\u003e\n\u003cli data-sourcepos=\"98:3-99:0\"\u003e\n\u003ccode\u003e:q!\u003c/code\u003e: 保存せずに強制終了\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 data-sourcepos=\"100:1-100:29\"\u003e\n\u003cspan id=\"ビジュアルモード\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%83%93%E3%82%B8%E3%83%A5%E3%82%A2%E3%83%AB%E3%83%A2%E3%83%BC%E3%83%89\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eビジュアルモード\u003c/h4\u003e\n\u003cul data-sourcepos=\"101:1-105:74\"\u003e\n\u003cli data-sourcepos=\"101:1-101:134\"\u003e\n\u003cstrong\u003e説明\u003c/strong\u003e: テキストを選択するモードです。通常モードから\u003ccode\u003ev\u003c/code\u003eキーを押すとこのモードに入ります。\u003c/li\u003e\n\u003cli data-sourcepos=\"102:1-105:74\"\u003e\n\u003cstrong\u003e主なコマンド\u003c/strong\u003e:\n\u003cul data-sourcepos=\"103:3-105:74\"\u003e\n\u003cli data-sourcepos=\"103:3-103:42\"\u003e\n\u003ccode\u003ev\u003c/code\u003e: ビジュアルモードに入る\u003c/li\u003e\n\u003cli data-sourcepos=\"104:3-104:66\"\u003e\n\u003ccode\u003eV\u003c/code\u003e: 行全体を選択するビジュアルモードに入る\u003c/li\u003e\n\u003cli data-sourcepos=\"105:3-105:74\"\u003e\n\u003ccode\u003eEsc\u003c/code\u003e: ビジュアルモードを終了し、通常モードに戻る\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "body": "### 1. nanoエディタの基本操作（Macの場合）\n\nnanoは初心者にも使いやすいテキストエディタで、画面の下部に操作方法が表示されるため直感的に操作が可能です。\n\n#### ファイルを開く\n```bash\nnano ファイル名\n```\n例:\n```bash\nnano /private/etc/hosts\n```\n\n#### 基本操作\n- **保存する**: `Ctrl + O`（「Write Out」）、Enterキーで確定\n  - 画面下部に「File Name to Write:」と表示されるのでEnterキーを押して保存を確定します。\n  \n- **終了する**: `Ctrl + X`（「Exit」）\n  - 変更がある場合、「Save modified buffer?」と表示されるので、`Y`（Yes）を押して保存するか、`N`（No）を押して保存せずに終了します。\n\n- **検索する**: `Ctrl + W`（「Where Is」）\n  - 画面下部に「Search:」と表示されるので、検索したい文字列を入力してEnterキーを押します。\n\n- **カット（切り取り）**: `Ctrl + K`\n  - カーソルがある行全体をカットします。\n\n- **ペースト**: `Ctrl + U`\n  - カットした内容をカーソル位置にペーストします。\n\n- **ヘルプを表示**: `Ctrl + G`\n  - nanoの操作方法に関するヘルプが表示されます。\n\n### 1. vimエディタの基本操作（Macの場合）\n\nvimはより強力でカスタマイズ性が高いエディタですが、操作に慣れるまで時間がかかることがあります。vimには「通常モード」と「挿入モード」があり、モードに応じてキーの動作が異なります。\n\n#### ファイルを開く\n```bash\nvim ファイル名\n```\n例:\n```bash\nvim /private/etc/hosts\n```\n\n#### 基本操作\n\n- **挿入モードに入る**: `i`\n  - これでテキストの入力が可能になります。\n\n- **挿入モードを終了し通常モードに戻る**: `Esc`\n  - 挿入モードから通常モードに戻ります。\n\n- **保存して終了する**: `:wq`\n  - 通常モードで `:` を押してコマンドラインモードに入り、`wq` と入力してEnterキーを押します。`w`は「write（保存）」、`q`は「quit（終了）」の意味です。\n\n- **保存せずに終了する**: `:q!`\n  - 通常モードで `:` を押してコマンドラインモードに入り、`q!` と入力してEnterキーを押します。`!`は強制終了の意味です。\n\n- **検索する**: `/検索文字列`\n  - 通常モードで `/` を押してから検索したい文字列を入力し、Enterキーを押します。次の一致箇所に移動するには `n` キーを押します。\n\n- **コピーする**: `yy`\n  - 通常モードで `yy` を押すと、カーソルがある行全体をコピーします。\n\n- **ペーストする**: `p`\n  - 通常モードで `p` を押すと、コピーした内容をカーソルの次の行にペーストします。\n\n- **カット（切り取り）**: `dd`\n  - 通常モードで `dd` を押すと、カーソルがある行全体をカットします。\n\n申し訳ありません、説明が少し混乱させてしまったかもしれません。モードについての説明は主に`vim`エディタに関するものです。`nano`エディタにはモードという概念はなく、すべての操作が同じ画面から行われます。\n\n### 3. vimエディタのモードについて\n\n#### 通常モード\n- **説明**: vimを起動したときに最初に入るモードです。テキストの編集や移動などのコマンドを入力します。\n- **主なコマンド**:\n  - `dd`: 行を削除\n  - `yy`: 行をコピー\n  - `p`: ペースト\n  - `:`: コマンドラインモードに移行\n  - `/`: 検索モードに移行\n\n#### 挿入モード\n- **説明**: テキストを直接入力するモードです。通常モードから`i`キーを押すとこのモードに入ります。\n- **主なコマンド**:\n  - `i`: カーソルの位置にテキストを挿入\n  - `a`: カーソルの次の位置にテキストを挿入\n  - `Esc`: 挿入モードを終了し、通常モードに戻る\n\n#### コマンドラインモード\n- **説明**: 通常モードで`:`を押すとこのモードに入ります。ファイルの保存や終了などのコマンドを入力します。\n- **主なコマンド**:\n  - `:w`: ファイルを保存\n  - `:q`: vimを終了\n  - `:wq`: 保存して終了\n  - `:q!`: 保存せずに強制終了\n\n#### ビジュアルモード\n- **説明**: テキストを選択するモードです。通常モードから`v`キーを押すとこのモードに入ります。\n- **主なコマンド**:\n  - `v`: ビジュアルモードに入る\n  - `V`: 行全体を選択するビジュアルモードに入る\n  - `Esc`: ビジュアルモードを終了し、通常モードに戻る\n",
    "coediting": false,
    "comments_count": 0,
    "created_at": "2024-08-22T13:00:00+09:00",
    "group": null,
    "id": "ac6163405803b16a9386",
    "likes_count": 0,
    "private": false,
    "reactions_count": 0,
    "stocks_count": 0,
    "tags": [
      {
        "name": "Vim",
        "versions": []
      },
      {
        "name": "Linux",
        "versions": []
      },
      {
        "name": "nano",
        "versions": []
      },
      {
        "name": "Linuxコマンド",
        "versions": []
      }
    ],
    "title": "nanoエディタとvimエディタの使い方（まとめ）",
    "updated_at": "2024-08-22T13:00:00+09:00",
    "url": "https://qiita.com/morrrrr/items/ac6163405803b16a9386",
    "user": {
      "description": "",
      "facebook_id": "",
      "followees_count": 27,
      "followers_count": 3,
      "github_login_name": "mori-tt",
      "id": "morrrrr",
      "items_count": 14,
      "linkedin_id": "",
      "location": "",
      "name": "",
      "organization": "",
      "permanent_id": 3623923,
      "profile_image_url": "https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3623923/profile-images/1716067843",
      "team_only": false,
      "twitter_screen_name": null,
      "website_url": "https://github.com/mori-tt"
    },
    "page_views_count": null,
    "team_membership": null,
    "organization_url_name": null,
    "slide": false
  },
  {
    "rendered_body": "\u003ch1 data-sourcepos=\"1:1-1:14\"\u003e\n\u003cspan id=\"はじめに\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eはじめに\u003c/h1\u003e\n\u003cp data-sourcepos=\"3:1-5:135\"\u003eモダンなJavaScript開発において、効率的で信頼性の高いテスト戦略を構築することは非常に重要です。\u003cbr\u003e\n本記事では、高速で使いやすいテストフレームワークである「Vitest」について詳しく解説します。\u003cbr\u003e\nVitestの基本的な使い方から、非同期テスト、モック、カバレッジまで、実践的な知識を提供します。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"7:1-7:15\"\u003e\n\u003cspan id=\"vitestとは\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#vitest%E3%81%A8%E3%81%AF\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eVitestとは\u003c/h2\u003e\n\u003cp data-sourcepos=\"9:1-10:140\"\u003eVitestは、Vite上に構築された高速なユニットテストフレームワークです。\u003cbr\u003e\nJest互換のAPIを持ちながら、Viteのエコシステムを活用して高速な実行と設定の簡素化を実現しています。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"12:1-12:39\"\u003e\n\u003cspan id=\"テストファイルの基本構造\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%83%86%E3%82%B9%E3%83%88%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%81%AE%E5%9F%BA%E6%9C%AC%E6%A7%8B%E9%80%A0\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eテストファイルの基本構造\u003c/h2\u003e\n\u003cp data-sourcepos=\"14:1-14:84\"\u003eVitestのテストファイルは、以下のような基本構造を持ちます：\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"typescript\" data-sourcepos=\"16:1-34:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"c1\"\u003e// 必要なモジュールをインポート\u003c/span\u003e\n\u003cspan class=\"k\"\u003eimport\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"nx\"\u003edescribe\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nx\"\u003eit\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nx\"\u003eexpect\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e \u003cspan class=\"k\"\u003efrom\u003c/span\u003e \u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"s1\"\u003evitest\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\n\u003cspan class=\"k\"\u003eimport\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"nx\"\u003esomeFunction\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e \u003cspan class=\"k\"\u003efrom\u003c/span\u003e \u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"s1\"\u003e./someModule\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e// someFunctionのテストスイートを定義\u003c/span\u003e\n\u003cspan class=\"nf\"\u003edescribe\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"s1\"\u003esomeFunction\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e// 期待される結果を返すかテスト\u003c/span\u003e\n  \u003cspan class=\"nf\"\u003eit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"s1\"\u003eshould return the expected result\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"kd\"\u003econst\u003c/span\u003e \u003cspan class=\"nx\"\u003eresult\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003esomeFunction\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n    \u003cspan class=\"nf\"\u003eexpect\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003eresult\u003c/span\u003e\u003cspan class=\"p\"\u003e).\u003c/span\u003e\u003cspan class=\"nf\"\u003etoBe\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"s1\"\u003eexpected result\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e})\u003c/span\u003e\n\n  \u003cspan class=\"c1\"\u003e// 無効な入力に対してエラーを投げるかテスト\u003c/span\u003e\n  \u003cspan class=\"nf\"\u003eit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"s1\"\u003eshould throw an error when given invalid input\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"nf\"\u003eexpect\u003c/span\u003e\u003cspan class=\"p\"\u003e(()\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan class=\"nf\"\u003esomeFunction\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kc\"\u003enull\u003c/span\u003e\u003cspan class=\"p\"\u003e)).\u003c/span\u003e\u003cspan class=\"nf\"\u003etoThrow\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"s1\"\u003eInvalid input\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e})\u003c/span\u003e\n\u003cspan class=\"p\"\u003e})\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"36:1-36:30\"\u003eここでのポイントは：\u003c/p\u003e\n\u003cul data-sourcepos=\"37:1-40:0\"\u003e\n\u003cli data-sourcepos=\"37:1-37:48\"\u003e\n\u003ccode\u003edescribe\u003c/code\u003eでテストのグループを定義\u003c/li\u003e\n\u003cli data-sourcepos=\"38:1-38:45\"\u003e\n\u003ccode\u003eit\u003c/code\u003eで個々のテストケースを記述\u003c/li\u003e\n\u003cli data-sourcepos=\"39:1-40:0\"\u003e\n\u003ccode\u003eexpect\u003c/code\u003eでアサーションを行う\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 data-sourcepos=\"41:1-41:36\"\u003e\n\u003cspan id=\"主要な関数とマッチャー\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E4%B8%BB%E8%A6%81%E3%81%AA%E9%96%A2%E6%95%B0%E3%81%A8%E3%83%9E%E3%83%83%E3%83%81%E3%83%A3%E3%83%BC\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e主要な関数とマッチャー\u003c/h2\u003e\n\u003cp data-sourcepos=\"43:1-43:84\"\u003eVitestでは、以下のような関数とマッチャーがよく使われます：\u003c/p\u003e\n\u003cul data-sourcepos=\"45:1-48:0\"\u003e\n\u003cli data-sourcepos=\"45:1-45:55\"\u003e\n\u003ccode\u003edescribe\u003c/code\u003e, \u003ccode\u003eit\u003c/code\u003e, \u003ccode\u003etest\u003c/code\u003e: テストの構造を定義\u003c/li\u003e\n\u003cli data-sourcepos=\"46:1-46:39\"\u003e\n\u003ccode\u003eexpect\u003c/code\u003e: アサーションを行う\u003c/li\u003e\n\u003cli data-sourcepos=\"47:1-48:0\"\u003e\n\u003ccode\u003etoBe()\u003c/code\u003e, \u003ccode\u003etoEqual()\u003c/code\u003e, \u003ccode\u003etoContain()\u003c/code\u003e, \u003ccode\u003etoThrow()\u003c/code\u003e: 値の比較や検証\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 data-sourcepos=\"49:1-49:21\"\u003e\n\u003cspan id=\"非同期テスト\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E9%9D%9E%E5%90%8C%E6%9C%9F%E3%83%86%E3%82%B9%E3%83%88\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e非同期テスト\u003c/h2\u003e\n\u003cp data-sourcepos=\"51:1-51:69\"\u003e非同期処理のテストも簡単に書くことができます：\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"typescript\" data-sourcepos=\"53:1-74:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"c1\"\u003e// 必要なモジュールをインポート\u003c/span\u003e\n\u003cspan class=\"k\"\u003eimport\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"nx\"\u003edescribe\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nx\"\u003eit\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nx\"\u003eexpect\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e \u003cspan class=\"k\"\u003efrom\u003c/span\u003e \u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"s1\"\u003evitest\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\n\u003cspan class=\"k\"\u003eimport\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"nx\"\u003efetchData\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e \u003cspan class=\"k\"\u003efrom\u003c/span\u003e \u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"s1\"\u003e./api\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e// fetchData関数のテストスイートを定義\u003c/span\u003e\n\u003cspan class=\"nf\"\u003edescribe\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"s1\"\u003efetchData\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e// 正常にデータを取得できるかテスト\u003c/span\u003e\n  \u003cspan class=\"nf\"\u003eit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"s1\"\u003eshould return data\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"k\"\u003easync \u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e// fetchData関数を呼び出し、結果を待機\u003c/span\u003e\n    \u003cspan class=\"kd\"\u003econst\u003c/span\u003e \u003cspan class=\"nx\"\u003edata\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003eawait\u003c/span\u003e \u003cspan class=\"nf\"\u003efetchData\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e// 返されたデータが期待通りの形式かチェック\u003c/span\u003e\n    \u003cspan class=\"nf\"\u003eexpect\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003edata\u003c/span\u003e\u003cspan class=\"p\"\u003e).\u003c/span\u003e\u003cspan class=\"nf\"\u003etoEqual\u003c/span\u003e\u003cspan class=\"p\"\u003e({\u003c/span\u003e \u003cspan class=\"na\"\u003eid\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"na\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"s1\"\u003eJohn\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e \u003cspan class=\"p\"\u003e})\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e})\u003c/span\u003e\n\n  \u003cspan class=\"c1\"\u003e// 無効なIDでエラーを投げるかテスト\u003c/span\u003e\n  \u003cspan class=\"nf\"\u003eit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"s1\"\u003eshould throw an error for invalid ID\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"k\"\u003easync \u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e// fetchDataに無効なID(-1)を渡し、エラーが発生することを期待\u003c/span\u003e\n    \u003cspan class=\"k\"\u003eawait\u003c/span\u003e \u003cspan class=\"nf\"\u003eexpect\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nf\"\u003efetchData\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)).\u003c/span\u003e\u003cspan class=\"nx\"\u003erejects\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003etoThrow\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"s1\"\u003eInvalid ID\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e})\u003c/span\u003e\n\u003cspan class=\"p\"\u003e})\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003ch2 data-sourcepos=\"76:1-76:24\"\u003e\n\u003cspan id=\"モックとスパイ\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%83%A2%E3%83%83%E3%82%AF%E3%81%A8%E3%82%B9%E3%83%91%E3%82%A4\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eモックとスパイ\u003c/h2\u003e\n\u003cp data-sourcepos=\"78:1-78:78\"\u003eVitestでは、関数やモジュールのモックも簡単に行えます：\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"typescript\" data-sourcepos=\"80:1-95:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"c1\"\u003e// Vitestからviオブジェクトをインポート\u003c/span\u003e\n\u003cspan class=\"k\"\u003eimport\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"nx\"\u003evi\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e \u003cspan class=\"k\"\u003efrom\u003c/span\u003e \u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"s1\"\u003evitest\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e// モック関数の作成\u003c/span\u003e\n\u003cspan class=\"kd\"\u003econst\u003c/span\u003e \u003cspan class=\"nx\"\u003emockFn\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nx\"\u003evi\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003efn\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e// モック関数の戻り値を設定\u003c/span\u003e\n\u003cspan class=\"nx\"\u003emockFn\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003emockReturnValue\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"s1\"\u003emocked value\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e// モジュール全体のモック\u003c/span\u003e\n\u003cspan class=\"nx\"\u003evi\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003emock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"s1\"\u003e./someModule\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e({\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e// someModule内のsomeFunctionをモック化\u003c/span\u003e\n  \u003cspan class=\"na\"\u003esomeFunction\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nx\"\u003evi\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003efn\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"nf\"\u003emockReturnValue\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"s1\"\u003emocked value\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}))\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003ch2 data-sourcepos=\"97:1-97:30\"\u003e\n\u003cspan id=\"セットアップと解体\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%82%BB%E3%83%83%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E3%81%A8%E8%A7%A3%E4%BD%93\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eセットアップと解体\u003c/h2\u003e\n\u003cp data-sourcepos=\"99:1-99:99\"\u003eテストの前後で共通の処理を行いたい場合は、以下の関数を使用します：\u003c/p\u003e\n\u003cul data-sourcepos=\"101:1-105:0\"\u003e\n\u003cli data-sourcepos=\"101:1-101:43\"\u003e\n\u003ccode\u003ebeforeEach\u003c/code\u003e: 各テストの前に実行\u003c/li\u003e\n\u003cli data-sourcepos=\"102:1-102:42\"\u003e\n\u003ccode\u003eafterEach\u003c/code\u003e: 各テストの後に実行\u003c/li\u003e\n\u003cli data-sourcepos=\"103:1-103:55\"\u003e\n\u003ccode\u003ebeforeAll\u003c/code\u003e: すべてのテストの前に1回実行\u003c/li\u003e\n\u003cli data-sourcepos=\"104:1-105:0\"\u003e\n\u003ccode\u003eafterAll\u003c/code\u003e: すべてのテストの後に1回実行\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 data-sourcepos=\"106:1-106:36\"\u003e\n\u003cspan id=\"スナップショットテスト\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%82%B9%E3%83%8A%E3%83%83%E3%83%97%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%E3%83%86%E3%82%B9%E3%83%88\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eスナップショットテスト\u003c/h2\u003e\n\u003cp data-sourcepos=\"108:1-108:135\"\u003eコンポーネントの出力が期待通りか確認するためのスナップショットテストも簡単に実装できます：\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"typescript\" data-sourcepos=\"110:1-115:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"nf\"\u003eit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"s1\"\u003eshould match snapshot\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n  \u003cspan class=\"kd\"\u003econst\u003c/span\u003e \u003cspan class=\"nx\"\u003euser\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"na\"\u003ename\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"s1\"\u003eJohn\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"na\"\u003eage\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e30\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n  \u003cspan class=\"nf\"\u003eexpect\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003euser\u003c/span\u003e\u003cspan class=\"p\"\u003e).\u003c/span\u003e\u003cspan class=\"nf\"\u003etoMatchSnapshot\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003cspan class=\"p\"\u003e})\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003ch2 data-sourcepos=\"117:1-117:18\"\u003e\n\u003cspan id=\"カバレッジ\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%82%AB%E3%83%90%E3%83%AC%E3%83%83%E3%82%B8\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eカバレッジ\u003c/h2\u003e\n\u003cp data-sourcepos=\"119:1-119:110\"\u003eコードカバレッジを取得するには、\u003ccode\u003epackage.json\u003c/code\u003eに以下のスクリプトを追加します：\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"json\" data-sourcepos=\"121:1-127:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n  \u003c/span\u003e\u003cspan class=\"nl\"\u003e\"scripts\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"w\"\u003e\n    \u003c/span\u003e\u003cspan class=\"nl\"\u003e\"test:coverage\"\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\u003cspan class=\"w\"\u003e \u003c/span\u003e\u003cspan class=\"s2\"\u003e\"vitest run --coverage\"\u003c/span\u003e\u003cspan class=\"w\"\u003e\n  \u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003ch1 data-sourcepos=\"129:1-129:11\"\u003e\n\u003cspan id=\"まとめ\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%81%BE%E3%81%A8%E3%82%81\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eまとめ\u003c/h1\u003e\n\u003cp data-sourcepos=\"131:1-131:474\"\u003eVitestは、モダンなJavaScript開発に適した高速で使いやすいテストフレームワークです。Jestと似たAPIを持ちながら、Viteのエコシステムを活用することで、より効率的なテスト環境を提供します。基本的なテストの書き方から、非同期テスト、モック、カバレッジまで、幅広い機能をサポートしているため、様々なプロジェクトで活用することができます。\u003c/p\u003e\n\u003cp data-sourcepos=\"133:1-133:264\"\u003eVitestを導入することで、開発者はより迅速かつ効果的にテストを書くことができ、結果としてプロジェクトの品質向上につながります。ぜひ、あなたのプロジェクトでVitestを試してみてください。\u003c/p\u003e\n\u003ch1 data-sourcepos=\"135:1-135:8\"\u003e\n\u003cspan id=\"参考\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E5%8F%82%E8%80%83\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e参考\u003c/h1\u003e\n\u003cul data-sourcepos=\"136:1-142:0\"\u003e\n\u003cli data-sourcepos=\"136:1-137:25\"\u003eVitest 公式ドキュメント\n\u003cul data-sourcepos=\"137:5-137:25\"\u003e\n\u003cli data-sourcepos=\"137:5-137:25\"\u003e\u003ca href=\"https://vitest.dev/\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://vitest.dev/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"138:1-139:25\"\u003eVite 公式サイト\n\u003cul data-sourcepos=\"139:5-139:25\"\u003e\n\u003cli data-sourcepos=\"139:5-139:25\"\u003e\u003ca href=\"https://vitejs.dev/\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://vitejs.dev/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli data-sourcepos=\"140:1-142:0\"\u003eJest から Vitest への移行ガイド\n\u003cul data-sourcepos=\"141:5-142:0\"\u003e\n\u003cli data-sourcepos=\"141:5-142:0\"\u003e\u003ca href=\"https://vitest.dev/guide/migration.html\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://vitest.dev/guide/migration.html\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "body": "# はじめに\n\nモダンなJavaScript開発において、効率的で信頼性の高いテスト戦略を構築することは非常に重要です。\n本記事では、高速で使いやすいテストフレームワークである「Vitest」について詳しく解説します。\nVitestの基本的な使い方から、非同期テスト、モック、カバレッジまで、実践的な知識を提供します。\n\n## Vitestとは\n\nVitestは、Vite上に構築された高速なユニットテストフレームワークです。\nJest互換のAPIを持ちながら、Viteのエコシステムを活用して高速な実行と設定の簡素化を実現しています。\n\n## テストファイルの基本構造\n\nVitestのテストファイルは、以下のような基本構造を持ちます：\n\n```typescript\n// 必要なモジュールをインポート\nimport { describe, it, expect } from 'vitest'\nimport { someFunction } from './someModule'\n\n// someFunctionのテストスイートを定義\ndescribe('someFunction', () =\u003e {\n  // 期待される結果を返すかテスト\n  it('should return the expected result', () =\u003e {\n    const result = someFunction()\n    expect(result).toBe('expected result')\n  })\n\n  // 無効な入力に対してエラーを投げるかテスト\n  it('should throw an error when given invalid input', () =\u003e {\n    expect(() =\u003e someFunction(null)).toThrow('Invalid input')\n  })\n})\n```\n\nここでのポイントは：\n- `describe`でテストのグループを定義\n- `it`で個々のテストケースを記述\n- `expect`でアサーションを行う\n\n## 主要な関数とマッチャー\n\nVitestでは、以下のような関数とマッチャーがよく使われます：\n\n- `describe`, `it`, `test`: テストの構造を定義\n- `expect`: アサーションを行う\n- `toBe()`, `toEqual()`, `toContain()`, `toThrow()`: 値の比較や検証\n\n## 非同期テスト\n\n非同期処理のテストも簡単に書くことができます：\n\n```typescript\n// 必要なモジュールをインポート\nimport { describe, it, expect } from 'vitest'\nimport { fetchData } from './api'\n\n// fetchData関数のテストスイートを定義\ndescribe('fetchData', () =\u003e {\n  // 正常にデータを取得できるかテスト\n  it('should return data', async () =\u003e {\n    // fetchData関数を呼び出し、結果を待機\n    const data = await fetchData()\n    // 返されたデータが期待通りの形式かチェック\n    expect(data).toEqual({ id: 1, name: 'John' })\n  })\n\n  // 無効なIDでエラーを投げるかテスト\n  it('should throw an error for invalid ID', async () =\u003e {\n    // fetchDataに無効なID(-1)を渡し、エラーが発生することを期待\n    await expect(fetchData(-1)).rejects.toThrow('Invalid ID')\n  })\n})\n```\n\n## モックとスパイ\n\nVitestでは、関数やモジュールのモックも簡単に行えます：\n\n```typescript\n// Vitestからviオブジェクトをインポート\nimport { vi } from 'vitest'\n\n// モック関数の作成\nconst mockFn = vi.fn()\n\n// モック関数の戻り値を設定\nmockFn.mockReturnValue('mocked value')\n\n// モジュール全体のモック\nvi.mock('./someModule', () =\u003e ({\n  // someModule内のsomeFunctionをモック化\n  someFunction: vi.fn().mockReturnValue('mocked value')\n}))\n```\n\n## セットアップと解体\n\nテストの前後で共通の処理を行いたい場合は、以下の関数を使用します：\n\n- `beforeEach`: 各テストの前に実行\n- `afterEach`: 各テストの後に実行\n- `beforeAll`: すべてのテストの前に1回実行\n- `afterAll`: すべてのテストの後に1回実行\n\n## スナップショットテスト\n\nコンポーネントの出力が期待通りか確認するためのスナップショットテストも簡単に実装できます：\n\n```typescript\nit('should match snapshot', () =\u003e {\n  const user = { name: 'John', age: 30 }\n  expect(user).toMatchSnapshot()\n})\n```\n\n## カバレッジ\n\nコードカバレッジを取得するには、`package.json`に以下のスクリプトを追加します：\n\n```json\n{\n  \"scripts\": {\n    \"test:coverage\": \"vitest run --coverage\"\n  }\n}\n```\n\n# まとめ\n\nVitestは、モダンなJavaScript開発に適した高速で使いやすいテストフレームワークです。Jestと似たAPIを持ちながら、Viteのエコシステムを活用することで、より効率的なテスト環境を提供します。基本的なテストの書き方から、非同期テスト、モック、カバレッジまで、幅広い機能をサポートしているため、様々なプロジェクトで活用することができます。\n\nVitestを導入することで、開発者はより迅速かつ効果的にテストを書くことができ、結果としてプロジェクトの品質向上につながります。ぜひ、あなたのプロジェクトでVitestを試してみてください。\n\n# 参考\n* Vitest 公式ドキュメント\n    * https://vitest.dev/\n* Vite 公式サイト\n    * https://vitejs.dev/\n* Jest から Vitest への移行ガイド\n    * https://vitest.dev/guide/migration.html\n\n",
    "coediting": false,
    "comments_count": 0,
    "created_at": "2024-08-22T12:44:21+09:00",
    "group": null,
    "id": "46549a52be2653ef6549",
    "likes_count": 0,
    "private": false,
    "reactions_count": 0,
    "stocks_count": 0,
    "tags": [
      {
        "name": "JavaScript",
        "versions": []
      },
      {
        "name": "Vitest",
        "versions": []
      }
    ],
    "title": "Vitestを使ったモダンなJavaScriptテスト入門",
    "updated_at": "2024-08-22T12:44:21+09:00",
    "url": "https://qiita.com/syukan3/items/46549a52be2653ef6549",
    "user": {
      "description": "",
      "facebook_id": "",
      "followees_count": 2,
      "followers_count": 7,
      "github_login_name": null,
      "id": "syukan3",
      "items_count": 95,
      "linkedin_id": "",
      "location": "",
      "name": "",
      "organization": "",
      "permanent_id": 308256,
      "profile_image_url": "https://pbs.twimg.com/profile_images/825257989598638080/SSWkxGct_bigger.jpg",
      "team_only": false,
      "twitter_screen_name": "syukan3",
      "website_url": ""
    },
    "page_views_count": null,
    "team_membership": null,
    "organization_url_name": null,
    "slide": false
  },
  {
    "rendered_body": "\u003cp data-sourcepos=\"2:1-2:42\"\u003e　どうも、カーブミラーです。\u003c/p\u003e\n\u003cp data-sourcepos=\"4:1-4:63\"\u003e　今回は、チェスボードパターンを描きます。\u003c/p\u003e\n\u003cdiv data-sourcepos=\"6:1-10:3\" class=\"note info\"\u003e\n\u003cspan class=\"fa fa-fw fa-check-circle\"\u003e\u003c/span\u003e\u003cdiv\u003e\n\u003cp data-sourcepos=\"7:1-9:36\"\u003e本記事は、ChatGPT（有料版）\u003cbr\u003e\nで行なっております。画像は\u003cbr\u003e\nあります。　あしからず。\u003c/p\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003cp data-sourcepos=\"12:1-12:30\"\u003e　まずは、ニュース。\u003c/p\u003e\n\u003cp data-sourcepos=\"14:1-14:54\"\u003e　というほどのことではないのですが。\u003c/p\u003e\n\u003cp data-sourcepos=\"16:1-19:36\"\u003e　MicrosoftのCopilot、\u003cbr\u003e\n　　いつのまにか日本語での\u003cbr\u003e\n　　　描画ができるように\u003cbr\u003e\n　　　　なっていました。\u003c/p\u003e\n\u003cp data-sourcepos=\"21:1-24:51\"\u003e　これまで\u003cbr\u003e\n　　日本語での指示は\u003cbr\u003e\n　　　“対応していません”的な\u003cbr\u003e\n　　　　メッセージが出てきました。\u003c/p\u003e\n\u003cp data-sourcepos=\"26:1-27:45\"\u003e　英語での指示でしか\u003cbr\u003e\n　　描画してくれませんでした。\u003c/p\u003e\n\u003cp data-sourcepos=\"29:1-30:51\"\u003e　それが日本語で\u003cbr\u003e\n　　指示して描画してくれるのです。\u003c/p\u003e\n\u003chr data-sourcepos=\"31:1-31:3\"\u003e\n\u003cp data-sourcepos=\"32:1-32:24\"\u003e　次のニュース。\u003c/p\u003e\n\u003cp data-sourcepos=\"34:1-36:43\"\u003e　AnthropicのClaudeですが、\u003cbr\u003e\n　　アプリ版（無料版）は、\u003cbr\u003e\n　　　これまで3.5 Sonnetでした。\u003c/p\u003e\n\u003cp data-sourcepos=\"38:1-40:143\"\u003e　それが現在では\u003cbr\u003e\n　　3 Haikuになっていました。\u003cbr\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/11f7382a30d33baa141290e1e82b66791652ec62/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f333435373233382f38393536363832392d336334392d336134342d343433342d6534353663363262333339342e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3457238%2F89566829-3c49-3a44-4434-e456c62b3394.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=f58e982dd2fc4fa4497be351204642b2\" alt=\"Screenshot_20240822-082349.png\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3457238/89566829-3c49-3a44-4434-e456c62b3394.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3457238%2F89566829-3c49-3a44-4434-e456c62b3394.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=33adb0cb382626e3f56bde2c5ed1fd60 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"42:1-43:49\"\u003e　Web版は\u003cbr\u003e\n　　変わらず、3.5 Sonnetでした。　　\u003c/p\u003e\n\u003cp data-sourcepos=\"45:1-47:36\"\u003e　Anthropicからは\u003cbr\u003e\n　　何もアナウンスは\u003cbr\u003e\n　　　ありませんでした。\u003c/p\u003e\n\u003cp data-sourcepos=\"49:1-51:45\"\u003e　Claudeを\u003cbr\u003e\n　　お使いの方は\u003cbr\u003e\n　　　お気を付けてくださいね。\u003c/p\u003e\n\u003cp data-sourcepos=\"53:1-57:42\"\u003e　それから\u003cbr\u003e\n　　有料版のClaudeも\u003cbr\u003e\n　　　“返答がおかしい”という報告が\u003cbr\u003e\n　　　　いくつも上がっています。\u003cbr\u003e\n　　　　　こちらもご注意を。\u003c/p\u003e\n\u003chr data-sourcepos=\"58:1-58:3\"\u003e\n\u003cp data-sourcepos=\"59:1-59:30\"\u003e　もひとつニュース。\u003c/p\u003e\n\u003cp data-sourcepos=\"61:1-69:71\"\u003e　画像生成AIの\u003cbr\u003e\n　　Midjourneyですが、\u003cbr\u003e\n　　　DiscordかGoogleの\u003cbr\u003e\n　　　　アカウントで\u003cbr\u003e\n　　　　　サインアップすると、\u003cbr\u003e\n　　　　　　25枚まで画像を\u003cbr\u003e\n　　　　　　　生成できるように\u003cbr\u003e\n　　　　　　　　なりました。\u003cbr\u003e\n\u003ca href=\"https://www.itmedia.co.jp/news/spv/2408/22/news090.html\" rel=\"nofollow noopener\" target=\"_blank\"\u003eITmedia NEWS\u003c/a\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"71:1-74:110\"\u003e　ちなみに\u003cbr\u003e\n　　これは無料トライアルみたい。\u003cbr\u003e\n　　　やるなら今！\u003cbr\u003e\n\u003ca href=\"https://x.com/midjourney/status/1826305298560418171?t=MWU9Wu_WmXVc8B-_bAl3jA\u0026amp;s=19\" rel=\"nofollow noopener\" target=\"_blank\"\u003eMidjourney公式ポスト\u003c/a\u003e\u003c/p\u003e\n\u003chr data-sourcepos=\"75:1-75:3\"\u003e\n\u003cp data-sourcepos=\"76:1-79:30\"\u003e　それからAIとは関係ありませんが、\u003cbr\u003e\n　　オリエント工業が\u003cbr\u003e\n　　　廃業することに\u003cbr\u003e\n　　　　なりました。\u003c/p\u003e\n\u003cp data-sourcepos=\"81:1-83:24\"\u003e　ラブドールを\u003cbr\u003e\n　　開発・販売してきた\u003cbr\u003e\n　　　企業です。\u003c/p\u003e\n\u003cp data-sourcepos=\"85:1-87:36\"\u003e　エロ雑誌に\u003cbr\u003e\n　　掲載があったから\u003cbr\u003e\n　　　見た人もいるかと。\u003c/p\u003e\n\u003cp data-sourcepos=\"89:1-92:45\"\u003e　単なる性処理用の\u003cbr\u003e\n　　ドールというよりも\u003cbr\u003e\n　　　使用者に寄り添うドールを\u003cbr\u003e\n　　　　目指していたそうです。\u003c/p\u003e\n\u003cp data-sourcepos=\"94:1-99:48\"\u003e　廃業理由は\u003cbr\u003e\n　　社長さんが健康を優先するため\u003cbr\u003e\n　　　だそうです。\u003cbr\u003e\n　　　　半世紀近くやっていたので\u003cbr\u003e\n　　　　　身体に無理が効かなくなった\u003cbr\u003e\n　　　　　　のかもしれませんね。\u003c/p\u003e\n\u003chr data-sourcepos=\"100:1-100:3\"\u003e\n\u003cp data-sourcepos=\"101:1-101:27\"\u003e　さて、本題です。\u003c/p\u003e\n\u003cp data-sourcepos=\"103:1-105:27\"\u003e　今回は、\u003cbr\u003e\n　　チェスボードパターンを\u003cbr\u003e\n　　　作成します。\u003c/p\u003e\n\u003cp data-sourcepos=\"107:1-110:42\"\u003e　なんだそりゃ、と\u003cbr\u003e\n　　言われそうですが、\u003cbr\u003e\n　　　Code Interpreterで\u003cbr\u003e\n　　　　描くことが目的です。\u003c/p\u003e\n\u003cp data-sourcepos=\"112:1-113:42\"\u003e　しかも今回は、\u003cbr\u003e\n　　OpenCVを使ってのものです。\u003c/p\u003e\n\u003cp data-sourcepos=\"115:1-116:127\"\u003e　出来上がりがこちら。\u003cbr\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/49579f39dd9753750e3a6a9187188ad2bdf585b5/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f333435373233382f62653636363230392d633936662d653366322d636330332d6263373938336461656666332e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3457238%2Fbe666209-c96f-e3f2-cc03-bc7983daeff3.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=f17864648e2ff3b53a12aacf0fe16873\" alt=\"chessboard.png\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3457238/be666209-c96f-e3f2-cc03-bc7983daeff3.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3457238%2Fbe666209-c96f-e3f2-cc03-bc7983daeff3.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=ebf4323dc0f2f3377291a96c8b8b8f37 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"118:1-118:30\"\u003e　Promptはこちらです。\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"Text\" data-sourcepos=\"119:1-121:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003eOpenCVを使って、チェスボードパターンを作成してください。\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"123:1-127:39\"\u003e　まだまだOpenCVを\u003cbr\u003e\n　　どう使っていいのか\u003cbr\u003e\n　　　わかりません。\u003cbr\u003e\n　　　　何かわかれば\u003cbr\u003e\n　　　　　また書きますね。\u003c/p\u003e\n\u003cp data-sourcepos=\"129:1-131:36\"\u003e　みなさんの\u003cbr\u003e\n　　作品制作の糧に\u003cbr\u003e\n　　　なれば、幸いです。\u003c/p\u003e\n\u003cp data-sourcepos=\"133:1-133:30\"\u003e　今回は、ここまで。\u003c/p\u003e\n",
    "body": "\n　どうも、カーブミラーです。\n\n　今回は、チェスボードパターンを描きます。\n\n:::note info\n本記事は、ChatGPT（有料版）\nで行なっております。画像は\nあります。　あしからず。\n:::\n\n　まずは、ニュース。\n\n　というほどのことではないのですが。\n\n　MicrosoftのCopilot、\n　　いつのまにか日本語での\n　　　描画ができるように\n　　　　なっていました。\n\n　これまで\n　　日本語での指示は\n　　　“対応していません”的な\n　　　　メッセージが出てきました。\n\n　英語での指示でしか\n　　描画してくれませんでした。\n\n　それが日本語で\n　　指示して描画してくれるのです。\n***\n　次のニュース。\n\n　AnthropicのClaudeですが、\n　　アプリ版（無料版）は、\n　　　これまで3.5 Sonnetでした。\n\n　それが現在では\n　　3 Haikuになっていました。\n![Screenshot_20240822-082349.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3457238/89566829-3c49-3a44-4434-e456c62b3394.png)\n\n　Web版は\n　　変わらず、3.5 Sonnetでした。　　\n\n　Anthropicからは\n　　何もアナウンスは\n　　　ありませんでした。\n\n　Claudeを\n　　お使いの方は\n　　　お気を付けてくださいね。\n\n　それから\n　　有料版のClaudeも\n　　　“返答がおかしい”という報告が\n　　　　いくつも上がっています。\n　　　　　こちらもご注意を。\n***\n　もひとつニュース。\n\n　画像生成AIの\n　　Midjourneyですが、\n　　　DiscordかGoogleの\n　　　　アカウントで\n　　　　　サインアップすると、\n　　　　　　25枚まで画像を\n　　　　　　　生成できるように\n　　　　　　　　なりました。\n[ITmedia NEWS](https://www.itmedia.co.jp/news/spv/2408/22/news090.html)\n\n　ちなみに\n　　これは無料トライアルみたい。\n　　　やるなら今！\n[Midjourney公式ポスト](https://x.com/midjourney/status/1826305298560418171?t=MWU9Wu_WmXVc8B-_bAl3jA\u0026s=19)\n***\n　それからAIとは関係ありませんが、\n　　オリエント工業が\n　　　廃業することに\n　　　　なりました。\n\n　ラブドールを\n　　開発・販売してきた\n　　　企業です。\n\n　エロ雑誌に\n　　掲載があったから\n　　　見た人もいるかと。\n\n　単なる性処理用の\n　　ドールというよりも\n　　　使用者に寄り添うドールを\n　　　　目指していたそうです。\n\n　廃業理由は\n　　社長さんが健康を優先するため\n　　　だそうです。\n　　　　半世紀近くやっていたので\n　　　　　身体に無理が効かなくなった\n　　　　　　のかもしれませんね。\n***\n　さて、本題です。\n\n　今回は、\n　　チェスボードパターンを\n　　　作成します。\n\n　なんだそりゃ、と\n　　言われそうですが、\n　　　Code Interpreterで\n　　　　描くことが目的です。\n\n　しかも今回は、\n　　OpenCVを使ってのものです。\n\n　出来上がりがこちら。\n![chessboard.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3457238/be666209-c96f-e3f2-cc03-bc7983daeff3.png)\n\n　Promptはこちらです。\n```Text:\nOpenCVを使って、チェスボードパターンを作成してください。\n```\n\n　まだまだOpenCVを\n　　どう使っていいのか\n　　　わかりません。\n　　　　何かわかれば\n　　　　　また書きますね。\n\n　みなさんの\n　　作品制作の糧に\n　　　なれば、幸いです。\n\n　今回は、ここまで。\n",
    "coediting": false,
    "comments_count": 0,
    "created_at": "2024-08-22T12:25:24+09:00",
    "group": null,
    "id": "904f3f38932bd588230b",
    "likes_count": 0,
    "private": false,
    "reactions_count": 0,
    "stocks_count": 0,
    "tags": [
      {
        "name": "ChatGPT",
        "versions": []
      },
      {
        "name": "CodeInterpreter",
        "versions": []
      }
    ],
    "title": "【ChatGPT＋Code Interpreter】でチェスボードパターンを描きます。",
    "updated_at": "2024-08-22T12:25:24+09:00",
    "url": "https://qiita.com/kabumira/items/904f3f38932bd588230b",
    "user": {
      "description": "カーブミラーです。東京在住。\r\n技術者ではありませんが、日曜プログラマぽいことをやったりしてます。\r\n最近は、ChatGPT（無料版を2023/02/11から、Plusは2023/11/03から）で、プロンプトの反応を楽しんでいます。\r\n名前の由来は、ひねくれ者なので、視点が人と違うことが多々あるようで、そこから来ています。やはりスマホでの記事書きは大変。PC欲しい。よろしく。",
      "facebook_id": "",
      "followees_count": 5,
      "followers_count": 19,
      "github_login_name": null,
      "id": "kabumira",
      "items_count": 456,
      "linkedin_id": "",
      "location": "東京都",
      "name": "カーブ ミラー",
      "organization": "",
      "permanent_id": 3457238,
      "profile_image_url": "https://lh3.googleusercontent.com/a/AAcHTtfJULBKsCQ3Q8nECE3t-nl7GIa3M3oY2veBa4Ujhg=s96-c",
      "team_only": false,
      "twitter_screen_name": null,
      "website_url": ""
    },
    "page_views_count": null,
    "team_membership": null,
    "organization_url_name": null,
    "slide": false
  },
  {
    "rendered_body": "\u003ch1 data-sourcepos=\"1:1-1:35\"\u003e\n\u003cspan id=\"ページが開けなくなった\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%83%9A%E3%83%BC%E3%82%B8%E3%81%8C%E9%96%8B%E3%81%91%E3%81%AA%E3%81%8F%E3%81%AA%E3%81%A3%E3%81%9F\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eページが開けなくなった\u003c/h1\u003e\n\u003cp data-sourcepos=\"2:1-4:99\"\u003e\u003ca href=\"https://qiita.com/AsilHatake/items/bec3386dade9522a3685\" id=\"reference-996bdae2e0ca6bd3606e\"\u003e前の記事\u003c/a\u003eで、調子に乗って指定された最大条件で動かした際の結果を貼り付けて投稿したところ、\u003cbr\u003e\n投稿自体は何ら問題なく出来たけど、\u003cbr\u003e\n対象ページを開くとブラウザがフリーズしてしまう事象が出てしまった。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"6:1-6:48\"\u003e\n\u003cspan id=\"サイズ的には問題ないけど\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%82%B5%E3%82%A4%E3%82%BA%E7%9A%84%E3%81%AB%E3%81%AF%E5%95%8F%E9%A1%8C%E3%81%AA%E3%81%84%E3%81%91%E3%81%A9\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eサイズ的には問題ないけど。。。\u003c/h2\u003e\n\u003cp data-sourcepos=\"7:1-8:133\"\u003e試しにその問題になった結果をファイルに保存して見たけど1MBぐらいしかなくて、\u003cbr\u003e\nQiitaの記事の最大サイズの100MB制限には微塵も引っかからなかったため投稿自体は出来てしまった。\u003c/p\u003e\n\u003ch1 data-sourcepos=\"10:1-10:38\"\u003e\n\u003cspan id=\"編集が出来なくてツムツム\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E7%B7%A8%E9%9B%86%E3%81%8C%E5%87%BA%E6%9D%A5%E3%81%AA%E3%81%8F%E3%81%A6%E3%83%84%E3%83%A0%E3%83%84%E3%83%A0\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e編集が出来なくてツムツム\u003c/h1\u003e\n\u003cp data-sourcepos=\"11:1-11:285\"\u003eQiitaでの記事編集は、マイページの記事一覧からは直接できなくて、1回対象のページに行かないとメニューが出てこないという残念仕様なので、そもそも重くて記事に行けないという時には編集が出来ない。。。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"13:1-13:63\"\u003e\n\u003cspan id=\"とにかく編集したいんだ炎上回避のため\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%81%A8%E3%81%AB%E3%81%8B%E3%81%8F%E7%B7%A8%E9%9B%86%E3%81%97%E3%81%9F%E3%81%84%E3%82%93%E3%81%A0%E7%82%8E%E4%B8%8A%E5%9B%9E%E9%81%BF%E3%81%AE%E3%81%9F%E3%82%81\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eとにかく編集したいんだ！（炎上回避のため\u003c/h2\u003e\n\u003cp data-sourcepos=\"14:1-17:72\"\u003eこんな拙い記事を誰が見るんだって話ではありますが、\u003cbr\u003e\nもしたまたま迷い込んで開いてしまったときにフリーズしたとかだと、\u003cbr\u003e\n下手したら炎上してしまうのではないかということで、\u003cbr\u003e\nとりあえず記事が開くまで気長に待ってみることに。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"19:1-19:51\"\u003e\n\u003cspan id=\"ガンバレよもっと熱くなれよcpu\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%82%AC%E3%83%B3%E3%83%90%E3%83%AC%E3%82%88%E3%82%82%E3%81%A3%E3%81%A8%E7%86%B1%E3%81%8F%E3%81%AA%E3%82%8C%E3%82%88cpu\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eガンバレよ、もっと熱くなれよ（CPU\u003c/h2\u003e\n\u003cp data-sourcepos=\"20:1-20:236\"\u003eただ3分、5分、10分...と待てど暮らせど、一向にブラウザは表示してくれないし、クリックでもしようもんなら「待機 or ページ離脱」を迫ってくるアラートを出して来る始末。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"22:1-22:45\"\u003e\n\u003cspan id=\"痺れを切らして別の方法を模索\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E7%97%BA%E3%82%8C%E3%82%92%E5%88%87%E3%82%89%E3%81%97%E3%81%A6%E5%88%A5%E3%81%AE%E6%96%B9%E6%B3%95%E3%82%92%E6%A8%A1%E7%B4%A2\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e痺れを切らして別の方法を模索\u003c/h2\u003e\n\u003cp data-sourcepos=\"23:1-24:63\"\u003e他のページではどうやって編集にいくんだろうということで、問題ないページを開いて、編集にいくメニューをホバーしたところURLが表示された。\u003cbr\u003e\n\u003ccode\u003ehttps://qiita.com/drafts/[ページのID的な文字列]/edit\u003c/code\u003e\u003c/p\u003e\n\u003ch2 data-sourcepos=\"26:1-26:39\"\u003e\n\u003cspan id=\"あれコレ行けんじゃね\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%81%82%E3%82%8C%E3%82%B3%E3%83%AC%E8%A1%8C%E3%81%91%E3%82%93%E3%81%98%E3%82%83%E3%81%AD\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eあれ？コレ行けんじゃね？\u003c/h2\u003e\n\u003cp data-sourcepos=\"27:1-27:352\"\u003eてっきりOnClickでJavaScriptでURL生成していってるんじゃないかと思ってたけど、URL直アクセスで行けそうだったため、マイページに戻り記事の一覧から問題のページのURLをコピり、そのURLから画面ID的な文字列を上のURLに貼っつけてブラウザに入力したところ。。。\u003c/p\u003e\n\u003ch1 data-sourcepos=\"29:1-29:44\"\u003e\n\u003cspan id=\"やっと編集できましたわ\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%82%84%E3%81%A3%E3%81%A8%E7%B7%A8%E9%9B%86%E3%81%A7%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F%E3%82%8F\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eやっと編集できましたわ。。。\u003c/h1\u003e\n\u003cp data-sourcepos=\"30:1-30:117\"\u003e無事に編集画面にたどり着き、急いで色々となかったことにして再掲載した次第です。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"32:1-32:9\"\u003e\n\u003cspan id=\"教訓\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E6%95%99%E8%A8%93\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e教訓\u003c/h2\u003e\n\u003cp data-sourcepos=\"33:1-34:228\"\u003e興味本位で色々やらない（違う、そうじゃない\u003cbr\u003e\nまぁでも、【流石にこの量を投稿しちゃまずいだろうな】ということを念頭に置いて記事を書くべきだなということは身を持って理解できたので、結果オーライかなと。\u003c/p\u003e\n\u003ch1 data-sourcepos=\"36:1-36:23\"\u003e\n\u003cspan id=\"なぁぜなぁぜ\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%81%AA%E3%81%81%E3%81%9C%E3%81%AA%E3%81%81%E3%81%9C\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eなぁぜなぁぜ？\u003c/h1\u003e\n\u003cp data-sourcepos=\"37:1-38:157\"\u003e全体で1MBじゃなくて1行が1MBとかとんでもない場合、ブラウザが対処しきれなくてダメなんじゃね？という予測。\u003cbr\u003e\nつまりコレって【1行に〇〇文字入力した記事を投稿するとフリーズする】っていうトリビアになりませんか（へぇ～\u003c/p\u003e\n\u003cp data-sourcepos=\"40:1-41:182\"\u003eQiitaさん、ぜひ検証してみて入力制限をかけるようにしてください！（他人任せ\u003cbr\u003e\nそもそも100MB制限かけてるのに、ソレを下回ってても表示が出来ないなんて不具合じゃないんですか？という疑問を残しつつ締めます。\u003c/p\u003e\n",
    "body": "# ページが開けなくなった\n[前の記事](https://qiita.com/AsilHatake/items/bec3386dade9522a3685)で、調子に乗って指定された最大条件で動かした際の結果を貼り付けて投稿したところ、\n投稿自体は何ら問題なく出来たけど、\n対象ページを開くとブラウザがフリーズしてしまう事象が出てしまった。\n\n## サイズ的には問題ないけど。。。\n試しにその問題になった結果をファイルに保存して見たけど1MBぐらいしかなくて、\nQiitaの記事の最大サイズの100MB制限には微塵も引っかからなかったため投稿自体は出来てしまった。\n\n# 編集が出来なくてツムツム\nQiitaでの記事編集は、マイページの記事一覧からは直接できなくて、1回対象のページに行かないとメニューが出てこないという残念仕様なので、そもそも重くて記事に行けないという時には編集が出来ない。。。\n\n## とにかく編集したいんだ！（炎上回避のため\nこんな拙い記事を誰が見るんだって話ではありますが、\nもしたまたま迷い込んで開いてしまったときにフリーズしたとかだと、\n下手したら炎上してしまうのではないかということで、\nとりあえず記事が開くまで気長に待ってみることに。\n\n## ガンバレよ、もっと熱くなれよ（CPU\nただ3分、5分、10分...と待てど暮らせど、一向にブラウザは表示してくれないし、クリックでもしようもんなら「待機 or ページ離脱」を迫ってくるアラートを出して来る始末。\n\n## 痺れを切らして別の方法を模索\n他のページではどうやって編集にいくんだろうということで、問題ないページを開いて、編集にいくメニューをホバーしたところURLが表示された。\n`https://qiita.com/drafts/[ページのID的な文字列]/edit`\n\n## あれ？コレ行けんじゃね？\nてっきりOnClickでJavaScriptでURL生成していってるんじゃないかと思ってたけど、URL直アクセスで行けそうだったため、マイページに戻り記事の一覧から問題のページのURLをコピり、そのURLから画面ID的な文字列を上のURLに貼っつけてブラウザに入力したところ。。。\n\n# やっと編集できましたわ。。。\n無事に編集画面にたどり着き、急いで色々となかったことにして再掲載した次第です。\n\n## 教訓\n興味本位で色々やらない（違う、そうじゃない\nまぁでも、【流石にこの量を投稿しちゃまずいだろうな】ということを念頭に置いて記事を書くべきだなということは身を持って理解できたので、結果オーライかなと。\n\n# なぁぜなぁぜ？\n全体で1MBじゃなくて1行が1MBとかとんでもない場合、ブラウザが対処しきれなくてダメなんじゃね？という予測。\nつまりコレって【1行に〇〇文字入力した記事を投稿するとフリーズする】っていうトリビアになりませんか（へぇ～\n\nQiitaさん、ぜひ検証してみて入力制限をかけるようにしてください！（他人任せ\nそもそも100MB制限かけてるのに、ソレを下回ってても表示が出来ないなんて不具合じゃないんですか？という疑問を残しつつ締めます。\n",
    "coediting": false,
    "comments_count": 0,
    "created_at": "2024-08-22T12:19:44+09:00",
    "group": null,
    "id": "b5071ac9ce4511cb7a98",
    "likes_count": 0,
    "private": false,
    "reactions_count": 0,
    "stocks_count": 0,
    "tags": [
      {
        "name": "Qiita",
        "versions": []
      },
      {
        "name": "記事作成",
        "versions": []
      },
      {
        "name": "やらかし",
        "versions": []
      }
    ],
    "title": "記事掲載でやらかした話（炎上はまだしてないはず。。。",
    "updated_at": "2024-08-22T12:19:44+09:00",
    "url": "https://qiita.com/AsilHatake/items/b5071ac9ce4511cb7a98",
    "user": {
      "description": "マネジメント？思い通りに動かない人より、命令通りに動くプログラムを動かしてたほうが楽しいじゃん。\r\n",
      "facebook_id": "",
      "followees_count": 2,
      "followers_count": 6,
      "github_login_name": null,
      "id": "AsilHatake",
      "items_count": 81,
      "linkedin_id": "",
      "location": "",
      "name": "Hatake え～すけ",
      "organization": "株式会社エイシル",
      "permanent_id": 996378,
      "profile_image_url": "https://s3-ap-northeast-1.amazonaws.com/qiita-image-store/0/996378/072660910a5c2a18a4661f8e7e13cec4c9c42c01/x_large.png?1610034116",
      "team_only": false,
      "twitter_screen_name": null,
      "website_url": ""
    },
    "page_views_count": null,
    "team_membership": null,
    "organization_url_name": "asil-inc",
    "slide": false
  },
  {
    "rendered_body": "\u003ch2 data-sourcepos=\"1:1-1:24\"\u003e\n\u003cspan id=\"この記事の内容\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%81%93%E3%81%AE%E8%A8%98%E4%BA%8B%E3%81%AE%E5%86%85%E5%AE%B9\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eこの記事の内容\u003c/h2\u003e\n\u003cp data-sourcepos=\"3:1-3:131\"\u003e\u003ca href=\"https://camo.qiitausercontent.com/cff87b9dfdd05913964b5a18ea53542409c1cd7c/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3633353333302f34616535343062392d373533372d373061632d626432362d6336326563623664326233392e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F4ae540b9-7537-70ac-bd26-c62ecb6d2b39.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=98a33a5b7f548e3ff9cd3e1358770cda\" alt=\"qiita-thumbnail.png\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/4ae540b9-7537-70ac-bd26-c62ecb6d2b39.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F4ae540b9-7537-70ac-bd26-c62ecb6d2b39.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=44a35fe729ad6deeb87ec04f572c229f 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul data-sourcepos=\"5:1-15:0\"\u003e\n\u003cli data-sourcepos=\"5:1-5:46\"\u003eアプリにVisionOSターゲットを追加\u003c/li\u003e\n\u003cli data-sourcepos=\"6:1-6:56\"\u003eビューの背景の透明度を切り替える方法\u003c/li\u003e\n\u003cli data-sourcepos=\"7:1-7:70\"\u003eVisionOSアプリ用のコードを条件付きで実行する方法\u003c/li\u003e\n\u003cli data-sourcepos=\"8:1-8:60\"\u003e3Dアセット（.usdzファイル）を読み込む方法\u003c/li\u003e\n\u003cli data-sourcepos=\"9:1-9:84\"\u003e没入空間を開き、2Dビューとともに3Dアセットを表示する方法\u003c/li\u003e\n\u003cli data-sourcepos=\"10:1-10:142\"\u003e新しいウィンドウを開いて3Dアセットを表示し、ユーザーがモデルを回転・ズームできるようにする方法\u003c/li\u003e\n\u003cli data-sourcepos=\"11:1-11:61\"\u003e3Dアセットの位置やスケールを変更する方法\u003c/li\u003e\n\u003cli data-sourcepos=\"12:1-12:61\"\u003e3Dアセットにアニメーションを適用する方法\u003c/li\u003e\n\u003cli data-sourcepos=\"13:1-13:32\"\u003e没入空間を閉じる方法\u003c/li\u003e\n\u003cli data-sourcepos=\"14:1-15:0\"\u003eオブジェクトを選択可能にする方法\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"16:1-16:156\"\u003eこの記事を読めば、部屋の中でランダムな場所に現れるロボットをタップして隠すミニゲームの作り方を学べます。\u003c/p\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/e37e1249faa107a3dbff8d4c0bec1fe083768a00/68747470733a2f2f61727469636c652d696d616765732e6d737a70726f2e636f6d2f7261636b2d612d626f742d67616d652d636f6d707265737365642e676966\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Farticle-images.mszpro.com%2Frack-a-bot-game-compressed.gif?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=5f4b2232bda5293eb0a75e421b630285\" data-canonical-src=\"https://article-images.mszpro.com/rack-a-bot-game-compressed.gif\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Farticle-images.mszpro.com%2Frack-a-bot-game-compressed.gif?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=21af691f2e76d2a603068965a404e6fb 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\n\u003cp data-sourcepos=\"21:1-22:117\"\u003eGithubリポジトリ: \u003ca href=\"https://github.com/mszpro/VisionOS101\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://github.com/mszpro/VisionOS101\u003c/a\u003e\u003cbr\u003e\nこの記事は \u003cstrong\u003e英語で\u003c/strong\u003e 録画されたYouTube動画もあります:\u003ca href=\"https://www.youtube.com/watch?v=6gDu80Jbnwo\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://www.youtube.com/watch?v=6gDu80Jbnwo\u003c/a\u003e\u003c/p\u003e\n\u003ch2 data-sourcepos=\"24:1-24:38\"\u003e\n\u003cspan id=\"私のvisionosアプリとゲーム\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E7%A7%81%E3%81%AEvisionos%E3%82%A2%E3%83%97%E3%83%AA%E3%81%A8%E3%82%B2%E3%83%BC%E3%83%A0\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e私のVisionOSアプリとゲーム\u003c/h2\u003e\n\u003cp data-sourcepos=\"26:1-26:93\"\u003eこれまでに3つのVisionOSアプリとゲームを個人で開発・公開しました。\u003c/p\u003e\n\u003ctable data-sourcepos=\"28:1-31:373\"\u003e\n\u003cthead\u003e\n\u003ctr data-sourcepos=\"28:1-28:43\"\u003e\n\u003cth data-sourcepos=\"28:2-28:16\"\u003eSpatial Dream\u003c/th\u003e\n\u003cth data-sourcepos=\"28:18-28:32\"\u003eSpatial Boxer\u003c/th\u003e\n\u003cth data-sourcepos=\"28:34-28:42\"\u003eSoraSNS\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr data-sourcepos=\"30:1-30:226\"\u003e\n\u003ctd data-sourcepos=\"30:2-30:60\"\u003e部屋の中で楽しめる小さなパズルゲーム。\u003c/td\u003e\n\u003ctd data-sourcepos=\"30:62-30:153\"\u003e音楽に合わせてボックスをパンチし、爆弾を避けるリズムゲーム。\u003c/td\u003e\n\u003ctd data-sourcepos=\"30:155-30:225\"\u003eMastodon、Bluesky、Misskey、Firefish向けのFediverseアプリ。\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr data-sourcepos=\"31:1-31:373\"\u003e\n\u003ctd data-sourcepos=\"31:2-31:124\"\u003e\u003ca href=\"https://camo.qiitausercontent.com/d5b6139901b01f14c4cb3a901f0bbf359531a895/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3633353333302f64353438353539612d636163632d373462652d386166382d3930306466666661666431372e6a706567\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2Fd548559a-cacc-74be-8af8-900dfffafd17.jpeg?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=d1e4555c4a80f33c8a74dac7d1d729f3\" alt=\"image.jpeg\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/d548559a-cacc-74be-8af8-900dfffafd17.jpeg\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2Fd548559a-cacc-74be-8af8-900dfffafd17.jpeg?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=fed900ab0bfcd244a00f426585a3433d 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd data-sourcepos=\"31:126-31:248\"\u003e\u003ca href=\"https://camo.qiitausercontent.com/5675e2593ace6b8bd615fdf0be9776b6dc526691/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3633353333302f36653061616565392d653165642d323530382d313932332d3734633834393938313539302e6a706567\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F6e0aaee9-e1ed-2508-1923-74c849981590.jpeg?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=3c12e3fd4a5e8bfdc00777263b0624cd\" alt=\"image.jpeg\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/6e0aaee9-e1ed-2508-1923-74c849981590.jpeg\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F6e0aaee9-e1ed-2508-1923-74c849981590.jpeg?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=b29d78d2cc9398623cafc76e09ad8c48 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd data-sourcepos=\"31:250-31:372\"\u003e\u003ca href=\"https://camo.qiitausercontent.com/49a126c3e89d1d2f3f7f764126221a1c1d01f84f/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3633353333302f31623061386436632d323438302d376336332d393437662d6631303833626535386566652e6a706567\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F1b0a8d6c-2480-7c63-947f-f1083be58efe.jpeg?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=7c84de644391fe53c7cdd843ea413ccf\" alt=\"image.jpeg\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/1b0a8d6c-2480-7c63-947f-f1083be58efe.jpeg\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F1b0a8d6c-2480-7c63-947f-f1083be58efe.jpeg?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=a0622c0f79aae7317f9e1b44665a0df3 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp data-sourcepos=\"33:1-33:30\"\u003eさあ、始めましょう！\u003c/p\u003e\n\u003ch2 data-sourcepos=\"35:1-35:15\"\u003e\n\u003cspan id=\"スタート\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%82%B9%E3%82%BF%E3%83%BC%E3%83%88\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eスタート\u003c/h2\u003e\n\u003cp data-sourcepos=\"37:1-37:70\"\u003e通常のiOS向けSwiftUIアプリケーションから始めます：\u003c/p\u003e\n\u003cp data-sourcepos=\"39:1-39:121\"\u003e\u003ca href=\"https://camo.qiitausercontent.com/297c096400aa4b48fab5a362649daa960bfc08b8/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3633353333302f38303061396263382d343736322d313030642d383237362d6434666336386335333730302e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F800a9bc8-4762-100d-8276-d4fc68c53700.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=6a41da70a346d23bb393b1a8e5223137\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/800a9bc8-4762-100d-8276-d4fc68c53700.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F800a9bc8-4762-100d-8276-d4fc68c53700.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=b252de986c9401d9a943a3b3a0c5bd31 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"41:1-41:150\"\u003e上記のようなファイル構造と、\"Hello World\" というテキストを表示する基本的なSwiftUIビューが用意されています。\u003c/p\u003e\n\u003cp data-sourcepos=\"43:1-43:121\"\u003e\u003ca href=\"https://camo.qiitausercontent.com/21e526d30ec506ab8ab6da9997c9cb9342be5fb4/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3633353333302f37336564613061382d646235342d643435342d636437662d3232656530373438666236382e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F73eda0a8-db54-d454-cd7f-22ee0748fb68.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=d187b7128b1b20aba4a0f743298d12d6\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/73eda0a8-db54-d454-cd7f-22ee0748fb68.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F73eda0a8-db54-d454-cd7f-22ee0748fb68.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=09fe377e72a034fdf9120b8408afa0bb 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"45:1-45:224\"\u003e次に、メインターゲットのプロジェクト設定に切り替えます。アプリがすでにApple Visionに対応していることが表示されていますが、\"Designed for iPad\" と書かれています。\u003c/p\u003e\n\u003cp data-sourcepos=\"47:1-47:121\"\u003e\u003ca href=\"https://camo.qiitausercontent.com/b8ae50be2da82f98204d1d6d89580bda7e45b92d/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3633353333302f63363832356532652d313833612d333531362d643436342d3932383863626635646337642e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2Fc6825e2e-183a-3516-d464-9288cbf5dc7d.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=556da0ca74a57d6035a298c403aa631a\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/c6825e2e-183a-3516-d464-9288cbf5dc7d.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2Fc6825e2e-183a-3516-d464-9288cbf5dc7d.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=6689540809642ba801360ed69f9dbc2d 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"49:1-50:169\"\u003eこれは、このアプリがVisionOSでiPadと同じように動作することを意味しており、3D空間のアセットをサポートせず、背景が美しくない状態です。\u003cbr\u003e\n例えば、ターゲットをApple Vision Proに切り替えて実行ボタンを押すと、今アプリをシミュレーターで実行することができます。\u003c/p\u003e\n\u003cp data-sourcepos=\"52:1-52:121\"\u003e\u003ca href=\"https://camo.qiitausercontent.com/77a343a6b57d0ffbd96a249f402ccf0b5afee265/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3633353333302f35396663636436342d343164612d313436632d343531382d3532613032376430373061662e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F59fccd64-41da-146c-4518-52a027d070af.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=c30b5aea4e1df58e89a1e13148d656d8\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/59fccd64-41da-146c-4518-52a027d070af.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F59fccd64-41da-146c-4518-52a027d070af.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=c56eef64ed8b2c81a48a4b0bddbdbf52 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"54:1-54:126\"\u003eビューには白い非透明な背景があり、ウィンドウのサイズを変更することが全くできません。\u003c/p\u003e\n\u003cp data-sourcepos=\"56:1-56:121\"\u003e\u003ca href=\"https://camo.qiitausercontent.com/c37f8973602ee2266f37956e83875732cecbb71e/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3633353333302f64303230666237632d613465352d656434392d663364362d3730376565326162303039622e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2Fd020fb7c-a4e5-ed49-f3d6-707ee2ab009b.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=7b625b51b5d0ddd8d50bfc23091b6e56\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/d020fb7c-a4e5-ed49-f3d6-707ee2ab009b.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2Fd020fb7c-a4e5-ed49-f3d6-707ee2ab009b.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=ef86434fd336a33ce8a5fadc9ca6a360 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"58:1-58:141\"\u003eサポートされている実行先のリストで、Apple Vision Pro（Designed for iPad）を削除し、Apple Visionを追加します。\u003c/p\u003e\n\u003cp data-sourcepos=\"60:1-60:121\"\u003e\u003ca href=\"https://camo.qiitausercontent.com/488e9e7b4adb326cd46cf6dc331e805ffd2da798/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3633353333302f39396536373761392d383133622d663164302d323637372d6366663838356532356633642e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F99e677a9-813b-f1d0-2677-cff885e25f3d.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=40f8629c12f3711f2ec204894176e67f\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/99e677a9-813b-f1d0-2677-cff885e25f3d.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F99e677a9-813b-f1d0-2677-cff885e25f3d.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=a2afcebeb4f4faad5c6d71427c2254a5 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"62:1-62:151\"\u003eもう一度シミュレーターでアプリを実行すると、ネイティブのApple Vision Proアプリになったことが確認できます。\u003c/p\u003e\n\u003cp data-sourcepos=\"64:1-64:121\"\u003e\u003ca href=\"https://camo.qiitausercontent.com/4f4a1ba5a247d913555d598699afc061a63be4d3/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3633353333302f30363331363139372d663965362d336532652d333364652d6638313835363934326663392e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F06316197-f9e6-3e2e-33de-f81856942fc9.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=d00d688520f51bf5f1b1613ea2c4cca0\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/06316197-f9e6-3e2e-33de-f81856942fc9.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F06316197-f9e6-3e2e-33de-f81856942fc9.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=1376cea0833984fa7569990b7453d9c6 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cdiv data-sourcepos=\"66:1-68:3\" class=\"note info\"\u003e\n\u003cspan class=\"fa fa-fw fa-check-circle\"\u003e\u003c/span\u003e\u003cdiv\u003e\n\u003cp data-sourcepos=\"67:1-67:488\"\u003e💡 既存のiOSアプリに上記の変更を適用すると、コンパイル警告が発生する可能性があります。例えば、使用しているサードパーティのパッケージやライブラリがApple Vision OSに対応していない場合や、一部のコードがVision OSで利用できない場合があります。その場合、Vision OS用に別のコードブランチを実行するために条件付きチェックを使用する必要があります。\u003c/p\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003ch2 data-sourcepos=\"70:1-70:60\"\u003e\n\u003cspan id=\"背景のガラスモーフィズム効果を制御する\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E8%83%8C%E6%99%AF%E3%81%AE%E3%82%AC%E3%83%A9%E3%82%B9%E3%83%A2%E3%83%BC%E3%83%95%E3%82%A3%E3%82%BA%E3%83%A0%E5%8A%B9%E6%9E%9C%E3%82%92%E5%88%B6%E5%BE%A1%E3%81%99%E3%82%8B\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e背景のガラスモーフィズム効果を制御する\u003c/h2\u003e\n\u003cp data-sourcepos=\"72:1-72:207\"\u003eデフォルトでは、ガラスモーフィズム（ガラスのように、ウィンドウの後ろが見える効果）の背景が適用されています。これを制御することができます。\u003c/p\u003e\n\u003cp data-sourcepos=\"74:1-74:188\"\u003eまず、アプリケーションウィンドウが2Dコンテンツのみを表示する場合、メインのSwiftUIアプリでwindowStyleを.plainに設定することができます：\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"swift\" data-sourcepos=\"76:1-88:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"kd\"\u003eimport\u003c/span\u003e \u003cspan class=\"kt\"\u003eSwiftUI\u003c/span\u003e\n\n\u003cspan class=\"kd\"\u003e@main\u003c/span\u003e\n\u003cspan class=\"kd\"\u003estruct\u003c/span\u003e \u003cspan class=\"kt\"\u003eVisionOSDemoApp\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eApp\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003ebody\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kd\"\u003esome\u003c/span\u003e \u003cspan class=\"kt\"\u003eScene\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"kt\"\u003eWindowGroup\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n            \u003cspan class=\"kt\"\u003eContentView\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n        \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ewindowStyle\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eplain\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"90:1-90:95\"\u003e次に、ContentViewでは背景のガラス効果を表示しないように設定します：\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"swift\" data-sourcepos=\"92:1-116:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"kd\"\u003estruct\u003c/span\u003e \u003cspan class=\"kt\"\u003eContentView\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eView\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \n    \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003ebody\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kd\"\u003esome\u003c/span\u003e \u003cspan class=\"kt\"\u003eView\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \n        \u003cspan class=\"kt\"\u003eVStack\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n            \n            \u003cspan class=\"kt\"\u003eImage\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003esystemName\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e\"swift\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eresizable\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003escaledToFit\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eframe\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003ewidth\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e120\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003eheight\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e120\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eforegroundStyle\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etint\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n            \n            \u003cspan class=\"kt\"\u003eText\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"I love SwiftUI! From MszPro.\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003efont\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elargeTitle\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ebold\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n            \n        \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n        \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003epadding\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eglassBackgroundEffect\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003edisplayMode\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enever\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n    \n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"118:1-118:42\"\u003eすると、次のようになります：\u003c/p\u003e\n\u003cp data-sourcepos=\"120:1-120:121\"\u003e\u003ca href=\"https://camo.qiitausercontent.com/275e4671514b242e74056be89e7a262b6cdba6d3/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3633353333302f36303334323434652d623530612d613362322d306434662d6538303032623035316430352e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F6034244e-b50a-a3b2-0d4f-e8002b051d05.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=d03a893af4f64d4a60852dcfe6f402cb\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/6034244e-b50a-a3b2-0d4f-e8002b051d05.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F6034244e-b50a-a3b2-0d4f-e8002b051d05.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=6aff39e0cbbfec09e775790ab6475722 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"122:1-122:167\"\u003e背景のガラス効果を動的に制御することも可能です。例えば、\u003ccode\u003e@State\u003c/code\u003e変数とトグルを使用して、以下のように制御できます：\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"swift\" data-sourcepos=\"124:1-153:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"kd\"\u003estruct\u003c/span\u003e \u003cspan class=\"kt\"\u003eContentView\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eView\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \n    \u003cspan class=\"kd\"\u003e@State\u003c/span\u003e \u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003eshowGlassBackground\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eBool\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003etrue\u003c/span\u003e\n    \n    \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003ebody\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kd\"\u003esome\u003c/span\u003e \u003cspan class=\"kt\"\u003eView\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \n        \u003cspan class=\"kt\"\u003eVStack\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n            \n            \u003cspan class=\"kt\"\u003eImage\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003esystemName\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e\"swift\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eresizable\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003escaledToFit\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eframe\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003ewidth\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e120\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003eheight\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e120\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eforegroundStyle\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etint\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n            \n            \u003cspan class=\"kt\"\u003eText\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"I love SwiftUI! From MszPro.\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003efont\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elargeTitle\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ebold\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n            \n            \u003cspan class=\"kt\"\u003eToggle\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"Show glass background\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003eisOn\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003e$showGlassBackground\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n            \n        \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n        \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003epadding\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eframe\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003ewidth\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e700\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eglassBackgroundEffect\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003edisplayMode\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eshowGlassBackground\u003c/span\u003e \u003cspan class=\"p\"\u003e?\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nv\"\u003ealways\u003c/span\u003e \u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enever\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n    \n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/91b4eaa31a9d8c1b43b21c1db0bf5e970a2e040e/68747470733a2f2f61727469636c652d696d616765732e6d737a70726f2e636f6d2f766973696f6e2d70726f2d746f67676c652d676c6173732d6566666563742d636f6d707265737365642e676966\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Farticle-images.mszpro.com%2Fvision-pro-toggle-glass-effect-compressed.gif?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=02cffef302326a6665778dc018f6e28c\" data-canonical-src=\"https://article-images.mszpro.com/vision-pro-toggle-glass-effect-compressed.gif\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Farticle-images.mszpro.com%2Fvision-pro-toggle-glass-effect-compressed.gif?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=5ab7bc606195daaa9f6ee8b2307ce838 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\n\u003ch2 data-sourcepos=\"157:1-157:38\"\u003e\n\u003cspan id=\"3dアセットバンドルの作成\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#3d%E3%82%A2%E3%82%BB%E3%83%83%E3%83%88%E3%83%90%E3%83%B3%E3%83%89%E3%83%AB%E3%81%AE%E4%BD%9C%E6%88%90\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e3Dアセットバンドルの作成\u003c/h2\u003e\n\u003cp data-sourcepos=\"159:1-160:71\"\u003eアプリに3Dアセットを埋め込むには、3Dアセットファイル（通常は.usdz）を含む特別なSwiftパッケージを作成する必要があります。\u003cbr\u003e\nまず、Reality Composer Proアプリケーションを開きます：\u003c/p\u003e\n\u003cp data-sourcepos=\"162:1-162:121\"\u003e\u003ca href=\"https://camo.qiitausercontent.com/484d477c0aa319f7d5eba3eb6fe5bb9ac79f4aca/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3633353333302f61383263393534612d303062632d303739392d653739642d3131633537666430623131382e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2Fa82c954a-00bc-0799-e79d-11c57fd0b118.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=43a69c2eaf9d55ea139ac9c6c0bb4d23\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/a82c954a-00bc-0799-e79d-11c57fd0b118.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2Fa82c954a-00bc-0799-e79d-11c57fd0b118.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=c6278385fa18e2d84bc427a3f572c2fb 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"164:1-165:146\"\u003e「Create new project」をクリックし、Xcodeプロジェクトと同じフォルダ内にプロジェクトを作成します。\u003cbr\u003e\n新しく作成されたプロジェクトフォルダを見ると、それが実際にはSwiftパッケージであることがわかります：\u003c/p\u003e\n\u003cp data-sourcepos=\"167:1-167:121\"\u003e\u003ca href=\"https://camo.qiitausercontent.com/d1eae01a10d211758907b7cdad2bef0785b8637a/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3633353333302f65383832366439322d393835642d306261632d653166632d3939393061313763653733632e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2Fe8826d92-985d-0bac-e1fc-9990a17ce73c.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=1831c5cae5bbcbb8539e7f48abe88f32\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/e8826d92-985d-0bac-e1fc-9990a17ce73c.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2Fe8826d92-985d-0bac-e1fc-9990a17ce73c.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=e5fa283602b8ad2beaed93999ecbc860 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"169:1-170:353\"\u003e次に、3Dアセットを追加します。Apple Developerのウェブサイトにあるサンプル3Dモデルの中から1つを使用できます。ここで取得できます: \u003ca href=\"https://developer.apple.com/augmented-reality/quick-look/\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://developer.apple.com/augmented-reality/quick-look/\u003c/a\u003e\u003cbr\u003e\nアニメーションについても説明するので、アニメーションが含まれているモデルを選んでください。例えば、ロボット（この記事のチュートリアルに従うためには、このロボットを使用する必要があります）。右クリックして.usdzファイルをダウンロードします。\u003c/p\u003e\n\u003cp data-sourcepos=\"172:1-172:121\"\u003e\u003ca href=\"https://camo.qiitausercontent.com/a3099d0ffa01b16524d60492217a1a9e7b29a987/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3633353333302f32353233336463302d336466332d343539662d656438642d6635653030323465613236652e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F25233dc0-3df3-459f-ed8d-f5e0024ea26e.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=d8ffd37b91d27a4d41bb2d0fc3841829\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/25233dc0-3df3-459f-ed8d-f5e0024ea26e.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F25233dc0-3df3-459f-ed8d-f5e0024ea26e.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=5c891af2df2d27e068b00f2c386ce690 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"174:1-174:224\"\u003eダウンロードしたファイルをReality Composer Proのプロジェクトブラウザにドラッグします。右側のパネルでアニメーション付きのモデルが再生されるのが確認できます：\u003c/p\u003e\n\u003cp data-sourcepos=\"176:1-176:121\"\u003e\u003ca href=\"https://camo.qiitausercontent.com/5fcb04ea3dfdb9c80a81d7a49154d6544dae2596/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3633353333302f62623063616462362d663236332d613237372d323635622d3739363630626262313230642e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2Fbb0cadb6-f263-a277-265b-79660bbb120d.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=5faa450eace12c6964566af75f4fc782\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/bb0cadb6-f263-a277-265b-79660bbb120d.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2Fbb0cadb6-f263-a277-265b-79660bbb120d.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=ea53aee5c1e65543afa826388b04a8ab 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"178:1-178:147\"\u003e次に、Xcodeのプロジェクトで、このReality Composer ProプロジェクトをSwiftパッケージのようにインポートします：\u003c/p\u003e\n\u003cp data-sourcepos=\"180:1-180:121\"\u003e\u003ca href=\"https://camo.qiitausercontent.com/c04adc07a0d51d61c133fb6d35c065c8078d35ae/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3633353333302f38373061373861612d353835612d346530652d333931642d3430313933613965343736382e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F870a78aa-585a-4e0e-391d-40193a9e4768.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=ba0ac659d94634062a67a349a6bf9f88\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/870a78aa-585a-4e0e-391d-40193a9e4768.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F870a78aa-585a-4e0e-391d-40193a9e4768.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=e2fb4f94d162f0c33a9f6aa9ca8ef048 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"182:1-182:121\"\u003e\u003ca href=\"https://camo.qiitausercontent.com/cbefd1b02463fd7cc55529b0b3d0a91ca6c2fe2c/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3633353333302f30356335396233612d306639632d656532342d666136372d3730366361363264333135382e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F05c59b3a-0f9c-ee24-fa67-706ca62d3158.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=2d4ba96ed9dfe379047acaa684fbb9df\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/05c59b3a-0f9c-ee24-fa67-706ca62d3158.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F05c59b3a-0f9c-ee24-fa67-706ca62d3158.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=287393850c29429bd8a02583bab5f9cc 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"184:1-184:121\"\u003e\u003ca href=\"https://camo.qiitausercontent.com/59e7f31ed3ec88c757d26782756afff6d672d30f/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3633353333302f35343663376630632d316435622d663739332d353832632d3965333532306465356639662e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F546c7f0c-1d5b-f793-582c-9e3520de5f9f.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=0aaa75fd2136d755247ee844e09156b2\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/546c7f0c-1d5b-f793-582c-9e3520de5f9f.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F546c7f0c-1d5b-f793-582c-9e3520de5f9f.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=4221ddf0613e1ddc528917c5f91aa388 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"186:1-187:119\"\u003eアセットをバンドルから読み込む関数\u003cbr\u003e\n次に、Reality Composer Proバンドルから特定のアセットを読み込むための関数を作成します：\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"swift\" data-sourcepos=\"189:1-210:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"cp\"\u003e#if os(visionOS)\u003c/span\u003e\n\n\u003cspan class=\"kd\"\u003eimport\u003c/span\u003e \u003cspan class=\"kt\"\u003eFoundation\u003c/span\u003e\n\u003cspan class=\"kd\"\u003eimport\u003c/span\u003e \u003cspan class=\"kt\"\u003eRealityKit\u003c/span\u003e\n\u003cspan class=\"kd\"\u003eimport\u003c/span\u003e \u003cspan class=\"kt\"\u003eDemoAssets\u003c/span\u003e\n\n\u003cspan class=\"kd\"\u003e@MainActor\u003c/span\u003e\n\u003cspan class=\"kd\"\u003efunc\u003c/span\u003e \u003cspan class=\"nf\"\u003eloadFromRealityComposerProject\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003enodeName\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eString\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003esceneFileName\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eString\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003easync\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"kt\"\u003eEntity\u003c/span\u003e\u003cspan class=\"p\"\u003e?\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003eentity\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eEntity\u003c/span\u003e\u003cspan class=\"p\"\u003e?\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003enil\u003c/span\u003e\n    \u003cspan class=\"k\"\u003edo\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003escene\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003etry\u003c/span\u003e \u003cspan class=\"k\"\u003eawait\u003c/span\u003e \u003cspan class=\"kt\"\u003eEntity\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003enamed\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003esceneFileName\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                                     \u003cspan class=\"nv\"\u003ein\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003edemoAssetsBundle\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eentity\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003escene\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003efindEntity\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003enamed\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003enodeName\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e \u003cspan class=\"k\"\u003ecatch\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"nf\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"Failed to load asset named \u003c/span\u003e\u003cspan class=\"se\"\u003e\\(\u003c/span\u003e\u003cspan class=\"n\"\u003enodeName\u003c/span\u003e\u003cspan class=\"se\"\u003e)\u003c/span\u003e\u003cspan class=\"s\"\u003e within the file \u003c/span\u003e\u003cspan class=\"se\"\u003e\\(\u003c/span\u003e\u003cspan class=\"n\"\u003esceneFileName\u003c/span\u003e\u003cspan class=\"se\"\u003e)\u003c/span\u003e\u003cspan class=\"s\"\u003e\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003eentity\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\n\u003cspan class=\"cp\"\u003e#endif\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"212:1-215:145\"\u003eこのコードがVisionOS向けにビルドされている場合のみコンパイルされるように、\u003ccode\u003eif os(visionOS)\u003c/code\u003e チェックを使用しています。\u003cbr\u003e\nこの関数の入力では、ノードの名前（任意）とファイルの名前を受け取ります。\u003cbr\u003e\nノードの名前は任意です。上記のコードでは特定のエンティティ名を指定しない場合、ルートノードが使用されます。ただし、必要に応じて、すべての子ノードを簡単にリストアップすることができます。\u003cbr\u003e\nまず、.usdzファイルを開き、SceneKitにエクスポートして、ファイルに含まれるノードのリストを確認します：\u003c/p\u003e\n\u003cp data-sourcepos=\"217:1-217:121\"\u003e\u003ca href=\"https://camo.qiitausercontent.com/4216aa30bda373d312827204dc150b6e1c651e66/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3633353333302f34366337333935612d633561642d343133642d353438652d3134616131313530343564352e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F46c7395a-c5ad-413d-548e-14aa115045d5.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=f62374f26adaba68fb230c52511ae172\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/46c7395a-c5ad-413d-548e-14aa115045d5.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2F46c7395a-c5ad-413d-548e-14aa115045d5.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=22fa63b70cbbec928e63ea3b1a27b972 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"219:1-219:121\"\u003e\u003ca href=\"https://camo.qiitausercontent.com/59b2cd2ed71ed9959b9aa70c703348f8962a010e/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3633353333302f66333362313766362d346330342d303134312d363233302d3465353963643537323739312e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2Ff33b17f6-4c04-0141-6230-4e59cd572791.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=0da5d6c002d6851b9a9b44255a66f76f\" alt=\"image.png\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/f33b17f6-4c04-0141-6230-4e59cd572791.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F635330%2Ff33b17f6-4c04-0141-6230-4e59cd572791.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=bf4aaaacd9d21214362500b3b2ce49b2 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"221:1-221:111\"\u003e上記の例では、\u003ccode\u003erobot_walk_idle\u003c/code\u003e がルート（トップレベル）エンティティの名前です。\u003c/p\u003e\n\u003cp data-sourcepos=\"223:1-223:159\"\u003eこれは、単一の.usdzファイル内に多くのアセットが含まれていて、そのうちの1つだけを読み込みたい場合に便利です。\u003c/p\u003e\n\u003cdiv data-sourcepos=\"225:1-227:3\" class=\"note info\"\u003e\n\u003cspan class=\"fa fa-fw fa-check-circle\"\u003e\u003c/span\u003e\u003cdiv\u003e\n\u003cp data-sourcepos=\"226:1-226:336\"\u003e💡 .usdzファイルはエンドユーザーに提供される読み取り専用の形式と考えてください。一方、.scnファイルは開発者や3Dデザイナーが3Dファイルの内容を編集するために使用する編集用形式です。.usdzはいつでも.scnに変換でき、暗号化されていません。\u003c/p\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003ch2 data-sourcepos=\"229:1-229:43\"\u003e\n\u003cspan id=\"没入空間-immersive-space-の作成\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E6%B2%A1%E5%85%A5%E7%A9%BA%E9%96%93-immersive-space-%E3%81%AE%E4%BD%9C%E6%88%90\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e没入空間 (Immersive Space) の作成\u003c/h2\u003e\n\u003cp data-sourcepos=\"231:1-231:290\"\u003eユーザーの部屋にコンテンツを表示するには、没入空間を作成する必要があります。これは、アプリケーションウィンドウ内に3Dコンテンツを表示するのとは異なります。この記事では、これらの両方を説明します。\u003c/p\u003e\n\u003cp data-sourcepos=\"233:1-233:208\"\u003e\u003ccode\u003eContentSpace.swift\u003c/code\u003e という新しいファイルを作成します。このファイルでは、RealityViewを使用して3Dアセットを読み込み、設定し、それをビューに追加します：\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"swift\" data-sourcepos=\"235:1-269:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"cp\"\u003e#if os(visionOS)\u003c/span\u003e\n\n\u003cspan class=\"kd\"\u003eimport\u003c/span\u003e \u003cspan class=\"kt\"\u003eSwiftUI\u003c/span\u003e\n\u003cspan class=\"kd\"\u003eimport\u003c/span\u003e \u003cspan class=\"kt\"\u003eRealityKit\u003c/span\u003e\n\n\u003cspan class=\"kd\"\u003estruct\u003c/span\u003e \u003cspan class=\"kt\"\u003eContentSpace\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eView\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \n    \u003cspan class=\"kd\"\u003e@State\u003c/span\u003e \u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003eloaded3DAsset\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eEntity\u003c/span\u003e\u003cspan class=\"p\"\u003e?\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003enil\u003c/span\u003e\n    \n    \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003ebody\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kd\"\u003esome\u003c/span\u003e \u003cspan class=\"kt\"\u003eView\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \n        \u003cspan class=\"kt\"\u003eRealityView\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"n\"\u003econtent\u003c/span\u003e \u003cspan class=\"k\"\u003ein\u003c/span\u003e\n            \n            \u003cspan class=\"n\"\u003eloaded3DAsset\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003eawait\u003c/span\u003e \u003cspan class=\"nf\"\u003eloadFromRealityComposerProject\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003enodeName\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e\"robot_walk_idle\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                                                                 \u003cspan class=\"nv\"\u003esceneFileName\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e\"robot_walk_idle.usdz\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n            \u003cspan class=\"n\"\u003eloaded3DAsset\u003c/span\u003e\u003cspan class=\"p\"\u003e?\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003escale\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003einit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003ez\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n            \u003cspan class=\"n\"\u003eloaded3DAsset\u003c/span\u003e\u003cspan class=\"p\"\u003e?\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eposition\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003einit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003ez\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n            \n            \u003cspan class=\"c1\"\u003e// TODO: Collision\u003c/span\u003e\n            \u003cspan class=\"c1\"\u003e// TODO: Allow user to tap on it\u003c/span\u003e\n            \u003cspan class=\"c1\"\u003e// TODO: add lighting\u003c/span\u003e\n            \n            \u003cspan class=\"k\"\u003eguard\u003c/span\u003e \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003eloaded3DAsset\u003c/span\u003e \u003cspan class=\"k\"\u003eelse\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n                \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n            \n            \u003cspan class=\"n\"\u003econtent\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eadd\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eloaded3DAsset\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n            \n        \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n        \n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n    \n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cdiv data-sourcepos=\"271:1-273:3\" class=\"note info\"\u003e\n\u003cspan class=\"fa fa-fw fa-check-circle\"\u003e\u003c/span\u003e\u003cdiv\u003e\n\u003cp data-sourcepos=\"272:1-272:345\"\u003eオブジェクトに適切なスケールを設定することを忘れないでください。オブジェクトのサイズ（メートル単位）を最初に確認し、それに基づいて計算することができます。オブジェクトが見えない場合、通常それは大きすぎるか小さすぎることが原因です。\u003c/p\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003cp data-sourcepos=\"275:1-275:311\"\u003e次に、メインアプリのコードに移り、上記のビューを没入空間として宣言します。また、スペースにIDを付与することで、アプリのどこからでもそれを呼び出すことができます。呼び出すと、それはユーザーの部屋に表示されます。\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"swift\" data-sourcepos=\"277:1-297:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"kd\"\u003eimport\u003c/span\u003e \u003cspan class=\"kt\"\u003eSwiftUI\u003c/span\u003e\n\n\u003cspan class=\"kd\"\u003e@main\u003c/span\u003e\n\u003cspan class=\"kd\"\u003estruct\u003c/span\u003e \u003cspan class=\"kt\"\u003eVisionOSDemoApp\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eApp\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \n    \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003ebody\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kd\"\u003esome\u003c/span\u003e \u003cspan class=\"kt\"\u003eScene\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \n        \u003cspan class=\"kt\"\u003eWindowGroup\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n            \u003cspan class=\"kt\"\u003eContentView\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n        \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ewindowStyle\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eplain\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \n        \u003cspan class=\"kt\"\u003eImmersiveSpace\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003eid\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e\"robotSpace\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n            \u003cspan class=\"kt\"\u003eContentSpace\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n        \n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n    \n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"299:1-299:136\"\u003e没入空間を開くか、現在表示されている没入空間を閉じるためには、SwiftUIの環境変数を使用します。\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"swift\" data-sourcepos=\"301:1-305:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"kd\"\u003e@Environment\u003c/span\u003e\u003cspan class=\"p\"\u003e(\\\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eopenImmersiveSpace\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003eopenImmersiveSpace\u003c/span\u003e\n\n\u003cspan class=\"kd\"\u003e@Environment\u003c/span\u003e\u003cspan class=\"p\"\u003e(\\\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edismissImmersiveSpace\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003edismissImmersiveSpace\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"307:1-307:193\"\u003e一度に表示できる没入空間は1つだけです。新しいものを表示するには、現在のものを閉じるようにしてください（もし表示されている場合）。\u003c/p\u003e\n\u003cp data-sourcepos=\"309:1-309:90\"\u003e次に、上記のrobotSpaceを2Dビュー内で表示するボタンを追加します。\u003c/p\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/8d66db571294418d436c7c192b7d56dfaf10282c/68747470733a2f2f61727469636c652d696d616765732e6d737a70726f2e636f6d2f73686f772d726f626f742d636f6d707265737365642e676966\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Farticle-images.mszpro.com%2Fshow-robot-compressed.gif?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=596f07e89a4cfdfc96b1d2270be89744\" data-canonical-src=\"https://article-images.mszpro.com/show-robot-compressed.gif\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Farticle-images.mszpro.com%2Fshow-robot-compressed.gif?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=00650deb87582ef96ffaa28b22cc7067 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\n\u003cp data-sourcepos=\"314:1-314:93\"\u003eもし大きすぎる場合は、.scaleファクターを調整することができます。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"316:1-316:36\"\u003e\n\u003cspan id=\"座標空間についての説明\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E5%BA%A7%E6%A8%99%E7%A9%BA%E9%96%93%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6%E3%81%AE%E8%AA%AC%E6%98%8E\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e座標空間についての説明\u003c/h2\u003e\n\u003cp data-sourcepos=\"318:1-318:166\"\u003e\u003ccode\u003eloaded3DAsset?.position = .init(x: 0, y: 0, z: -3)\u003c/code\u003e を使用して、3Dアセットの初期位置を設定していることに気づいたかもしれません。\u003c/p\u003e\n\u003cul data-sourcepos=\"319:1-322:0\"\u003e\n\u003cli data-sourcepos=\"319:1-319:36\"\u003eX軸は左右を意味します。\u003c/li\u003e\n\u003cli data-sourcepos=\"320:1-320:54\"\u003eY軸は上下（天井や床）を意味します。\u003c/li\u003e\n\u003cli data-sourcepos=\"321:1-322:0\"\u003eZ軸はユーザーに対して近いか遠いかを意味します。負の値であればユーザーの前にあり、正の値であればユーザーの後ろにあることを示します。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 data-sourcepos=\"323:1-323:33\"\u003e\n\u003cspan id=\"アニメーションの追加\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%82%A2%E3%83%8B%E3%83%A1%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E3%81%AE%E8%BF%BD%E5%8A%A0\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eアニメーションの追加\u003c/h2\u003e\n\u003cp data-sourcepos=\"325:1-325:137\"\u003eもし3Dモデルにアニメーションが付属している場合、そのアニメーションを再生することができます：\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"swift\" data-sourcepos=\"327:1-350:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"kt\"\u003eRealityView\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"n\"\u003econtent\u003c/span\u003e \u003cspan class=\"k\"\u003ein\u003c/span\u003e\n            \n            \u003cspan class=\"c1\"\u003e// ...\u003c/span\u003e\n            \n            \u003cspan class=\"k\"\u003eguard\u003c/span\u003e \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003eloaded3DAsset\u003c/span\u003e \u003cspan class=\"k\"\u003eelse\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n                \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n            \n            \u003cspan class=\"c1\"\u003e// animation\u003c/span\u003e\n            \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003efirstAnimation\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eloaded3DAsset\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eavailableAnimations\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efirst\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n                \u003cspan class=\"n\"\u003eloaded3DAsset\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eplayAnimation\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003efirstAnimation\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003erepeat\u003c/span\u003e\u003cspan class=\"p\"\u003e(),\u003c/span\u003e\n                                           \u003cspan class=\"nv\"\u003etransitionDuration\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                                           \u003cspan class=\"nv\"\u003estartsPaused\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kc\"\u003efalse\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n            \n            \u003cspan class=\"c1\"\u003e// TODO: Collision\u003c/span\u003e\n            \u003cspan class=\"c1\"\u003e// TODO: Allow user to tap on it\u003c/span\u003e\n            \u003cspan class=\"c1\"\u003e// TODO: add lighting\u003c/span\u003e\n            \n            \u003cspan class=\"n\"\u003econtent\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eadd\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eloaded3DAsset\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n            \n        \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"353:1-353:72\"\u003eこれで、ロボットが歩いているのが見えるはずです。\u003c/p\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/9b6ea085e0db3dec435427009d0fc0341ee54e5b/68747470733a2f2f61727469636c652d696d616765732e6d737a70726f2e636f6d2f77616c6b696e672d726f626f742d636f6d707265737365642e676966\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Farticle-images.mszpro.com%2Fwalking-robot-compressed.gif?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=f273d1eb87124986015cc2de04b72f17\" data-canonical-src=\"https://article-images.mszpro.com/walking-robot-compressed.gif\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Farticle-images.mszpro.com%2Fwalking-robot-compressed.gif?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=f876dd9432298b656878b2453b35ec6c 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\n\u003ch2 data-sourcepos=\"357:1-357:90\"\u003e\n\u003cspan id=\"インタラクティブなロボットモデル用の新しいウィンドウを開く\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%82%A4%E3%83%B3%E3%82%BF%E3%83%A9%E3%82%AF%E3%83%86%E3%82%A3%E3%83%96%E3%81%AA%E3%83%AD%E3%83%9C%E3%83%83%E3%83%88%E3%83%A2%E3%83%87%E3%83%AB%E7%94%A8%E3%81%AE%E6%96%B0%E3%81%97%E3%81%84%E3%82%A6%E3%82%A3%E3%83%B3%E3%83%89%E3%82%A6%E3%82%92%E9%96%8B%E3%81%8F\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eインタラクティブなロボットモデル用の新しいウィンドウを開く\u003c/h2\u003e\n\u003cp data-sourcepos=\"359:1-359:303\"\u003eロボットをユーザーの空間ではなく新しいウィンドウで表示し、ユーザーがピンチ操作でロボットをスケールしたり、手を使って回転させたりしてさまざまな角度からロボットを見ることができるようにすることもできます。\u003c/p\u003e\n\u003cp data-sourcepos=\"361:1-361:125\"\u003eまず、どのARモデルを表示するかをアプリに指示するデータ構造を定義する必要があります。\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"swift\" data-sourcepos=\"363:1-372:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"kd\"\u003estruct\u003c/span\u003e \u003cspan class=\"kt\"\u003eARModelOpenParameter\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eIdentifiable\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"kt\"\u003eHashable\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"kt\"\u003eCodable\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003eid\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eString\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s\"\u003e\"\u003c/span\u003e\u003cspan class=\"se\"\u003e\\(\u003c/span\u003e\u003cspan class=\"n\"\u003emodelName\u003c/span\u003e\u003cspan class=\"se\"\u003e)\u003c/span\u003e\u003cspan class=\"s\"\u003e-\u003c/span\u003e\u003cspan class=\"se\"\u003e\\(\u003c/span\u003e\u003cspan class=\"n\"\u003emodelNodeName\u003c/span\u003e\u003cspan class=\"se\"\u003e)\u003c/span\u003e\u003cspan class=\"s\"\u003e\"\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n    \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003emodelName\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eString\u003c/span\u003e\n    \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003emodelNodeName\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eString\u003c/span\u003e\n    \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003einitialScale\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eFloat\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"374:1-374:285\"\u003e次に、ユーザーがドラッグしてモデルを回転できるようにするため、以下のヘルパー関数をコード内に追加します。以下のコードは、Apple Developerのドキュメントにあるサンプルプロジェクトから引用しています。\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"swift\" data-sourcepos=\"376:1-515:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"cm\"\u003e/*\nSee the LICENSE.txt file for this sample’s licensing information.\n\nAbstract:\nA modifier for turning drag gestures into rotation.\n*/\u003c/span\u003e\n\n\u003cspan class=\"kd\"\u003eimport\u003c/span\u003e \u003cspan class=\"kt\"\u003eSwiftUI\u003c/span\u003e\n\u003cspan class=\"kd\"\u003eimport\u003c/span\u003e \u003cspan class=\"kt\"\u003eRealityKit\u003c/span\u003e\n\n\u003cspan class=\"kd\"\u003eextension\u003c/span\u003e \u003cspan class=\"kt\"\u003eView\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e/// Enables people to drag an entity to rotate it, with optional limitations\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e/// on the rotation in yaw and pitch.\u003c/span\u003e\n    \u003cspan class=\"kd\"\u003efunc\u003c/span\u003e \u003cspan class=\"nf\"\u003edragRotation\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n        \u003cspan class=\"nv\"\u003eyawLimit\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eAngle\u003c/span\u003e\u003cspan class=\"p\"\u003e?\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003enil\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n        \u003cspan class=\"nv\"\u003epitchLimit\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eAngle\u003c/span\u003e\u003cspan class=\"p\"\u003e?\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003enil\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n        \u003cspan class=\"nv\"\u003esensitivity\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eDouble\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e10\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n        \u003cspan class=\"nv\"\u003eaxRotateClockwise\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eBool\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003efalse\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n        \u003cspan class=\"nv\"\u003eaxRotateCounterClockwise\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eBool\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003efalse\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"kd\"\u003esome\u003c/span\u003e \u003cspan class=\"kt\"\u003eView\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"k\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003emodifier\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n            \u003cspan class=\"kt\"\u003eDragRotationModifier\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n                \u003cspan class=\"nv\"\u003eyawLimit\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eyawLimit\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                \u003cspan class=\"nv\"\u003epitchLimit\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003epitchLimit\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                \u003cspan class=\"nv\"\u003esensitivity\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003esensitivity\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                \u003cspan class=\"nv\"\u003eaxRotateClockwise\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eaxRotateClockwise\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                \u003cspan class=\"nv\"\u003eaxRotateCounterClockwise\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eaxRotateCounterClockwise\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e/// A modifier converts drag gestures into entity rotation.\u003c/span\u003e\n\u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e \u003cspan class=\"kd\"\u003estruct\u003c/span\u003e \u003cspan class=\"kt\"\u003eDragRotationModifier\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eViewModifier\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003eyawLimit\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eAngle\u003c/span\u003e\u003cspan class=\"p\"\u003e?\u003c/span\u003e\n    \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003epitchLimit\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eAngle\u003c/span\u003e\u003cspan class=\"p\"\u003e?\u003c/span\u003e\n    \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003esensitivity\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eDouble\u003c/span\u003e\n    \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003eaxRotateClockwise\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eBool\u003c/span\u003e\n    \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003eaxRotateCounterClockwise\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eBool\u003c/span\u003e\n\n    \u003cspan class=\"kd\"\u003e@State\u003c/span\u003e \u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003ebaseYaw\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eDouble\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\n    \u003cspan class=\"kd\"\u003e@State\u003c/span\u003e \u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003eyaw\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eDouble\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\n    \u003cspan class=\"kd\"\u003e@State\u003c/span\u003e \u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003ebasePitch\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eDouble\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\n    \u003cspan class=\"kd\"\u003e@State\u003c/span\u003e \u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003epitch\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eDouble\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\n\n    \u003cspan class=\"kd\"\u003efunc\u003c/span\u003e \u003cspan class=\"nf\"\u003ebody\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003econtent\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eContent\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"kd\"\u003esome\u003c/span\u003e \u003cspan class=\"kt\"\u003eView\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"n\"\u003econtent\u003c/span\u003e\n            \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003erotation3DEffect\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eradians\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eyaw\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e \u003cspan class=\"p\"\u003e?\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.01\u003c/span\u003e \u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eyaw\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"nv\"\u003eaxis\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n            \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003erotation3DEffect\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eradians\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epitch\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e \u003cspan class=\"p\"\u003e?\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.01\u003c/span\u003e \u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003epitch\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"nv\"\u003eaxis\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n            \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003egesture\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eDragGesture\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003eminimumDistance\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.0\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003etargetedToAnyEntity\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eonChanged\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"n\"\u003evalue\u003c/span\u003e \u003cspan class=\"k\"\u003ein\u003c/span\u003e\n                    \u003cspan class=\"c1\"\u003e// Find the current linear displacement.\u003c/span\u003e\n                    \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003elocation3D\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003evalue\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003econvert\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003evalue\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elocation3D\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003efrom\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elocal\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003eto\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003escene\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                    \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003estartLocation3D\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003evalue\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003econvert\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003evalue\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estartLocation3D\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003efrom\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elocal\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003eto\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003escene\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                    \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003edelta\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003elocation3D\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"n\"\u003estartLocation3D\u003c/span\u003e\n\n                    \u003cspan class=\"c1\"\u003e// Use an interactive spring animation that becomes\u003c/span\u003e\n                    \u003cspan class=\"c1\"\u003e// a spring animation when the gesture ends below.\u003c/span\u003e\n                    \u003cspan class=\"nf\"\u003ewithAnimation\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003einteractiveSpring\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n                        \u003cspan class=\"n\"\u003eyaw\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003espin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003edisplacement\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eDouble\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edelta\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"nv\"\u003ebase\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003ebaseYaw\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003elimit\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eyawLimit\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                        \u003cspan class=\"n\"\u003epitch\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003espin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003edisplacement\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eDouble\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edelta\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"nv\"\u003ebase\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003ebasePitch\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003elimit\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003epitchLimit\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n                \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eonEnded\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"n\"\u003evalue\u003c/span\u003e \u003cspan class=\"k\"\u003ein\u003c/span\u003e\n                    \u003cspan class=\"c1\"\u003e// Find the current and predicted final linear displacements.\u003c/span\u003e\n                    \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003elocation3D\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003evalue\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003econvert\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003evalue\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elocation3D\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003efrom\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elocal\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003eto\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003escene\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                    \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003estartLocation3D\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003evalue\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003econvert\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003evalue\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003estartLocation3D\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003efrom\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elocal\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003eto\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003escene\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                    \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003epredictedEndLocation3D\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003evalue\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003econvert\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003evalue\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epredictedEndLocation3D\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003efrom\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elocal\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003eto\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003escene\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                    \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003edelta\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003elocation3D\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"n\"\u003estartLocation3D\u003c/span\u003e\n                    \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003epredictedDelta\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003epredictedEndLocation3D\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"n\"\u003elocation3D\u003c/span\u003e\n\n                    \u003cspan class=\"c1\"\u003e// Set the final spin value using a spring animation.\u003c/span\u003e\n                    \u003cspan class=\"nf\"\u003ewithAnimation\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003espring\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n                        \u003cspan class=\"n\"\u003eyaw\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003efinalSpin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n                            \u003cspan class=\"nv\"\u003edisplacement\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eDouble\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edelta\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n                            \u003cspan class=\"nv\"\u003epredictedDisplacement\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eDouble\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epredictedDelta\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n                            \u003cspan class=\"nv\"\u003ebase\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003ebaseYaw\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                            \u003cspan class=\"nv\"\u003elimit\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eyawLimit\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                        \u003cspan class=\"n\"\u003epitch\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003efinalSpin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n                            \u003cspan class=\"nv\"\u003edisplacement\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eDouble\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edelta\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n                            \u003cspan class=\"nv\"\u003epredictedDisplacement\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eDouble\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epredictedDelta\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n                            \u003cspan class=\"nv\"\u003ebase\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003ebasePitch\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                            \u003cspan class=\"nv\"\u003elimit\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003epitchLimit\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\n                    \u003cspan class=\"c1\"\u003e// Store the last value for use by the next gesture.\u003c/span\u003e\n                    \u003cspan class=\"n\"\u003ebaseYaw\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eyaw\u003c/span\u003e\n                    \u003cspan class=\"n\"\u003ebasePitch\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003epitch\u003c/span\u003e\n                \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e)\u003c/span\u003e\n            \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eonChange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003eof\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eaxRotateClockwise\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n                \u003cspan class=\"nf\"\u003ewithAnimation\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003espring\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n                    \u003cspan class=\"n\"\u003eyaw\u003c/span\u003e \u003cspan class=\"o\"\u003e-=\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epi\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"mi\"\u003e6\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                    \u003cspan class=\"n\"\u003ebaseYaw\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eyaw\u003c/span\u003e\n                \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n            \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eonChange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003eof\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eaxRotateCounterClockwise\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n                \u003cspan class=\"nf\"\u003ewithAnimation\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003espring\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n                    \u003cspan class=\"n\"\u003eyaw\u003c/span\u003e \u003cspan class=\"o\"\u003e+=\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epi\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"mi\"\u003e6\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                    \u003cspan class=\"n\"\u003ebaseYaw\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eyaw\u003c/span\u003e\n                \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\n    \u003cspan class=\"c1\"\u003e/// Finds the spin for the specified linear displacement, subject to an\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e/// optional limit.\u003c/span\u003e\n    \u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e \u003cspan class=\"kd\"\u003efunc\u003c/span\u003e \u003cspan class=\"nf\"\u003espin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n        \u003cspan class=\"nv\"\u003edisplacement\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eDouble\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n        \u003cspan class=\"nv\"\u003ebase\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eDouble\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n        \u003cspan class=\"nv\"\u003elimit\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eAngle\u003c/span\u003e\u003cspan class=\"p\"\u003e?\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"kt\"\u003eDouble\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003elimit\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n            \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"nf\"\u003eatan\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edisplacement\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003esensitivity\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elimit\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edegrees\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"mi\"\u003e90\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e}\u003c/span\u003e \u003cspan class=\"k\"\u003eelse\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n            \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ebase\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003edisplacement\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003esensitivity\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\n    \u003cspan class=\"c1\"\u003e/// Finds the final spin given the current and predicted final linear\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e/// displacements, or zero when the spin is restricted.\u003c/span\u003e\n    \u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e \u003cspan class=\"kd\"\u003efunc\u003c/span\u003e \u003cspan class=\"nf\"\u003efinalSpin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n        \u003cspan class=\"nv\"\u003edisplacement\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eDouble\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n        \u003cspan class=\"nv\"\u003epredictedDisplacement\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eDouble\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n        \u003cspan class=\"nv\"\u003ebase\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eDouble\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n        \u003cspan class=\"nv\"\u003elimit\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eAngle\u003c/span\u003e\u003cspan class=\"p\"\u003e?\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"kt\"\u003eDouble\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e// If there is a spin limit, always return to zero spin at the end.\u003c/span\u003e\n        \u003cspan class=\"k\"\u003eguard\u003c/span\u003e \u003cspan class=\"n\"\u003elimit\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"kc\"\u003enil\u003c/span\u003e \u003cspan class=\"k\"\u003eelse\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e// Find the projected final linear displacement, capped at 1 more revolution.\u003c/span\u003e\n        \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003ecap\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epi\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"mf\"\u003e2.0\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"n\"\u003esensitivity\u003c/span\u003e\n        \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003edelta\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003edisplacement\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"nf\"\u003emax\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003ecap\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nf\"\u003emin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecap\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003epredictedDisplacement\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e// Find the final spin.\u003c/span\u003e\n        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ebase\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003edelta\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003esensitivity\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"517:1-517:70\"\u003eその後、メインのSwiftUIアプリコードを更新します。\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"swift\" data-sourcepos=\"519:1-565:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"kd\"\u003e@main\u003c/span\u003e\n\u003cspan class=\"kd\"\u003estruct\u003c/span\u003e \u003cspan class=\"kt\"\u003eVisionOSDemoApp\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eApp\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \n    \u003cspan class=\"kd\"\u003e@Environment\u003c/span\u003e\u003cspan class=\"p\"\u003e(\\\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edismissWindow\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003edismissWindow\u003c/span\u003e\n    \n    \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003ebody\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kd\"\u003esome\u003c/span\u003e \u003cspan class=\"kt\"\u003eSwiftUI\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"kt\"\u003eScene\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \n        \u003cspan class=\"kt\"\u003eWindowGroup\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n            \u003cspan class=\"kt\"\u003eContentView\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n        \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ewindowStyle\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eplain\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \n        \u003cspan class=\"kt\"\u003eImmersiveSpace\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003eid\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e\"robotSpace\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n            \u003cspan class=\"kt\"\u003eContentSpace\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n        \n        \u003cspan class=\"kt\"\u003eWindowGroup\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003efor\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eARModelOpenParameter\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"k\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"n\"\u003e$object\u003c/span\u003e \u003cspan class=\"k\"\u003ein\u003c/span\u003e\n            \u003cspan class=\"c1\"\u003e// 3D view\u003c/span\u003e\n            \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003eobject\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n                \u003cspan class=\"kt\"\u003eRealityView\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"n\"\u003econtent\u003c/span\u003e \u003cspan class=\"k\"\u003ein\u003c/span\u003e\n                    \u003cspan class=\"k\"\u003eguard\u003c/span\u003e \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003earAsset\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003eawait\u003c/span\u003e \u003cspan class=\"nf\"\u003eloadFromRealityComposerProject\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n                        \u003cspan class=\"nv\"\u003enodeName\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eobject\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emodelNodeName\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                        \u003cspan class=\"nv\"\u003esceneFileName\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eobject\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emodelName\u003c/span\u003e\n                    \u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eelse\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n                        \u003cspan class=\"nf\"\u003efatalError\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"Unable to load beam from Reality Composer Pro project.\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n                    \u003cspan class=\"n\"\u003earAsset\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003egenerateCollisionShapes\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003erecursive\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kc\"\u003etrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                    \u003cspan class=\"n\"\u003earAsset\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eposition\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003einit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003ez\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                    \u003cspan class=\"n\"\u003earAsset\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003escale\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003einit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eobject\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003einitialScale\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                                          \u003cspan class=\"nv\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eobject\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003einitialScale\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                                          \u003cspan class=\"nv\"\u003ez\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eobject\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003einitialScale\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                    \u003cspan class=\"n\"\u003earAsset\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecomponents\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"kt\"\u003eInputTargetComponent\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"k\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kt\"\u003eInputTargetComponent\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003eallowedInputTypes\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eall\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                    \u003cspan class=\"n\"\u003econtent\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eadd\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003earAsset\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003edragRotation\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eframe\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003ewidth\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e900\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003eheight\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e900\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eglassBackgroundEffect\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003edisplayMode\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ealways\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n        \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ewindowStyle\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evolumetric\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003edefaultSize\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003ewidth\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.5\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003eheight\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.5\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003edepth\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.5\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003ein\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emeters\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n    \n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"567:1-567:48\"\u003eここでの変更点は以下の通りです：\u003c/p\u003e\n\u003cul data-sourcepos=\"569:1-572:0\"\u003e\n\u003cli data-sourcepos=\"569:1-569:190\"\u003eビューのタイプを\u003ccode\u003eScene\u003c/code\u003eから\u003ccode\u003eSwiftUI.Scene\u003c/code\u003eに変更します。これは、同じ名前がRealityKitのシステムフレームワーク内でも使われているためです。\u003c/li\u003e\n\u003cli data-sourcepos=\"570:1-570:270\"\u003e\n\u003ccode\u003eWindowGroup\u003c/code\u003eを新しく追加し、\u003ccode\u003eARModelOpenParameter\u003c/code\u003eで提供されたデータに基づいて開くようにします。新しいウィンドウ内では、ARオブジェクトが表示され、そのオブジェクトを回転させることができます。\u003c/li\u003e\n\u003cli data-sourcepos=\"571:1-572:0\"\u003eそのウィンドウを表示するには、\u003ccode\u003eopenWindow\u003c/code\u003e環境変数を使用し、\u003ccode\u003eARModelOpenParameter\u003c/code\u003eを構築します。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"573:1-579:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e@Environment(\\.openWindow) private var openWindow\n\nButton(\"Inspect robot in a window\") {\n    self.openWindow(value: ARModelOpenParameter(modelName: \"robot_walk_idle.usdz\", modelNodeName: \"robot_walk_idle\", initialScale: 0.01))\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"581:1-581:161\"\u003eこれで、このボタンをタップして、回転可能な3Dモデルを検査するための別のウィンドウを表示できるようになります。\u003c/p\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/fa9038a5b8ea8998ba246fcbf5e17e76114a31ec/68747470733a2f2f61727469636c652d696d616765732e6d737a70726f2e636f6d2f696e73706563742d33642d6d6f64656c2e676966\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Farticle-images.mszpro.com%2Finspect-3d-model.gif?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=8cd6803e21a26ed1c1b01037521e9358\" data-canonical-src=\"https://article-images.mszpro.com/inspect-3d-model.gif\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Farticle-images.mszpro.com%2Finspect-3d-model.gif?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=0898997d764f9c9eef4138f16ae82ed7 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\n\u003ch2 data-sourcepos=\"585:1-585:44\"\u003e\n\u003cspan id=\"システム3dビューアを使用する\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A03d%E3%83%93%E3%83%A5%E3%83%BC%E3%82%A2%E3%82%92%E4%BD%BF%E7%94%A8%E3%81%99%E3%82%8B\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eシステム3Dビューアを使用する\u003c/h2\u003e\n\u003cp data-sourcepos=\"587:1-587:230\"\u003e上記の方法では、モデルのプレビューウィンドウを自分でデザインする方法を示しましたが、VisionOSシステムが提供するモデルプレビューを直接使用することもできます：\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"swift\" data-sourcepos=\"589:1-597:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"kt\"\u003eModel3D\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003enamed\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e\"Robot-Drummer\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"n\"\u003emodel\u003c/span\u003e \u003cspan class=\"k\"\u003ein\u003c/span\u003e\n    \u003cspan class=\"n\"\u003emodel\u003c/span\u003e\n        \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eresizable\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003easpectRatio\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003econtentMode\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efit\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e \u003cspan class=\"nv\"\u003eplaceholder\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"kt\"\u003eProgressView\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"599:1-599:81\"\u003e作成したパッケージ内のモデルを使用することも可能です：\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"swift\" data-sourcepos=\"601:1-604:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"kt\"\u003eModel3D\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003enamed\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e\"robot_walk_idle.usdz\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003ebundle\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003edemoAssetsBundle\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003epadding\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebottom\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e50\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"606:1-606:61\"\u003e\u003ciframe id=\"qiita-embed-content__522ec846f5dbe2bb880bbcf537bca22e\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__522ec846f5dbe2bb880bbcf537bca22e\" data-content=\"https%3A%2F%2Fdeveloper.apple.com%2Fdocumentation%2Frealitykit%2Fmodel3d%2F\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\"\u003e\n\u003c/iframe\u003e\n\u003c/p\u003e\n\u003ch2 data-sourcepos=\"608:1-608:48\"\u003e\n\u003cspan id=\"モデルをタップできるようにする\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E3%82%BF%E3%83%83%E3%83%97%E3%81%A7%E3%81%8D%E3%82%8B%E3%82%88%E3%81%86%E3%81%AB%E3%81%99%E3%82%8B\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eモデルをタップできるようにする\u003c/h2\u003e\n\u003cp data-sourcepos=\"610:1-610:190\"\u003eVisionOSでは、ユーザーは目で見て、指をピンチすることで何かを選択できます。この機能をシーンに追加した3Dオブジェクトに適用できます。\u003c/p\u003e\n\u003cdiv data-sourcepos=\"612:1-614:3\" class=\"note info\"\u003e\n\u003cspan class=\"fa fa-fw fa-check-circle\"\u003e\u003c/span\u003e\u003cdiv\u003e\n\u003cp data-sourcepos=\"613:1-613:173\"\u003e💡注意: これは、手で3Dモデルに触れるのとは異なります。手の追跡は複雑なため、この機能については別の記事で説明します。\u003c/p\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003cp data-sourcepos=\"616:1-616:158\"\u003e読み込んだ3Dエンティティに以下の行を追加することで、ユーザーがそれをタップできるようにすることができます：\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"swift\" data-sourcepos=\"618:1-620:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003earAsset\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecomponents\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"kt\"\u003eInputTargetComponent\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"k\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kt\"\u003eInputTargetComponent\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003eallowedInputTypes\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eall\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"622:1-622:363\"\u003eタップイベントを受け取るには、\u003ccode\u003eRealityView\u003c/code\u003e を使用している3D空間ビューに次のジェスチャーを追加する必要があります。ユーザーがタップしたノードは \u003ccode\u003etappedNode\u003c/code\u003e として取得できますが、これは追加したノードの子ノードである可能性があることに注意してください。\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"swift\" data-sourcepos=\"624:1-631:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003egesture\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eTapGesture\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003etargetedToAnyEntity\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eonEnded\u003c/span\u003e\u003cspan class=\"p\"\u003e({\u003c/span\u003e \u003cspan class=\"n\"\u003etap\u003c/span\u003e \u003cspan class=\"k\"\u003ein\u003c/span\u003e\n            \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003etappedNode\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etap\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eentity\u003c/span\u003e\n            \n        \u003cspan class=\"p\"\u003e}))\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"633:1-633:270\"\u003eそのため、ユーザーが探しているノードをタップしたかどうかを確認するために、親ノードを再帰的にチェックすることを忘れないでください。ノードの名前を識別子として使用することもできます。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"635:1-635:16\"\u003e\n\u003cspan id=\"whac-a-robot\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#whac-a-robot\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eWhac-A-Robot!\u003c/h2\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/e37e1249faa107a3dbff8d4c0bec1fe083768a00/68747470733a2f2f61727469636c652d696d616765732e6d737a70726f2e636f6d2f7261636b2d612d626f742d67616d652d636f6d707265737365642e676966\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Farticle-images.mszpro.com%2Frack-a-bot-game-compressed.gif?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=5f4b2232bda5293eb0a75e421b630285\" data-canonical-src=\"https://article-images.mszpro.com/rack-a-bot-game-compressed.gif\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Farticle-images.mszpro.com%2Frack-a-bot-game-compressed.gif?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=21af691f2e76d2a603068965a404e6fb 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\n\u003cp data-sourcepos=\"639:1-639:269\"\u003e前のステップでは、読み込んだ3DモデルをContentSpaceに保存しました。アプリ内のどこからでもそのアセットにアクセスできるように、より柔軟にするために、それをモデルに保存することができます：\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"swift\" data-sourcepos=\"641:1-698:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"kd\"\u003eimport\u003c/span\u003e \u003cspan class=\"kt\"\u003eSwiftUI\u003c/span\u003e\n\u003cspan class=\"kd\"\u003eimport\u003c/span\u003e \u003cspan class=\"kt\"\u003eRealityKit\u003c/span\u003e\n\n\u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003econtentSpaceOrigin\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kt\"\u003eEntity\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\n\u003cspan class=\"kd\"\u003e@Observable\u003c/span\u003e\n\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"kt\"\u003eContentModel\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \n    \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003eloaded3DAsset\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eEntity\u003c/span\u003e\u003cspan class=\"p\"\u003e?\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003enil\u003c/span\u003e\n    \n    \u003cspan class=\"kd\"\u003e@MainActor\u003c/span\u003e\n    \u003cspan class=\"kd\"\u003efunc\u003c/span\u003e \u003cspan class=\"nf\"\u003eloadAssets\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"k\"\u003easync\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eloaded3DAsset\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003eawait\u003c/span\u003e \u003cspan class=\"nf\"\u003eloadFromRealityComposerProject\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003enodeName\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e\"robot_walk_idle\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                                                             \u003cspan class=\"nv\"\u003esceneFileName\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e\"robot_walk_idle.usdz\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eloaded3DAsset\u003c/span\u003e\u003cspan class=\"p\"\u003e?\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ename\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e\"robot_root_node\"\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eloaded3DAsset\u003c/span\u003e\u003cspan class=\"p\"\u003e?\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003escale\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003einit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.05\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.05\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003ez\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.05\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eloaded3DAsset\u003c/span\u003e\u003cspan class=\"p\"\u003e?\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eposition\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003einit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003ez\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \n        \u003cspan class=\"k\"\u003eguard\u003c/span\u003e \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003eloaded3DAsset\u003c/span\u003e \u003cspan class=\"k\"\u003eelse\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n            \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n        \n        \u003cspan class=\"c1\"\u003e// animation\u003c/span\u003e\n        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003efirstAnimation\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eloaded3DAsset\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eavailableAnimations\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efirst\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n            \u003cspan class=\"n\"\u003eloaded3DAsset\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eplayAnimation\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003efirstAnimation\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003erepeat\u003c/span\u003e\u003cspan class=\"p\"\u003e(),\u003c/span\u003e\n                                        \u003cspan class=\"nv\"\u003etransitionDuration\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                                        \u003cspan class=\"nv\"\u003estartsPaused\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kc\"\u003efalse\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n        \n        \u003cspan class=\"c1\"\u003e// allow tap\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eloaded3DAsset\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003egenerateCollisionShapes\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003erecursive\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kc\"\u003etrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eloaded3DAsset\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecomponents\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"kt\"\u003eInputTargetComponent\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"k\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kt\"\u003eInputTargetComponent\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003eallowedInputTypes\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eall\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \n        \u003cspan class=\"n\"\u003econtentSpaceOrigin\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eaddChild\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eloaded3DAsset\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n    \n    \u003cspan class=\"kd\"\u003efunc\u003c/span\u003e \u003cspan class=\"nf\"\u003eaddRobotAtRandomPosition\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"k\"\u003eguard\u003c/span\u003e \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003eloaded3DAsset\u003c/span\u003e \u003cspan class=\"k\"\u003eelse\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e// clone the already loaded robot node\u003c/span\u003e\n        \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003enewRobot\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eloaded3DAsset\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eclone\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003erecursive\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kc\"\u003etrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"n\"\u003enewRobot\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ename\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e\"robot_root_node\"\u003c/span\u003e\n        \u003cspan class=\"n\"\u003enewRobot\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eposition\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003einit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eFloat\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003erandom\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003ein\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"o\"\u003e...\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n                                  \u003cspan class=\"nv\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eFloat\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003erandom\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003ein\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"o\"\u003e...\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n                                  \u003cspan class=\"nv\"\u003ez\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eFloat\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003erandom\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003ein\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"o\"\u003e...\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e)))\u003c/span\u003e\n        \u003cspan class=\"n\"\u003enewRobot\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003escale\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003einit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.03\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.03\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003ez\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.03\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003efirstAnimation\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003enewRobot\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eavailableAnimations\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efirst\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n            \u003cspan class=\"n\"\u003enewRobot\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eplayAnimation\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003efirstAnimation\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003erepeat\u003c/span\u003e\u003cspan class=\"p\"\u003e(),\u003c/span\u003e\n                                        \u003cspan class=\"nv\"\u003etransitionDuration\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                                        \u003cspan class=\"nv\"\u003estartsPaused\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kc\"\u003efalse\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n        \u003cspan class=\"n\"\u003enewRobot\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003egenerateCollisionShapes\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003erecursive\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kc\"\u003etrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"n\"\u003enewRobot\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecomponents\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"kt\"\u003eInputTargetComponent\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"k\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kt\"\u003eInputTargetComponent\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003eallowedInputTypes\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eall\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"n\"\u003econtentSpaceOrigin\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eaddChild\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003enewRobot\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n    \n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"700:1-700:317\"\u003eここでは、アセットをメモリに読み込む\u003ccode\u003eloadAssets\u003c/code\u003e関数があります。この関数は、没入空間が表示される前に呼び出されます。また、ゲーム内でロボットをランダムな場所に追加するために使用される\u003ccode\u003eaddRobotAtRandomPosition\u003c/code\u003e関数もあります。\u003c/p\u003e\n\u003cp data-sourcepos=\"702:1-702:568\"\u003e上記のコードに見られるように、没入ビューが表示される前にアセットを読み込む必要があります。しかし、没入ビューが表示される前には、アセットを追加するためのワールドルートノード（部屋の環境のノード）が存在しません。そのため、すべての読み込んだアセットを追加するためのグローバル変数\u003ccode\u003econtentSpaceOrigin\u003c/code\u003eを作成し、没入空間が表示されたときに、\u003ccode\u003econtentSpaceOrigin\u003c/code\u003eをワールドノードの子として追加します。\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"swift\" data-sourcepos=\"705:1-739:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"kd\"\u003eimport\u003c/span\u003e \u003cspan class=\"kt\"\u003eSwiftUI\u003c/span\u003e\n\u003cspan class=\"kd\"\u003eimport\u003c/span\u003e \u003cspan class=\"kt\"\u003eRealityKit\u003c/span\u003e\n\n\u003cspan class=\"kd\"\u003estruct\u003c/span\u003e \u003cspan class=\"kt\"\u003eContentSpace\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eView\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \n    \u003cspan class=\"kd\"\u003e@Environment\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eContentModel\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"k\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003egameModel\u003c/span\u003e\n    \n    \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003ebody\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kd\"\u003esome\u003c/span\u003e \u003cspan class=\"kt\"\u003eView\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \n        \u003cspan class=\"kt\"\u003eRealityView\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"n\"\u003econtent\u003c/span\u003e \u003cspan class=\"k\"\u003ein\u003c/span\u003e\n            \u003cspan class=\"n\"\u003econtent\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eadd\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003econtentSpaceOrigin\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n        \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003egesture\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eTapGesture\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n            \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003etargetedToAnyEntity\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n            \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eonEnded\u003c/span\u003e\u003cspan class=\"p\"\u003e({\u003c/span\u003e \u003cspan class=\"n\"\u003etap\u003c/span\u003e \u003cspan class=\"k\"\u003ein\u003c/span\u003e\n                \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003etappedNode\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etap\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eentity\u003c/span\u003e\n                \u003cspan class=\"c1\"\u003e// look up until it reaches the robot main node\u003c/span\u003e\n                \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003efoundRobotMainNode\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eEntity\u003c/span\u003e\u003cspan class=\"p\"\u003e?\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etappedNode\u003c/span\u003e\n                \u003cspan class=\"k\"\u003ewhile\u003c/span\u003e \u003cspan class=\"n\"\u003efoundRobotMainNode\u003c/span\u003e \u003cspan class=\"o\"\u003e!=\u003c/span\u003e \u003cspan class=\"kc\"\u003enil\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e\n                        \u003cspan class=\"n\"\u003efoundRobotMainNode\u003c/span\u003e\u003cspan class=\"p\"\u003e?\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eparent\u003c/span\u003e \u003cspan class=\"o\"\u003e!=\u003c/span\u003e \u003cspan class=\"kc\"\u003enil\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n                    \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003efoundRobotMainNode\u003c/span\u003e\u003cspan class=\"p\"\u003e?\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ename\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"s\"\u003e\"robot_root_node\"\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n                        \u003cspan class=\"k\"\u003ebreak\u003c/span\u003e \u003cspan class=\"c1\"\u003e// we found it!\u003c/span\u003e\n                    \u003cspan class=\"p\"\u003e}\u003c/span\u003e \u003cspan class=\"k\"\u003eelse\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n                        \u003cspan class=\"n\"\u003efoundRobotMainNode\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003efoundRobotMainNode\u003c/span\u003e\u003cspan class=\"p\"\u003e?\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eparent\u003c/span\u003e\n                    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n                \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n                \u003cspan class=\"n\"\u003efoundRobotMainNode\u003c/span\u003e\u003cspan class=\"p\"\u003e?\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eremoveFromParent\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n                \u003cspan class=\"nf\"\u003espeak\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003etext\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e\"まだね\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e}))\u003c/span\u003e\n        \n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n    \n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"741:1-742:95\"\u003e上記のコードでは、タップジェスチャーの検出を設定しています。タップジェスチャーがあるたびに、アプリはタップされたロボットを削除します。\u003cbr\u003e\n次に、2Dビューにランダムなロボットを追加するボタンを配置します。\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"swift\" data-sourcepos=\"744:1-805:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"kd\"\u003eimport\u003c/span\u003e \u003cspan class=\"kt\"\u003eSwiftUI\u003c/span\u003e\n\u003cspan class=\"kd\"\u003eimport\u003c/span\u003e \u003cspan class=\"kt\"\u003eRealityKit\u003c/span\u003e\n\n\u003cspan class=\"kd\"\u003estruct\u003c/span\u003e \u003cspan class=\"kt\"\u003eContentView\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eView\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \n    \u003cspan class=\"kd\"\u003e@State\u003c/span\u003e \u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003eshowGlassBackground\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eBool\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kc\"\u003etrue\u003c/span\u003e\n    \n    \u003cspan class=\"kd\"\u003e@Environment\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eContentModel\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"k\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003egameModel\u003c/span\u003e\n    \u003cspan class=\"kd\"\u003e@Environment\u003c/span\u003e\u003cspan class=\"p\"\u003e(\\\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eopenImmersiveSpace\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003eopenImmersiveSpace\u003c/span\u003e\n    \u003cspan class=\"kd\"\u003e@Environment\u003c/span\u003e\u003cspan class=\"p\"\u003e(\\\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edismissImmersiveSpace\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003edismissImmersiveSpace\u003c/span\u003e\n    \n    \u003cspan class=\"kd\"\u003e@Environment\u003c/span\u003e\u003cspan class=\"p\"\u003e(\\\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eopenWindow\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"kd\"\u003eprivate\u003c/span\u003e \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003eopenWindow\u003c/span\u003e\n    \n    \u003cspan class=\"k\"\u003evar\u003c/span\u003e \u003cspan class=\"nv\"\u003ebody\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kd\"\u003esome\u003c/span\u003e \u003cspan class=\"kt\"\u003eView\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \n        \u003cspan class=\"kt\"\u003eVStack\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n            \n            \u003cspan class=\"kt\"\u003eImage\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003esystemName\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e\"swift\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eresizable\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003escaledToFit\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eframe\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003ewidth\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e120\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003eheight\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e120\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eforegroundStyle\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etint\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n            \n            \u003cspan class=\"kt\"\u003eText\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"I love SwiftUI! From MszPro.\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003efont\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elargeTitle\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ebold\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n            \n            \u003cspan class=\"kt\"\u003eToggle\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"Show glass background\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003eisOn\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003e$showGlassBackground\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n            \n            \u003cspan class=\"kt\"\u003eButton\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"Open immersive space\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n                \u003cspan class=\"kt\"\u003eTask\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"kd\"\u003e@MainActor\u003c/span\u003e \u003cspan class=\"k\"\u003ein\u003c/span\u003e\n                    \u003cspan class=\"k\"\u003eawait\u003c/span\u003e \u003cspan class=\"k\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003edismissImmersiveSpace\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n                    \u003cspan class=\"k\"\u003eawait\u003c/span\u003e \u003cspan class=\"k\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003egameModel\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eloadAssets\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n                    \u003cspan class=\"k\"\u003eawait\u003c/span\u003e \u003cspan class=\"k\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eopenImmersiveSpace\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003eid\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e\"robotSpace\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n            \n            \u003cspan class=\"kt\"\u003eButton\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"Add random robot in room\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n                \u003cspan class=\"k\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003egameModel\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eaddRobotAtRandomPosition\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n                \u003cspan class=\"nf\"\u003espeak\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003etext\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e\"ハロー\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n            \n            \u003cspan class=\"kt\"\u003eButton\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"Hide robot\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n                \u003cspan class=\"kt\"\u003eTask\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"kd\"\u003e@MainActor\u003c/span\u003e \u003cspan class=\"k\"\u003ein\u003c/span\u003e\n                    \u003cspan class=\"k\"\u003eawait\u003c/span\u003e \u003cspan class=\"k\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003edismissImmersiveSpace\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n                \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n            \n            \u003cspan class=\"kt\"\u003eButton\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"Inspect robot in a window\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n                \u003cspan class=\"k\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eopenWindow\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003evalue\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eARModelOpenParameter\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003emodelName\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e\"robot_walk_idle.usdz\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003emodelNodeName\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e\"robot_walk_idle\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nv\"\u003einitialScale\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.01\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n            \n        \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n        \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003epadding\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eframe\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003ewidth\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mi\"\u003e700\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eglassBackgroundEffect\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003edisplayMode\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eshowGlassBackground\u003c/span\u003e \u003cspan class=\"p\"\u003e?\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nv\"\u003ealways\u003c/span\u003e \u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enever\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n    \n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"807:1-807:87\"\u003eそして、テキスト入力を読み上げるヘルパー関数はこちらです。\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"swift\" data-sourcepos=\"809:1-822:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"kd\"\u003eimport\u003c/span\u003e \u003cspan class=\"kt\"\u003eSwiftUI\u003c/span\u003e\n\u003cspan class=\"kd\"\u003eimport\u003c/span\u003e \u003cspan class=\"kt\"\u003eAVFoundation\u003c/span\u003e\n\n\u003cspan class=\"kd\"\u003efunc\u003c/span\u003e \u003cspan class=\"nf\"\u003espeak\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003etext\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"kt\"\u003eString\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003eutterance\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kt\"\u003eAVSpeechUtterance\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003estring\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003etext\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eutterance\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003evoice\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kt\"\u003eAVSpeechSynthesisVoice\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nv\"\u003elanguage\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e\"ja-JP\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eutterance\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erate\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.5\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eutterance\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epitchMultiplier\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mf\"\u003e1.5\u003c/span\u003e\n    \n    \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"nv\"\u003esynthesizer\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"kt\"\u003eAVSpeechSynthesizer\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n    \u003cspan class=\"n\"\u003esynthesizer\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003espeak\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eutterance\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"824:1-824:192\"\u003eこれでアプリを実行し、ボタンをタップしてランダムなロボットを約10体追加し、ロボットをタップしてWHAC-A-BOTをプレイすることができます！\u003c/p\u003e\n\u003ch1 data-sourcepos=\"826:1-826:56\"\u003e\n\u003cspan id=\"読んでいただきありがとうございます\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E8%AA%AD%E3%82%93%E3%81%A7%E3%81%84%E3%81%9F%E3%81%A0%E3%81%8D%E3%81%82%E3%82%8A%E3%81%8C%E3%81%A8%E3%81%86%E3%81%94%E3%81%96%E3%81%84%E3%81%BE%E3%81%99\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e読んでいただきありがとうございます！\u003c/h1\u003e\n\u003cp data-sourcepos=\"828:1-828:71\"\u003eEnglish version: \u003ca href=\"https://mszpro.com/blog/blog-article-visionos-dev-101/\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://mszpro.com/blog/blog-article-visionos-dev-101/\u003c/a\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"830:1-832:35\"\u003e私の記事は、アプリクリップで読むことができます：\u003ca href=\"https://appclip.apple.com/id?p=com.ShunzheMa.MszMagic.Clip\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://appclip.apple.com/id?p=com.ShunzheMa.MszMagic.Clip\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://buymeacoffee.com/mszpro\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://buymeacoffee.com/mszpro\u003c/a\u003e\u003cbr\u003e\nTwitter: \u003ca href=\"https://twitter.com/mszpro\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://twitter.com/mszpro\u003c/a\u003e\u003c/p\u003e\n\u003ch3 data-sourcepos=\"834:1-834:33\"\u003e\n\u003cspan id=\"appleコードライセンス\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#apple%E3%82%B3%E3%83%BC%E3%83%89%E3%83%A9%E3%82%A4%E3%82%BB%E3%83%B3%E3%82%B9\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eAppleコードライセンス\u003c/h3\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"836:1-876:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003eCopyright (C) 2024 Apple Inc. All Rights Reserved.\n\nIMPORTANT:  This Apple software is supplied to you by Apple\nInc. (\"Apple\") in consideration of your agreement to the following\nterms, and your use, installation, modification or redistribution of\nthis Apple software constitutes acceptance of these terms.  If you do\nnot agree with these terms, please do not use, install, modify or\nredistribute this Apple software.\n\nIn consideration of your agreement to abide by the following terms, and\nsubject to these terms, Apple grants you a personal, non-exclusive\nlicense, under Apple's copyrights in this original Apple software (the\n\"Apple Software\"), to use, reproduce, modify and redistribute the Apple\nSoftware, with or without modifications, in source and/or binary forms;\nprovided that if you redistribute the Apple Software in its entirety and\nwithout modifications, you must retain this notice and the following\ntext and disclaimers in all such redistributions of the Apple Software.\nNeither the name, trademarks, service marks or logos of Apple Inc. may\nbe used to endorse or promote products derived from the Apple Software\nwithout specific prior written permission from Apple.  Except as\nexpressly stated in this notice, no other rights or licenses, express or\nimplied, are granted by Apple herein, including but not limited to any\npatent rights that may be infringed by your derivative works or by other\nworks in which the Apple Software may be incorporated.\n\nThe Apple Software is provided by Apple on an \"AS IS\" basis.  APPLE\nMAKES NO WARRANTIES, EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION\nTHE IMPLIED WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY AND FITNESS\nFOR A PARTICULAR PURPOSE, REGARDING THE APPLE SOFTWARE OR ITS USE AND\nOPERATION ALONE OR IN COMBINATION WITH YOUR PRODUCTS.\n\nIN NO EVENT SHALL APPLE BE LIABLE FOR ANY SPECIAL, INDIRECT, INCIDENTAL\nOR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\nSUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) ARISING IN ANY WAY OUT OF THE USE, REPRODUCTION,\nMODIFICATION AND/OR DISTRIBUTION OF THE APPLE SOFTWARE, HOWEVER CAUSED\nAND WHETHER UNDER THEORY OF CONTRACT, TORT (INCLUDING NEGLIGENCE),\nSTRICT LIABILITY OR OTHERWISE, EVEN IF APPLE HAS BEEN ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGE.\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n",
    "body": "## この記事の内容\n\n![qiita-thumbnail.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/4ae540b9-7537-70ac-bd26-c62ecb6d2b39.png)\n\n- アプリにVisionOSターゲットを追加\n- ビューの背景の透明度を切り替える方法\n- VisionOSアプリ用のコードを条件付きで実行する方法\n- 3Dアセット（.usdzファイル）を読み込む方法\n- 没入空間を開き、2Dビューとともに3Dアセットを表示する方法\n- 新しいウィンドウを開いて3Dアセットを表示し、ユーザーがモデルを回転・ズームできるようにする方法\n- 3Dアセットの位置やスケールを変更する方法\n- 3Dアセットにアニメーションを適用する方法\n- 没入空間を閉じる方法\n- オブジェクトを選択可能にする方法\n\nこの記事を読めば、部屋の中でランダムな場所に現れるロボットをタップして隠すミニゲームの作り方を学べます。\n\n\u003cimg src=\"https://article-images.mszpro.com/rack-a-bot-game-compressed.gif\"\u003e\n\n\nGithubリポジトリ: https://github.com/mszpro/VisionOS101\nこの記事は **英語で** 録画されたYouTube動画もあります:https://www.youtube.com/watch?v=6gDu80Jbnwo\n\n## 私のVisionOSアプリとゲーム\n\nこれまでに3つのVisionOSアプリとゲームを個人で開発・公開しました。\n\n| Spatial Dream | Spatial Boxer | SoraSNS |\n|---|---|---|\n| 部屋の中で楽しめる小さなパズルゲーム。 | 音楽に合わせてボックスをパンチし、爆弾を避けるリズムゲーム。 | Mastodon、Bluesky、Misskey、Firefish向けのFediverseアプリ。 |\n|![image.jpeg](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/d548559a-cacc-74be-8af8-900dfffafd17.jpeg)|![image.jpeg](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/6e0aaee9-e1ed-2508-1923-74c849981590.jpeg)|![image.jpeg](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/1b0a8d6c-2480-7c63-947f-f1083be58efe.jpeg)|\n\nさあ、始めましょう！\n\n## スタート\n\n通常のiOS向けSwiftUIアプリケーションから始めます：\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/800a9bc8-4762-100d-8276-d4fc68c53700.png)\n\n上記のようなファイル構造と、\"Hello World\" というテキストを表示する基本的なSwiftUIビューが用意されています。\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/73eda0a8-db54-d454-cd7f-22ee0748fb68.png)\n\n次に、メインターゲットのプロジェクト設定に切り替えます。アプリがすでにApple Visionに対応していることが表示されていますが、\"Designed for iPad\" と書かれています。\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/c6825e2e-183a-3516-d464-9288cbf5dc7d.png)\n\nこれは、このアプリがVisionOSでiPadと同じように動作することを意味しており、3D空間のアセットをサポートせず、背景が美しくない状態です。\n例えば、ターゲットをApple Vision Proに切り替えて実行ボタンを押すと、今アプリをシミュレーターで実行することができます。\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/59fccd64-41da-146c-4518-52a027d070af.png)\n\nビューには白い非透明な背景があり、ウィンドウのサイズを変更することが全くできません。\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/d020fb7c-a4e5-ed49-f3d6-707ee2ab009b.png)\n\nサポートされている実行先のリストで、Apple Vision Pro（Designed for iPad）を削除し、Apple Visionを追加します。\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/99e677a9-813b-f1d0-2677-cff885e25f3d.png)\n\nもう一度シミュレーターでアプリを実行すると、ネイティブのApple Vision Proアプリになったことが確認できます。\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/06316197-f9e6-3e2e-33de-f81856942fc9.png)\n\n:::note info\n💡 既存のiOSアプリに上記の変更を適用すると、コンパイル警告が発生する可能性があります。例えば、使用しているサードパーティのパッケージやライブラリがApple Vision OSに対応していない場合や、一部のコードがVision OSで利用できない場合があります。その場合、Vision OS用に別のコードブランチを実行するために条件付きチェックを使用する必要があります。\n:::\n\n## 背景のガラスモーフィズム効果を制御する\n\nデフォルトでは、ガラスモーフィズム（ガラスのように、ウィンドウの後ろが見える効果）の背景が適用されています。これを制御することができます。\n\nまず、アプリケーションウィンドウが2Dコンテンツのみを表示する場合、メインのSwiftUIアプリでwindowStyleを.plainに設定することができます：\n\n```swift\nimport SwiftUI\n\n@main\nstruct VisionOSDemoApp: App {\n    var body: some Scene {\n        WindowGroup {\n            ContentView()\n        }\n        .windowStyle(.plain)\n    }\n}\n```\n\n次に、ContentViewでは背景のガラス効果を表示しないように設定します：\n\n```swift\nstruct ContentView: View {\n    \n    var body: some View {\n        \n        VStack {\n            \n            Image(systemName: \"swift\")\n                .resizable()\n                .scaledToFit()\n                .frame(width: 120, height: 120)\n                .foregroundStyle(.tint)\n            \n            Text(\"I love SwiftUI! From MszPro.\")\n                .font(.largeTitle)\n                .bold()\n            \n        }\n        .padding()\n        .glassBackgroundEffect(displayMode: .never)\n        \n    }\n    \n}\n```\n\nすると、次のようになります：\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/6034244e-b50a-a3b2-0d4f-e8002b051d05.png)\n\n背景のガラス効果を動的に制御することも可能です。例えば、`@State`変数とトグルを使用して、以下のように制御できます：\n\n```swift\nstruct ContentView: View {\n    \n    @State private var showGlassBackground: Bool = true\n    \n    var body: some View {\n        \n        VStack {\n            \n            Image(systemName: \"swift\")\n                .resizable()\n                .scaledToFit()\n                .frame(width: 120, height: 120)\n                .foregroundStyle(.tint)\n            \n            Text(\"I love SwiftUI! From MszPro.\")\n                .font(.largeTitle)\n                .bold()\n            \n            Toggle(\"Show glass background\", isOn: $showGlassBackground)\n            \n        }\n        .padding()\n        .frame(width: 700)\n        .glassBackgroundEffect(displayMode: showGlassBackground ? .always : .never)\n        \n    }\n    \n}\n```\n\n\u003cimg src=\"https://article-images.mszpro.com/vision-pro-toggle-glass-effect-compressed.gif\"\u003e\n\n## 3Dアセットバンドルの作成\n\nアプリに3Dアセットを埋め込むには、3Dアセットファイル（通常は.usdz）を含む特別なSwiftパッケージを作成する必要があります。\nまず、Reality Composer Proアプリケーションを開きます：\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/a82c954a-00bc-0799-e79d-11c57fd0b118.png)\n\n「Create new project」をクリックし、Xcodeプロジェクトと同じフォルダ内にプロジェクトを作成します。\n新しく作成されたプロジェクトフォルダを見ると、それが実際にはSwiftパッケージであることがわかります：\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/e8826d92-985d-0bac-e1fc-9990a17ce73c.png)\n\n次に、3Dアセットを追加します。Apple Developerのウェブサイトにあるサンプル3Dモデルの中から1つを使用できます。ここで取得できます: https://developer.apple.com/augmented-reality/quick-look/\nアニメーションについても説明するので、アニメーションが含まれているモデルを選んでください。例えば、ロボット（この記事のチュートリアルに従うためには、このロボットを使用する必要があります）。右クリックして.usdzファイルをダウンロードします。\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/25233dc0-3df3-459f-ed8d-f5e0024ea26e.png)\n\nダウンロードしたファイルをReality Composer Proのプロジェクトブラウザにドラッグします。右側のパネルでアニメーション付きのモデルが再生されるのが確認できます：\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/bb0cadb6-f263-a277-265b-79660bbb120d.png)\n\n次に、Xcodeのプロジェクトで、このReality Composer ProプロジェクトをSwiftパッケージのようにインポートします：\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/870a78aa-585a-4e0e-391d-40193a9e4768.png)\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/05c59b3a-0f9c-ee24-fa67-706ca62d3158.png)\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/546c7f0c-1d5b-f793-582c-9e3520de5f9f.png)\n\nアセットをバンドルから読み込む関数\n次に、Reality Composer Proバンドルから特定のアセットを読み込むための関数を作成します：\n\n```swift\n#if os(visionOS)\n\nimport Foundation\nimport RealityKit\nimport DemoAssets\n\n@MainActor\nfunc loadFromRealityComposerProject(nodeName: String, sceneFileName: String) async -\u003e Entity? {\n    var entity: Entity? = nil\n    do {\n        let scene = try await Entity(named: sceneFileName,\n                                     in: demoAssetsBundle)\n        entity = scene.findEntity(named: nodeName)\n    } catch {\n        print(\"Failed to load asset named \\(nodeName) within the file \\(sceneFileName)\")\n    }\n    return entity\n}\n\n#endif\n```\n\nこのコードがVisionOS向けにビルドされている場合のみコンパイルされるように、`if os(visionOS)` チェックを使用しています。\nこの関数の入力では、ノードの名前（任意）とファイルの名前を受け取ります。\nノードの名前は任意です。上記のコードでは特定のエンティティ名を指定しない場合、ルートノードが使用されます。ただし、必要に応じて、すべての子ノードを簡単にリストアップすることができます。\nまず、.usdzファイルを開き、SceneKitにエクスポートして、ファイルに含まれるノードのリストを確認します：\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/46c7395a-c5ad-413d-548e-14aa115045d5.png)\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/f33b17f6-4c04-0141-6230-4e59cd572791.png)\n\n上記の例では、`robot_walk_idle` がルート（トップレベル）エンティティの名前です。\n\nこれは、単一の.usdzファイル内に多くのアセットが含まれていて、そのうちの1つだけを読み込みたい場合に便利です。\n\n:::note info\n💡 .usdzファイルはエンドユーザーに提供される読み取り専用の形式と考えてください。一方、.scnファイルは開発者や3Dデザイナーが3Dファイルの内容を編集するために使用する編集用形式です。.usdzはいつでも.scnに変換でき、暗号化されていません。\n:::\n\n## 没入空間 (Immersive Space) の作成\n\nユーザーの部屋にコンテンツを表示するには、没入空間を作成する必要があります。これは、アプリケーションウィンドウ内に3Dコンテンツを表示するのとは異なります。この記事では、これらの両方を説明します。\n\n`ContentSpace.swift` という新しいファイルを作成します。このファイルでは、RealityViewを使用して3Dアセットを読み込み、設定し、それをビューに追加します：\n\n```swift\n#if os(visionOS)\n\nimport SwiftUI\nimport RealityKit\n\nstruct ContentSpace: View {\n    \n    @State private var loaded3DAsset: Entity? = nil\n    \n    var body: some View {\n        \n        RealityView { content in\n            \n            loaded3DAsset = await loadFromRealityComposerProject(nodeName: \"robot_walk_idle\",\n                                                                 sceneFileName: \"robot_walk_idle.usdz\")\n            loaded3DAsset?.scale = .init(x: 0.1, y: 0.1, z: 0.1)\n            loaded3DAsset?.position = .init(x: 0, y: 0, z: -3)\n            \n            // TODO: Collision\n            // TODO: Allow user to tap on it\n            // TODO: add lighting\n            \n            guard let loaded3DAsset else {\n                return\n            }\n            \n            content.add(loaded3DAsset)\n            \n        }\n        \n    }\n    \n}\n```\n\n:::note info\nオブジェクトに適切なスケールを設定することを忘れないでください。オブジェクトのサイズ（メートル単位）を最初に確認し、それに基づいて計算することができます。オブジェクトが見えない場合、通常それは大きすぎるか小さすぎることが原因です。\n:::\n\n次に、メインアプリのコードに移り、上記のビューを没入空間として宣言します。また、スペースにIDを付与することで、アプリのどこからでもそれを呼び出すことができます。呼び出すと、それはユーザーの部屋に表示されます。\n\n```swift\nimport SwiftUI\n\n@main\nstruct VisionOSDemoApp: App {\n    \n    var body: some Scene {\n        \n        WindowGroup {\n            ContentView()\n        }\n        .windowStyle(.plain)\n        \n        ImmersiveSpace(id: \"robotSpace\") {\n            ContentSpace()\n        }\n        \n    }\n    \n}\n```\n\n没入空間を開くか、現在表示されている没入空間を閉じるためには、SwiftUIの環境変数を使用します。\n\n```swift\n@Environment(\\.openImmersiveSpace) private var openImmersiveSpace\n\n@Environment(\\.dismissImmersiveSpace) private var dismissImmersiveSpace\n```\n\n一度に表示できる没入空間は1つだけです。新しいものを表示するには、現在のものを閉じるようにしてください（もし表示されている場合）。\n\n次に、上記のrobotSpaceを2Dビュー内で表示するボタンを追加します。\n\n\u003cimg src=\"https://article-images.mszpro.com/show-robot-compressed.gif\"\u003e\n\n\nもし大きすぎる場合は、.scaleファクターを調整することができます。\n\n## 座標空間についての説明\n\n`loaded3DAsset?.position = .init(x: 0, y: 0, z: -3)` を使用して、3Dアセットの初期位置を設定していることに気づいたかもしれません。\n- X軸は左右を意味します。\n- Y軸は上下（天井や床）を意味します。\n- Z軸はユーザーに対して近いか遠いかを意味します。負の値であればユーザーの前にあり、正の値であればユーザーの後ろにあることを示します。\n\n## アニメーションの追加\n\nもし3Dモデルにアニメーションが付属している場合、そのアニメーションを再生することができます：\n\n```swift\nRealityView { content in\n            \n            // ...\n            \n            guard let loaded3DAsset else {\n                return\n            }\n            \n            // animation\n            if let firstAnimation = loaded3DAsset.availableAnimations.first {\n                loaded3DAsset.playAnimation(firstAnimation.repeat(),\n                                           transitionDuration: 0,\n                                           startsPaused: false)\n            }\n            \n            // TODO: Collision\n            // TODO: Allow user to tap on it\n            // TODO: add lighting\n            \n            content.add(loaded3DAsset)\n            \n        }\n```\n\n\nこれで、ロボットが歩いているのが見えるはずです。\n\n\u003cimg src=\"https://article-images.mszpro.com/walking-robot-compressed.gif\"\u003e\n\n## インタラクティブなロボットモデル用の新しいウィンドウを開く\n\nロボットをユーザーの空間ではなく新しいウィンドウで表示し、ユーザーがピンチ操作でロボットをスケールしたり、手を使って回転させたりしてさまざまな角度からロボットを見ることができるようにすることもできます。\n\nまず、どのARモデルを表示するかをアプリに指示するデータ構造を定義する必要があります。\n\n```swift\nstruct ARModelOpenParameter: Identifiable, Hashable, Codable {\n    var id: String {\n        return \"\\(modelName)-\\(modelNodeName)\"\n    }\n    var modelName: String\n    var modelNodeName: String\n    var initialScale: Float\n}\n```\n\n次に、ユーザーがドラッグしてモデルを回転できるようにするため、以下のヘルパー関数をコード内に追加します。以下のコードは、Apple Developerのドキュメントにあるサンプルプロジェクトから引用しています。\n\n```swift\n/*\nSee the LICENSE.txt file for this sample’s licensing information.\n\nAbstract:\nA modifier for turning drag gestures into rotation.\n*/\n\nimport SwiftUI\nimport RealityKit\n\nextension View {\n    /// Enables people to drag an entity to rotate it, with optional limitations\n    /// on the rotation in yaw and pitch.\n    func dragRotation(\n        yawLimit: Angle? = nil,\n        pitchLimit: Angle? = nil,\n        sensitivity: Double = 10,\n        axRotateClockwise: Bool = false,\n        axRotateCounterClockwise: Bool = false\n    ) -\u003e some View {\n        self.modifier(\n            DragRotationModifier(\n                yawLimit: yawLimit,\n                pitchLimit: pitchLimit,\n                sensitivity: sensitivity,\n                axRotateClockwise: axRotateClockwise,\n                axRotateCounterClockwise: axRotateCounterClockwise\n            )\n        )\n    }\n}\n\n/// A modifier converts drag gestures into entity rotation.\nprivate struct DragRotationModifier: ViewModifier {\n    var yawLimit: Angle?\n    var pitchLimit: Angle?\n    var sensitivity: Double\n    var axRotateClockwise: Bool\n    var axRotateCounterClockwise: Bool\n\n    @State private var baseYaw: Double = 0\n    @State private var yaw: Double = 0\n    @State private var basePitch: Double = 0\n    @State private var pitch: Double = 0\n\n    func body(content: Content) -\u003e some View {\n        content\n            .rotation3DEffect(.radians(yaw == 0 ? 0.01 : yaw), axis: .y)\n            .rotation3DEffect(.radians(pitch == 0 ? 0.01 : pitch), axis: .x)\n            .gesture(DragGesture(minimumDistance: 0.0)\n                .targetedToAnyEntity()\n                .onChanged { value in\n                    // Find the current linear displacement.\n                    let location3D = value.convert(value.location3D, from: .local, to: .scene)\n                    let startLocation3D = value.convert(value.startLocation3D, from: .local, to: .scene)\n                    let delta = location3D - startLocation3D\n\n                    // Use an interactive spring animation that becomes\n                    // a spring animation when the gesture ends below.\n                    withAnimation(.interactiveSpring) {\n                        yaw = spin(displacement: Double(delta.x), base: baseYaw, limit: yawLimit)\n                        pitch = spin(displacement: Double(delta.y), base: basePitch, limit: pitchLimit)\n                    }\n                }\n                .onEnded { value in\n                    // Find the current and predicted final linear displacements.\n                    let location3D = value.convert(value.location3D, from: .local, to: .scene)\n                    let startLocation3D = value.convert(value.startLocation3D, from: .local, to: .scene)\n                    let predictedEndLocation3D = value.convert(value.predictedEndLocation3D, from: .local, to: .scene)\n                    let delta = location3D - startLocation3D\n                    let predictedDelta = predictedEndLocation3D - location3D\n\n                    // Set the final spin value using a spring animation.\n                    withAnimation(.spring) {\n                        yaw = finalSpin(\n                            displacement: Double(delta.x),\n                            predictedDisplacement: Double(predictedDelta.x),\n                            base: baseYaw,\n                            limit: yawLimit)\n                        pitch = finalSpin(\n                            displacement: Double(delta.y),\n                            predictedDisplacement: Double(predictedDelta.y),\n                            base: basePitch,\n                            limit: pitchLimit)\n                    }\n\n                    // Store the last value for use by the next gesture.\n                    baseYaw = yaw\n                    basePitch = pitch\n                }\n            )\n            .onChange(of: axRotateClockwise) {\n                withAnimation(.spring) {\n                    yaw -= (.pi / 6)\n                    baseYaw = yaw\n                }\n            }\n            .onChange(of: axRotateCounterClockwise) {\n                withAnimation(.spring) {\n                    yaw += (.pi / 6)\n                    baseYaw = yaw\n                }\n            }\n    }\n\n    /// Finds the spin for the specified linear displacement, subject to an\n    /// optional limit.\n    private func spin(\n        displacement: Double,\n        base: Double,\n        limit: Angle?\n    ) -\u003e Double {\n        if let limit {\n            return atan(displacement * sensitivity) * (limit.degrees / 90)\n        } else {\n            return base + displacement * sensitivity\n        }\n    }\n\n    /// Finds the final spin given the current and predicted final linear\n    /// displacements, or zero when the spin is restricted.\n    private func finalSpin(\n        displacement: Double,\n        predictedDisplacement: Double,\n        base: Double,\n        limit: Angle?\n    ) -\u003e Double {\n        // If there is a spin limit, always return to zero spin at the end.\n        guard limit == nil else { return 0 }\n\n        // Find the projected final linear displacement, capped at 1 more revolution.\n        let cap = .pi * 2.0 / sensitivity\n        let delta = displacement + max(-cap, min(cap, predictedDisplacement))\n\n        // Find the final spin.\n        return base + delta * sensitivity\n    }\n}\n```\n\nその後、メインのSwiftUIアプリコードを更新します。\n\n```swift\n@main\nstruct VisionOSDemoApp: App {\n    \n    @Environment(\\.dismissWindow) private var dismissWindow\n    \n    var body: some SwiftUI.Scene {\n        \n        WindowGroup {\n            ContentView()\n        }\n        .windowStyle(.plain)\n        \n        ImmersiveSpace(id: \"robotSpace\") {\n            ContentSpace()\n        }\n        \n        WindowGroup(for: ARModelOpenParameter.self) { $object in\n            // 3D view\n            if let object {\n                RealityView { content in\n                    guard let arAsset = await loadFromRealityComposerProject(\n                        nodeName: object.modelNodeName,\n                        sceneFileName: object.modelName\n                    ) else {\n                        fatalError(\"Unable to load beam from Reality Composer Pro project.\")\n                    }\n                    arAsset.generateCollisionShapes(recursive: true)\n                    arAsset.position = .init(x: 0, y: 0, z: 0)\n                    arAsset.scale = .init(x: object.initialScale,\n                                          y: object.initialScale,\n                                          z: object.initialScale)\n                    arAsset.components[InputTargetComponent.self] = InputTargetComponent(allowedInputTypes: .all)\n                    content.add(arAsset)\n                }\n                .dragRotation()\n                .frame(width: 900, height: 900)\n                .glassBackgroundEffect(displayMode: .always)\n            }\n        }\n        .windowStyle(.volumetric)\n        .defaultSize(width: 0.5, height: 0.5, depth: 0.5, in: .meters)\n        \n    }\n    \n}\n```\n\nここでの変更点は以下の通りです：\n\n- ビューのタイプを`Scene`から`SwiftUI.Scene`に変更します。これは、同じ名前がRealityKitのシステムフレームワーク内でも使われているためです。\n- `WindowGroup`を新しく追加し、`ARModelOpenParameter`で提供されたデータに基づいて開くようにします。新しいウィンドウ内では、ARオブジェクトが表示され、そのオブジェクトを回転させることができます。\n- そのウィンドウを表示するには、`openWindow`環境変数を使用し、`ARModelOpenParameter`を構築します。\n\n```\n@Environment(\\.openWindow) private var openWindow\n\nButton(\"Inspect robot in a window\") {\n    self.openWindow(value: ARModelOpenParameter(modelName: \"robot_walk_idle.usdz\", modelNodeName: \"robot_walk_idle\", initialScale: 0.01))\n}\n```\n\nこれで、このボタンをタップして、回転可能な3Dモデルを検査するための別のウィンドウを表示できるようになります。\n\n\u003cimg src=\"https://article-images.mszpro.com/inspect-3d-model.gif\"\u003e\n\n## システム3Dビューアを使用する\n\n上記の方法では、モデルのプレビューウィンドウを自分でデザインする方法を示しましたが、VisionOSシステムが提供するモデルプレビューを直接使用することもできます：\n\n```swift\nModel3D(named: \"Robot-Drummer\") { model in\n    model\n        .resizable()\n        .aspectRatio(contentMode: .fit)\n} placeholder: {\n    ProgressView()\n}\n```\n\n作成したパッケージ内のモデルを使用することも可能です：\n\n```swift\nModel3D(named: \"robot_walk_idle.usdz\", bundle: demoAssetsBundle)\n    .padding(.bottom, 50)\n```\n\nhttps://developer.apple.com/documentation/realitykit/model3d/\n\n## モデルをタップできるようにする\n\nVisionOSでは、ユーザーは目で見て、指をピンチすることで何かを選択できます。この機能をシーンに追加した3Dオブジェクトに適用できます。\n\n:::note info\n💡注意: これは、手で3Dモデルに触れるのとは異なります。手の追跡は複雑なため、この機能については別の記事で説明します。\n:::\n\n読み込んだ3Dエンティティに以下の行を追加することで、ユーザーがそれをタップできるようにすることができます：\n\n```swift\narAsset.components[InputTargetComponent.self] = InputTargetComponent(allowedInputTypes: .all)\n```\n\nタップイベントを受け取るには、`RealityView` を使用している3D空間ビューに次のジェスチャーを追加する必要があります。ユーザーがタップしたノードは `tappedNode` として取得できますが、これは追加したノードの子ノードである可能性があることに注意してください。   \n\n```swift\n.gesture(TapGesture()\n        .targetedToAnyEntity()\n        .onEnded({ tap in\n            let tappedNode = tap.entity\n            \n        }))\n```\n\nそのため、ユーザーが探しているノードをタップしたかどうかを確認するために、親ノードを再帰的にチェックすることを忘れないでください。ノードの名前を識別子として使用することもできます。\n\n## Whac-A-Robot!\n\n\u003cimg src=\"https://article-images.mszpro.com/rack-a-bot-game-compressed.gif\"\u003e\n\n前のステップでは、読み込んだ3DモデルをContentSpaceに保存しました。アプリ内のどこからでもそのアセットにアクセスできるように、より柔軟にするために、それをモデルに保存することができます：\n\n```swift\nimport SwiftUI\nimport RealityKit\n\nlet contentSpaceOrigin = Entity()\n\n@Observable\nclass ContentModel {\n    \n    var loaded3DAsset: Entity? = nil\n    \n    @MainActor\n    func loadAssets() async {\n        loaded3DAsset = await loadFromRealityComposerProject(nodeName: \"robot_walk_idle\",\n                                                             sceneFileName: \"robot_walk_idle.usdz\")\n        loaded3DAsset?.name = \"robot_root_node\"\n        loaded3DAsset?.scale = .init(x: 0.05, y: 0.05, z: 0.05)\n        loaded3DAsset?.position = .init(x: 0, y: 0, z: -3)\n        \n        guard let loaded3DAsset else {\n            return\n        }\n        \n        // animation\n        if let firstAnimation = loaded3DAsset.availableAnimations.first {\n            loaded3DAsset.playAnimation(firstAnimation.repeat(),\n                                        transitionDuration: 0,\n                                        startsPaused: false)\n        }\n        \n        // allow tap\n        loaded3DAsset.generateCollisionShapes(recursive: true)\n        loaded3DAsset.components[InputTargetComponent.self] = InputTargetComponent(allowedInputTypes: .all)\n        \n        contentSpaceOrigin.addChild(loaded3DAsset)\n    }\n    \n    func addRobotAtRandomPosition() {\n        guard let loaded3DAsset else { return }\n        // clone the already loaded robot node\n        let newRobot = loaded3DAsset.clone(recursive: true)\n        newRobot.name = \"robot_root_node\"\n        newRobot.position = .init(x: Float.random(in: -2...2),\n                                  y: Float.random(in: 0...2),\n                                  z: Float.random(in: (-3)...(-2)))\n        newRobot.scale = .init(x: 0.03, y: 0.03, z: 0.03)\n        if let firstAnimation = newRobot.availableAnimations.first {\n            newRobot.playAnimation(firstAnimation.repeat(),\n                                        transitionDuration: 0,\n                                        startsPaused: false)\n        }\n        newRobot.generateCollisionShapes(recursive: true)\n        newRobot.components[InputTargetComponent.self] = InputTargetComponent(allowedInputTypes: .all)\n        contentSpaceOrigin.addChild(newRobot)\n    }\n    \n}\n```\n\nここでは、アセットをメモリに読み込む`loadAssets`関数があります。この関数は、没入空間が表示される前に呼び出されます。また、ゲーム内でロボットをランダムな場所に追加するために使用される`addRobotAtRandomPosition`関数もあります。\n\n上記のコードに見られるように、没入ビューが表示される前にアセットを読み込む必要があります。しかし、没入ビューが表示される前には、アセットを追加するためのワールドルートノード（部屋の環境のノード）が存在しません。そのため、すべての読み込んだアセットを追加するためのグローバル変数`contentSpaceOrigin`を作成し、没入空間が表示されたときに、`contentSpaceOrigin`をワールドノードの子として追加します。\n\n\n```swift\nimport SwiftUI\nimport RealityKit\n\nstruct ContentSpace: View {\n    \n    @Environment(ContentModel.self) var gameModel\n    \n    var body: some View {\n        \n        RealityView { content in\n            content.add(contentSpaceOrigin)\n        }\n        .gesture(TapGesture()\n            .targetedToAnyEntity()\n            .onEnded({ tap in\n                let tappedNode = tap.entity\n                // look up until it reaches the robot main node\n                var foundRobotMainNode: Entity? = tappedNode\n                while foundRobotMainNode != nil \u0026\u0026\n                        foundRobotMainNode?.parent != nil {\n                    if foundRobotMainNode?.name == \"robot_root_node\" {\n                        break // we found it!\n                    } else {\n                        foundRobotMainNode = foundRobotMainNode?.parent\n                    }\n                }\n                foundRobotMainNode?.removeFromParent()\n                speak(text: \"まだね\")\n            }))\n        \n    }\n    \n}\n```\n\n上記のコードでは、タップジェスチャーの検出を設定しています。タップジェスチャーがあるたびに、アプリはタップされたロボットを削除します。\n次に、2Dビューにランダムなロボットを追加するボタンを配置します。\n\n```swift\nimport SwiftUI\nimport RealityKit\n\nstruct ContentView: View {\n    \n    @State private var showGlassBackground: Bool = true\n    \n    @Environment(ContentModel.self) var gameModel\n    @Environment(\\.openImmersiveSpace) private var openImmersiveSpace\n    @Environment(\\.dismissImmersiveSpace) private var dismissImmersiveSpace\n    \n    @Environment(\\.openWindow) private var openWindow\n    \n    var body: some View {\n        \n        VStack {\n            \n            Image(systemName: \"swift\")\n                .resizable()\n                .scaledToFit()\n                .frame(width: 120, height: 120)\n                .foregroundStyle(.tint)\n            \n            Text(\"I love SwiftUI! From MszPro.\")\n                .font(.largeTitle)\n                .bold()\n            \n            Toggle(\"Show glass background\", isOn: $showGlassBackground)\n            \n            Button(\"Open immersive space\") {\n                Task { @MainActor in\n                    await self.dismissImmersiveSpace()\n                    await self.gameModel.loadAssets()\n                    await self.openImmersiveSpace(id: \"robotSpace\")\n                }\n            }\n            \n            Button(\"Add random robot in room\") {\n                self.gameModel.addRobotAtRandomPosition()\n                speak(text: \"ハロー\")\n            }\n            \n            Button(\"Hide robot\") {\n                Task { @MainActor in\n                    await self.dismissImmersiveSpace()\n                }\n            }\n            \n            Button(\"Inspect robot in a window\") {\n                self.openWindow(value: ARModelOpenParameter(modelName: \"robot_walk_idle.usdz\", modelNodeName: \"robot_walk_idle\", initialScale: 0.01))\n            }\n            \n        }\n        .padding()\n        .frame(width: 700)\n        .glassBackgroundEffect(displayMode: showGlassBackground ? .always : .never)\n        \n    }\n    \n}\n```\n\nそして、テキスト入力を読み上げるヘルパー関数はこちらです。\n\n```swift\nimport SwiftUI\nimport AVFoundation\n\nfunc speak(text: String) {\n    let utterance = AVSpeechUtterance(string: text)\n    utterance.voice = AVSpeechSynthesisVoice(language: \"ja-JP\")\n    utterance.rate = 0.5\n    utterance.pitchMultiplier = 1.5\n    \n    let synthesizer = AVSpeechSynthesizer()\n    synthesizer.speak(utterance)\n}\n```\n\nこれでアプリを実行し、ボタンをタップしてランダムなロボットを約10体追加し、ロボットをタップしてWHAC-A-BOTをプレイすることができます！\n\n# 読んでいただきありがとうございます！\n\nEnglish version: https://mszpro.com/blog/blog-article-visionos-dev-101/\n\n私の記事は、アプリクリップで読むことができます：https://appclip.apple.com/id?p=com.ShunzheMa.MszMagic.Clip\nhttps://buymeacoffee.com/mszpro\nTwitter: https://twitter.com/mszpro\n\n### Appleコードライセンス\n\n```\nCopyright (C) 2024 Apple Inc. All Rights Reserved.\n\nIMPORTANT:  This Apple software is supplied to you by Apple\nInc. (\"Apple\") in consideration of your agreement to the following\nterms, and your use, installation, modification or redistribution of\nthis Apple software constitutes acceptance of these terms.  If you do\nnot agree with these terms, please do not use, install, modify or\nredistribute this Apple software.\n\nIn consideration of your agreement to abide by the following terms, and\nsubject to these terms, Apple grants you a personal, non-exclusive\nlicense, under Apple's copyrights in this original Apple software (the\n\"Apple Software\"), to use, reproduce, modify and redistribute the Apple\nSoftware, with or without modifications, in source and/or binary forms;\nprovided that if you redistribute the Apple Software in its entirety and\nwithout modifications, you must retain this notice and the following\ntext and disclaimers in all such redistributions of the Apple Software.\nNeither the name, trademarks, service marks or logos of Apple Inc. may\nbe used to endorse or promote products derived from the Apple Software\nwithout specific prior written permission from Apple.  Except as\nexpressly stated in this notice, no other rights or licenses, express or\nimplied, are granted by Apple herein, including but not limited to any\npatent rights that may be infringed by your derivative works or by other\nworks in which the Apple Software may be incorporated.\n\nThe Apple Software is provided by Apple on an \"AS IS\" basis.  APPLE\nMAKES NO WARRANTIES, EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION\nTHE IMPLIED WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY AND FITNESS\nFOR A PARTICULAR PURPOSE, REGARDING THE APPLE SOFTWARE OR ITS USE AND\nOPERATION ALONE OR IN COMBINATION WITH YOUR PRODUCTS.\n\nIN NO EVENT SHALL APPLE BE LIABLE FOR ANY SPECIAL, INDIRECT, INCIDENTAL\nOR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\nSUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) ARISING IN ANY WAY OUT OF THE USE, REPRODUCTION,\nMODIFICATION AND/OR DISTRIBUTION OF THE APPLE SOFTWARE, HOWEVER CAUSED\nAND WHETHER UNDER THEORY OF CONTRACT, TORT (INCLUDING NEGLIGENCE),\nSTRICT LIABILITY OR OTHERWISE, EVEN IF APPLE HAS BEEN ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGE.\n```\n",
    "coediting": false,
    "comments_count": 0,
    "created_at": "2024-08-22T12:19:16+09:00",
    "group": null,
    "id": "1ed5f909f2bdd2b1d4c2",
    "likes_count": 0,
    "private": false,
    "reactions_count": 0,
    "stocks_count": 0,
    "tags": [
      {
        "name": "SwiftUI",
        "versions": []
      },
      {
        "name": "realitykit",
        "versions": []
      },
      {
        "name": "visionOS",
        "versions": []
      },
      {
        "name": "VisionPro",
        "versions": []
      }
    ],
    "title": "VisionOSアプリゲーム開発入門 (2Dビュー、没入空間ImmersiveSpace、タップ可能な3Dオブジェクト、WindowGroup)",
    "updated_at": "2024-08-22T13:04:17+09:00",
    "url": "https://qiita.com/mashunzhe/items/1ed5f909f2bdd2b1d4c2",
    "user": {
      "description": "、😎ソフトウェアエンジニア、#Swift #iOS, 👨🏼‍💻 #WWDC Scholarship 17/18、U^ェ^U プログラミングの力で、一緒に、世界をより良くする。\r\n📎https://MszPro.com\r\n✨https://twitter.com/MszPro\r\n🐘https://sns.mszpro.com/ ✌️ @me@mszpro.com",
      "facebook_id": "",
      "followees_count": 192,
      "followers_count": 169,
      "github_login_name": "mszpro",
      "id": "mashunzhe",
      "items_count": 98,
      "linkedin_id": "",
      "location": "",
      "name": "シュンジョーァ",
      "organization": "",
      "permanent_id": 635330,
      "profile_image_url": "https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/635330/profile-images/1675843155",
      "team_only": false,
      "twitter_screen_name": "MszPro",
      "website_url": "https://mszpro.com"
    },
    "page_views_count": null,
    "team_membership": null,
    "organization_url_name": "SmartSofuto",
    "slide": false
  },
  {
    "rendered_body": "\u003ch2 data-sourcepos=\"1:1-1:18\"\u003e\n\u003cspan id=\"アジェンダ\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%82%A2%E3%82%B8%E3%82%A7%E3%83%B3%E3%83%80\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eアジェンダ\u003c/h2\u003e\n\u003cp data-sourcepos=\"2:1-2:97\"\u003emapメソッドとforEachメソッドの基本的な違いについて簡単にまとめます。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"4:1-4:19\"\u003e\n\u003cspan id=\"違い共通点\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E9%81%95%E3%81%84%E5%85%B1%E9%80%9A%E7%82%B9\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e違い/共通点\u003c/h2\u003e\n\u003ch3 data-sourcepos=\"5:1-5:10\"\u003e\n\u003cspan id=\"違い\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E9%81%95%E3%81%84\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e違い\u003c/h3\u003e\n\u003cdiv data-sourcepos=\"6:1-9:3\" class=\"note info\"\u003e\n\u003cspan class=\"fa fa-fw fa-check-circle\"\u003e\u003c/span\u003e\u003cdiv\u003e\n\u003cul data-sourcepos=\"7:1-8:79\"\u003e\n\u003cli data-sourcepos=\"7:1-8:79\"\u003e\n\u003cstrong\u003e戻り値\u003c/strong\u003eがあるかないか。\u003cbr\u003e\n\u003ccode\u003emap\u003c/code\u003eは\u003cstrong\u003e新しい配列\u003c/strong\u003eを返すが、\u003ccode\u003eforEach\u003c/code\u003eは\u003cstrong\u003e何も返さない\u003c/strong\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003ch3 data-sourcepos=\"11:1-11:13\"\u003e\n\u003cspan id=\"共通点\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E5%85%B1%E9%80%9A%E7%82%B9\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e共通点\u003c/h3\u003e\n\u003cdiv data-sourcepos=\"12:1-15:3\" class=\"note info\"\u003e\n\u003cspan class=\"fa fa-fw fa-check-circle\"\u003e\u003c/span\u003e\u003cdiv\u003e\n\u003cul data-sourcepos=\"13:1-14:65\"\u003e\n\u003cli data-sourcepos=\"13:1-13:44\"\u003e各要素に対しての処理を行う。\u003c/li\u003e\n\u003cli data-sourcepos=\"14:1-14:65\"\u003e元の配列を\u003cstrong\u003e編集しない\u003c/strong\u003e。(非破壊的メソッド)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003ch2 data-sourcepos=\"17:1-17:9\"\u003e\n\u003cspan id=\"詳細\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E8%A9%B3%E7%B4%B0\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e詳細\u003c/h2\u003e\n\u003ch3 data-sourcepos=\"18:1-18:8\"\u003e\n\u003cspan id=\"map\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#map\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003emap:\u003c/h3\u003e\n\u003cul data-sourcepos=\"19:3-21:0\"\u003e\n\u003cli data-sourcepos=\"19:3-21:0\"\u003e各要素に対して指定した関数を実行し、その\u003cstrong\u003e結果を新しい配列として返します\u003c/strong\u003e。\u003cbr\u003e\n元の配列を変えずに、新しい配列を作成する場合に使用します。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"javascript\" data-sourcepos=\"22:3-26:5\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"kd\"\u003econst\u003c/span\u003e \u003cspan class=\"nx\"\u003earray\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e];\u003c/span\u003e\n\u003cspan class=\"kd\"\u003econst\u003c/span\u003e \u003cspan class=\"nx\"\u003enewArray\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nx\"\u003earray\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003emap\u003c/span\u003e\u003cspan class=\"p\"\u003e((\u003c/span\u003e\u003cspan class=\"nx\"\u003eelement\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan class=\"nx\"\u003eelement\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003cspan class=\"nx\"\u003econsole\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003elog\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003enewArray\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e  \u003cspan class=\"c1\"\u003e// [2, 4, 6] - 元の配列の各要素を2倍にした新しい配列\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003ch3 data-sourcepos=\"28:1-28:12\"\u003e\n\u003cspan id=\"foreach\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#foreach\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eforEach:\u003c/h3\u003e\n\u003cul data-sourcepos=\"29:3-31:0\"\u003e\n\u003cli data-sourcepos=\"29:3-31:0\"\u003e各要素に対して指定した関数を実行しますが、\u003cstrong\u003e戻り値はありません\u003c/strong\u003e。\u003cbr\u003e\n配列を変更せず、単に副作用を伴う処理（例えば、ログを出力したり、他の値を変更したりすること）を行う場合に使用します。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"javascript\" data-sourcepos=\"32:3-38:5\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"kd\"\u003econst\u003c/span\u003e \u003cspan class=\"nx\"\u003earray\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e];\u003c/span\u003e\n\u003cspan class=\"nx\"\u003earray\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eforEach\u003c/span\u003e\u003cspan class=\"p\"\u003e((\u003c/span\u003e\u003cspan class=\"nx\"\u003eelement\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n  \u003cspan class=\"nx\"\u003econsole\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003elog\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003eelement\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e  \u003cspan class=\"c1\"\u003e// 各要素を2倍にしてコンソールに出力\u003c/span\u003e\n\u003cspan class=\"p\"\u003e});\u003c/span\u003e\n\u003cspan class=\"c1\"\u003e// 出力: 2, 4, 6\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003ch2 data-sourcepos=\"40:1-40:12\"\u003e\n\u003cspan id=\"まとめ\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%81%BE%E3%81%A8%E3%82%81\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eまとめ\u003c/h2\u003e\n\u003cul data-sourcepos=\"41:1-42:104\"\u003e\n\u003cli data-sourcepos=\"41:1-41:127\"\u003e\n\u003cstrong\u003e新しい配列が必要\u003c/strong\u003eな場合は \u003ccode\u003emap\u003c/code\u003e を使用して、変換された値を新しい配列として返します。\u003c/li\u003e\n\u003cli data-sourcepos=\"42:1-42:104\"\u003e\n\u003cstrong\u003e戻り値が不要\u003c/strong\u003eで、副作用を伴う処理を行う場合は \u003ccode\u003eforEach\u003c/code\u003e を使用します。\u003c/li\u003e\n\u003c/ul\u003e\n",
    "body": "## アジェンダ\nmapメソッドとforEachメソッドの基本的な違いについて簡単にまとめます。\n\n## 違い/共通点\n### 違い\n:::note info\n- **戻り値**があるかないか。\n`map`は**新しい配列**を返すが、`forEach`は**何も返さない**。\n:::\n\n### 共通点\n:::note info\n- 各要素に対しての処理を行う。\n- 元の配列を**編集しない**。(非破壊的メソッド)\n:::\n\n## 詳細\n### map:\n  - 各要素に対して指定した関数を実行し、その**結果を新しい配列として返します**。\n  元の配列を変えずに、新しい配列を作成する場合に使用します。\n\n  ```javascript\n  const array = [1, 2, 3];\n  const newArray = array.map((element) =\u003e element * 2);\n  console.log(newArray);  // [2, 4, 6] - 元の配列の各要素を2倍にした新しい配列\n  ```\n\n### forEach:\n  - 各要素に対して指定した関数を実行しますが、**戻り値はありません**。\n  配列を変更せず、単に副作用を伴う処理（例えば、ログを出力したり、他の値を変更したりすること）を行う場合に使用します。\n\n  ```javascript\n  const array = [1, 2, 3];\n  array.forEach((element) =\u003e {\n    console.log(element * 2);  // 各要素を2倍にしてコンソールに出力\n  });\n  // 出力: 2, 4, 6\n  ```\n\n## まとめ\n- **新しい配列が必要**な場合は `map` を使用して、変換された値を新しい配列として返します。\n- **戻り値が不要**で、副作用を伴う処理を行う場合は `forEach` を使用します。\n",
    "coediting": false,
    "comments_count": 0,
    "created_at": "2024-08-22T12:15:57+09:00",
    "group": null,
    "id": "af022d437aed9261be15",
    "likes_count": 0,
    "private": false,
    "reactions_count": 0,
    "stocks_count": 0,
    "tags": [
      {
        "name": "JavaScript",
        "versions": []
      }
    ],
    "title": "mapメソッドとforEachメソッドの基本的な違い",
    "updated_at": "2024-08-22T12:16:35+09:00",
    "url": "https://qiita.com/kimascript/items/af022d437aed9261be15",
    "user": {
      "description": "null",
      "facebook_id": "",
      "followees_count": 10,
      "followers_count": 5,
      "github_login_name": null,
      "id": "kimascript",
      "items_count": 94,
      "linkedin_id": "",
      "location": "",
      "name": ". .",
      "organization": "",
      "permanent_id": 2961835,
      "profile_image_url": "https://s3-ap-northeast-1.amazonaws.com/qiita-image-store/0/2961835/7e744fe37dd6a8427471dd425a554ceda47a5b99/large.png?1714383867",
      "team_only": false,
      "twitter_screen_name": "kimascript",
      "website_url": ""
    },
    "page_views_count": null,
    "team_membership": null,
    "organization_url_name": null,
    "slide": false
  },
  {
    "rendered_body": "\u003cp data-sourcepos=\"1:1-1:279\"\u003e最近、EC2のAuto Scalingグループがスケールアップした際に、ALB（Application Load Balancer）のリクエスト数が急に増えましたが、IPアドレスからリクエストが来ているのかが分からなかったのでメモ程度に残します。\u003c/p\u003e\n\u003cp data-sourcepos=\"3:1-3:11\"\u003e\u003cstrong\u003e方法:\u003c/strong\u003e\u003c/p\u003e\n\u003col data-sourcepos=\"4:1-9:0\"\u003e\n\u003cli data-sourcepos=\"4:1-9:0\"\u003e\n\u003cstrong\u003eELBのアクセスログを確認する:\u003c/strong\u003e\u003cbr\u003e\n最初に試すべきは、ALB（Application Load Balancer） のアクセスログを確認することです。このログには、どのIPアドレスからリクエストが送られてきたかが記録されています。ログは、AWSコンソールやCLIから取得できます。この方法で、リクエスト元のIPアドレスを簡単に特定できます。\u003cbr\u003e\nALBのアクセスログはS3に保存されています。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp data-sourcepos=\"10:1-10:48\"\u003e※以下からは確認できないので注意\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"12:5-12:22\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003eAuto Scalingログ\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"13:4-13:289\"\u003eAuto Scalingログは、イベントやアクティビティを記録するためのものであり、リクエスト元のIPアドレスを含みません。したがって、リクエスト元IPを特定するためには、Auto Scalingログを使用することはできません。\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"15:5-16:4\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003eEC2インスタンスのログ\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"17:4-17:412\"\u003eEC2インスタンスのログは、そのインスタンスで実行されたアクティビティを記録しますが、リクエスト元のIPアドレスを含みません。そのため、EC2インスタンスのログを使用してリクエスト元IPを特定することはできません。サーバ内にアクセスログなどあれば見れるとは思いますがALBから見たほうが安易\u003c/p\u003e\n",
    "body": "最近、EC2のAuto Scalingグループがスケールアップした際に、ALB（Application Load Balancer）のリクエスト数が急に増えましたが、IPアドレスからリクエストが来ているのかが分からなかったのでメモ程度に残します。\n\n**方法:**\n1. **ELBのアクセスログを確認する:**  \n最初に試すべきは、ALB（Application Load Balancer） のアクセスログを確認することです。このログには、どのIPアドレスからリクエストが送られてきたかが記録されています。ログは、AWSコンソールやCLIから取得できます。この方法で、リクエスト元のIPアドレスを簡単に特定できます。\nALBのアクセスログはS3に保存されています。\n\n\n\n※以下からは確認できないので注意\n    \n    Auto Scalingログ\n   Auto Scalingログは、イベントやアクティビティを記録するためのものであり、リクエスト元のIPアドレスを含みません。したがって、リクエスト元IPを特定するためには、Auto Scalingログを使用することはできません。\n\n    EC2インスタンスのログ\n    \n   EC2インスタンスのログは、そのインスタンスで実行されたアクティビティを記録しますが、リクエスト元のIPアドレスを含みません。そのため、EC2インスタンスのログを使用してリクエスト元IPを特定することはできません。サーバ内にアクセスログなどあれば見れるとは思いますがALBから見たほうが安易\n",
    "coediting": false,
    "comments_count": 0,
    "created_at": "2024-08-22T12:14:23+09:00",
    "group": null,
    "id": "5f4a8d5ba85227ba3e19",
    "likes_count": 0,
    "private": false,
    "reactions_count": 0,
    "stocks_count": 0,
    "tags": [
      {
        "name": "AWS",
        "versions": []
      }
    ],
    "title": "EC2へのアクセス元IPを特定する方法",
    "updated_at": "2024-08-22T12:14:23+09:00",
    "url": "https://qiita.com/snowmag/items/5f4a8d5ba85227ba3e19",
    "user": {
      "description": "AWSのインフラエンジニアしてます。メモ程度に記事残してます。誤りもあるかもしれませんがその際は指摘していただけますと幸いです。\r\n基本的に下書きで残しているのを気まぐれで投稿してます。",
      "facebook_id": "",
      "followees_count": 2,
      "followers_count": 0,
      "github_login_name": null,
      "id": "snowmag",
      "items_count": 10,
      "linkedin_id": "",
      "location": "",
      "name": "",
      "organization": "",
      "permanent_id": 1737452,
      "profile_image_url": "https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1737452/profile-images/1705417261",
      "team_only": false,
      "twitter_screen_name": null,
      "website_url": ""
    },
    "page_views_count": null,
    "team_membership": null,
    "organization_url_name": null,
    "slide": false
  },
  {
    "rendered_body": "\u003ch1 data-sourcepos=\"7:1-7:14\"\u003e\n\u003cspan id=\"はじめに\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eはじめに\u003c/h1\u003e\n\u003cp data-sourcepos=\"10:1-10:480\"\u003eV4L2 のストリーミングI/O(V4L2_MEMORY_MMAP) はV4L2 ストリーミング I/O の方式の一つで、V4L2 ドライバ内(カーネル内)で確保した V4L2 バッファを mmap 機構を使ってユーザー空間にマッピングすることで、ユーザープログラムが V4L2 バッファにアクセスできるようにします。V4L2バッファをユーザー空間から直接アクセス出来るため、この方式は比較的よく使われます。\u003c/p\u003e\n\u003cp data-sourcepos=\"14:1-14:245\"\u003eしかし、ある種の V4L2 ドライバでは、mmap でユーザー空間にマッピングする際にキャッシュがオフになってしまってメモリアクセスが非常に遅くなり性能が出ない問題がありました。\u003c/p\u003e\n\u003cp data-sourcepos=\"16:1-16:102\"\u003eこの問題を起こす V4L2 ドライバの一つとして、Xilinx の Video DMA があります。\u003c/p\u003e\n\u003cp data-sourcepos=\"20:1-20:63\"\u003eこの記事では、そのメカニズムを説明します。\u003c/p\u003e\n\u003cp data-sourcepos=\"24:1-24:75\"\u003eなお、この記事は、以前投稿した次記事の改訂版です。\u003c/p\u003e\n\u003cul data-sourcepos=\"26:1-32:0\"\u003e\n\u003cli data-sourcepos=\"26:1-32:0\"\u003e\u003ca href=\"https://qiita.com/ikwzm/items/8f1c98efe6fd4ae2490a\" id=\"reference-48cf7bcf59237b3b354e\"\u003e『V4L2 ストリーミングI/O(V4L2_MEMORY_MMAP) で性能が出ない問題)』 @Qiita\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 data-sourcepos=\"33:1-33:50\"\u003e\n\u003cspan id=\"キャッシュがオフになるメカニズム\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%82%AD%E3%83%A3%E3%83%83%E3%82%B7%E3%83%A5%E3%81%8C%E3%82%AA%E3%83%95%E3%81%AB%E3%81%AA%E3%82%8B%E3%83%A1%E3%82%AB%E3%83%8B%E3%82%BA%E3%83%A0\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eキャッシュがオフになるメカニズム\u003c/h1\u003e\n\u003ch2 data-sourcepos=\"37:1-37:50\"\u003e\n\u003cspan id=\"v4l2-バッファのメモリアロケータ\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#v4l2-%E3%83%90%E3%83%83%E3%83%95%E3%82%A1%E3%81%AE%E3%83%A1%E3%83%A2%E3%83%AA%E3%82%A2%E3%83%AD%E3%82%B1%E3%83%BC%E3%82%BF\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eV4L2 バッファのメモリアロケータ−\u003c/h2\u003e\n\u003cp data-sourcepos=\"40:1-40:86\"\u003eV4L2 バッファのメモリアロケータ−には次の３種類があります。\u003c/p\u003e\n\u003cul data-sourcepos=\"42:3-45:0\"\u003e\n\u003cli data-sourcepos=\"42:3-42:56\"\u003evmalloc\t\t\t: DMA を伴わない V4L2 ドライバ用\u003c/li\u003e\n\u003cli data-sourcepos=\"43:3-43:65\"\u003edma-sg\t\t\t: Scatter Gather に対応した DMA デバイス用\u003c/li\u003e\n\u003cli data-sourcepos=\"44:3-45:0\"\u003edma-contig\t: Scatter Gather に対応していない DMA デバイス用\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"46:1-46:66\"\u003eこのうち、問題となるのは最後の dma-contig です。\u003c/p\u003e\n\u003ch3 data-sourcepos=\"49:1-49:11\"\u003e\n\u003cspan id=\"vmalloc\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#vmalloc\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003evmalloc\u003c/h3\u003e\n\u003cp data-sourcepos=\"52:1-52:465\"\u003evmalloc はDMAを伴わない V4L2 ドライバのためのメモリアロケータ−です。例えば、USB Camera のV4L2 ドライバがこれにあたります。USB の場合は USB のデバイスドライバが USB デバイスとのデータ転送を行い、V4L2 ドライバ自体は直接 USB デバイスとのデータ転送を行いません。そのため、カーネルが普通に使っている vmalloc を使ってメモリを確保します。\u003c/p\u003e\n\u003ch3 data-sourcepos=\"55:1-55:10\"\u003e\n\u003cspan id=\"dma-sg\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#dma-sg\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003edma-sg\u003c/h3\u003e\n\u003cp data-sourcepos=\"58:1-58:409\"\u003edma-sg は Scatter Gather に対応した DMA を持つデバイスのためのメモリアロケータ−です。Scatter Gather に対応しているため、バッファが物理メモリ空間上に連続していなくても DMA 転送が可能です。 dma-sg は Linux Kernel の DMA Mapping API を使ってメモリを確保します。しかし、mmap は DMA Mapping API を使っていません。\u003c/p\u003e\n\u003ch3 data-sourcepos=\"61:1-61:14\"\u003e\n\u003cspan id=\"dma-contig\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#dma-contig\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003edma-contig\u003c/h3\u003e\n\u003cp data-sourcepos=\"64:1-64:561\"\u003edma-contig は Scatter Gather に対応していない DMA を持つデバイスのためのメモリアロケータ−です。Scatter Gather に対応していないため、バッファは物理メモリ空間上に連続していなければなりません。dma-contig は Linux  Kernel の DMA Mapping API を使ってメモリを確保します。そして mmap も DMA Mapping API を使います。実は Linux Kernel の DMA Mapping API が提供する mmap は、Cache Coherence Hardware を持っていないと CPU Cache が無効になります。\u003c/p\u003e\n\u003cp data-sourcepos=\"66:1-66:197\"\u003eしたがって、この dma-contig を採用した V4L2 ドライバでV4L2_MEMORY_MMAP を使うと、Cache Coherence Hardware を持っていない場合は CPU Cache が無効になります。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"69:1-69:22\"\u003e\n\u003cspan id=\"dma-contig-の-mmap\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#dma-contig-%E3%81%AE-mmap\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003edma-contig の mmap\u003c/h2\u003e\n\u003cp data-sourcepos=\"72:1-72:59\"\u003edma-contig の mmap は次のようになっています。\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"C\" data-sourcepos=\"75:1-103:3\"\u003e\n\u003cdiv class=\"code-lang\"\u003e\u003cspan class=\"bold\"\u003ehttps://elixir.bootlin.com/linux/v6.1.97/source/drivers/media/common/videobuf2/videobuf2-dma-contig.c\u003c/span\u003e\u003c/div\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"k\"\u003estatic\u003c/span\u003e \u003cspan class=\"kt\"\u003eint\u003c/span\u003e \u003cspan class=\"nf\"\u003evb2_dc_mmap\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003evoid\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"n\"\u003ebuf_priv\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"k\"\u003estruct\u003c/span\u003e \u003cspan class=\"n\"\u003evm_area_struct\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"n\"\u003evma\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\t\u003cspan class=\"k\"\u003estruct\u003c/span\u003e \u003cspan class=\"n\"\u003evb2_dc_buf\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"n\"\u003ebuf\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ebuf_priv\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\t\u003cspan class=\"kt\"\u003eint\u003c/span\u003e \u003cspan class=\"n\"\u003eret\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\t\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e!\u003c/span\u003e\u003cspan class=\"n\"\u003ebuf\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\t\t\u003cspan class=\"n\"\u003eprintk\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eKERN_ERR\u003c/span\u003e \u003cspan class=\"s\"\u003e\"No buffer to map\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\u003c/span\u003e\u003cspan class=\"s\"\u003e\"\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\t\t\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e\u003cspan class=\"n\"\u003eEINVAL\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\t\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\t\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ebuf\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003enon_coherent_mem\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\t\t\u003cspan class=\"n\"\u003eret\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003edma_mmap_noncontiguous\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ebuf\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003edev\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003evma\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ebuf\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\t\t\t\t\t     \u003cspan class=\"n\"\u003ebuf\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003edma_sgt\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\t\u003cspan class=\"k\"\u003eelse\u003c/span\u003e\n\t\t\u003cspan class=\"n\"\u003eret\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003edma_mmap_attrs\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ebuf\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003edev\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003evma\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ebuf\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003ecookie\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ebuf\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003edma_addr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\t\t\t\t     \u003cspan class=\"n\"\u003ebuf\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ebuf\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003eattrs\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\t\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eret\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\t\t\u003cspan class=\"n\"\u003epr_err\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"Remapping memory failed, error: %d\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\u003c/span\u003e\u003cspan class=\"s\"\u003e\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eret\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\t\t\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003eret\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\t\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\t\u003cspan class=\"n\"\u003evma\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003evm_flags\u003c/span\u003e\t\t\u003cspan class=\"o\"\u003e|=\u003c/span\u003e \u003cspan class=\"n\"\u003eVM_DONTEXPAND\u003c/span\u003e \u003cspan class=\"o\"\u003e|\u003c/span\u003e \u003cspan class=\"n\"\u003eVM_DONTDUMP\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\t\u003cspan class=\"n\"\u003evma\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003evm_private_data\u003c/span\u003e\t\u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003ebuf\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003ehandler\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\t\u003cspan class=\"n\"\u003evma\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003evm_ops\u003c/span\u003e\t\t\u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003evb2_common_vm_ops\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\t\u003cspan class=\"n\"\u003evma\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003evm_ops\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003eopen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003evma\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\t\u003cspan class=\"n\"\u003epr_debug\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"%s: mapped dma addr 0x%08lx at 0x%08lx, size %lu\u003c/span\u003e\u003cspan class=\"se\"\u003e\\n\u003c/span\u003e\u003cspan class=\"s\"\u003e\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\t\t \u003cspan class=\"n\"\u003e__func__\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003eunsigned\u003c/span\u003e \u003cspan class=\"kt\"\u003elong\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"n\"\u003ebuf\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003edma_addr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003evma\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003evm_start\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\t\t \u003cspan class=\"n\"\u003ebuf\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\t\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cp data-sourcepos=\"106:1-106:315\"\u003e実際に mmap を行うのはLinux Kernel  の DMA Mapping API で提供される dma_mmap_attrs() です。なお、non_coherent_mem の場合は dma_mmap_noncontiguous() が呼ばれていますが、これは別の DMA Buffer をImport した時に使われるものなので、ここでは説明しません。\u003c/p\u003e\n\u003cp data-sourcepos=\"110:1-110:236\"\u003eLinux Kernel 5.0 以降の DMA Mapping API は dma-direct という実装が使われています。この dma-direct の説明と、CPU Cache が無効になるメカニズムについては、次の記事を参照してください。\u003c/p\u003e\n\u003cul data-sourcepos=\"112:3-116:0\"\u003e\n\u003cli data-sourcepos=\"112:3-112:196\"\u003e\u003ca href=\"https://qiita.com/ikwzm/items/f32985890b2013ef5528\" id=\"reference-9842d10a4e4a47f8f1e8\"\u003e『Linux で DMA Bufferを mmap した時に CPU Cacheが無効になる場合がある (Linux Kernel の Cache 問題の扱い)』 @Qiita\u003c/a\u003e\u003c/li\u003e\n\u003cli data-sourcepos=\"113:3-116:0\"\u003e\u003ca href=\"https://qiita.com/ikwzm/items/0f77072158ce842018fc\" id=\"reference-5a18eaa41618b1bc6680\"\u003e『Linux では Cache Coherence Hardware を持っていないとDMA Buffer をmmap する際に CPU Cache が無効になる』 @Qiita\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"117:1-117:143\"\u003e結論だけを述べると、dma-contig の mmap は、Cache Coherence Hardware を持っていないと CPU Cache が無効になります。\u003c/p\u003e\n\u003ch1 data-sourcepos=\"120:1-120:14\"\u003e\n\u003cspan id=\"対処方法\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E5%AF%BE%E5%87%A6%E6%96%B9%E6%B3%95\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e対処方法\u003c/h1\u003e\n\u003cp data-sourcepos=\"123:1-123:209\"\u003e\u003cdel\u003eこちらに udmabuf を使って対処する方法を投稿しました。\u003c/del\u003e(2024年改定 この記事の内容は古くて現在は使えません。現在、dma-heap を使った対処方法を執筆中)\u003c/p\u003e\n\u003cul data-sourcepos=\"125:1-131:0\"\u003e\n\u003cli data-sourcepos=\"125:1-131:0\"\u003e\u003ca href=\"https://qiita.com/ikwzm/items/c81e8b749576300fceee\" id=\"reference-f868b8b56d8c20642099\"\u003e「Linuxでユーザー空間で動作するプログラムとハードウェアがメモリを共有するためのデバイスドライバ(V4L2_MEMORY_USERPTR編)」@Qiita\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 data-sourcepos=\"132:1-132:8\"\u003e\n\u003cspan id=\"参考\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E5%8F%82%E8%80%83\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e参考\u003c/h1\u003e\n\u003ch2 data-sourcepos=\"136:1-136:78\"\u003e\n\u003cspan id=\"v4l2-v4l2_memory_userptrcontiguous-mapping-is-too-small-40961228800\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#v4l2-v4l2_memory_userptrcontiguous-mapping-is-too-small-40961228800\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e「V4l2 V4L2_MEMORY_USERPTR:contiguous mapping is too small 4096/1228800」\u003c/h2\u003e\n\u003cp data-sourcepos=\"139:1-139:67\"\u003eXilinx のフォーラムに次のスレッドがありました。\u003c/p\u003e\n\u003cul data-sourcepos=\"143:1-147:0\"\u003e\n\u003cli data-sourcepos=\"143:1-147:0\"\u003e「V4l2 V4L2_MEMORY_USERPTR:contiguous mapping is too small 4096/1228800」\u003cbr\u003e\n\u003ca href=\"https://forums.xilinx.com/t5/Embedded-Linux/V4l2-V4L2-MEMORY-USERPTR-contiguous-mapping-is-too-small-4096/td-p/825067\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://forums.xilinx.com/t5/Embedded-Linux/V4l2-V4L2-MEMORY-USERPTR-contiguous-mapping-is-too-small-4096/td-p/825067\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"148:1-148:317\"\u003eXilinx の VDMA を使ってキャプチャーしようとしたら mmap によるデータ転送が遅いので、mmap の代わりにユーザー空間にバッファを確保して、それをV4L2 ドライバに渡したら、今度は連続した空間が小さすぎてエラーになったという話です。\u003c/p\u003e\n\u003cp data-sourcepos=\"150:1-150:191\"\u003eこれは考えてみれば当たり前な話で、Xilinx の VDMA は Scatter Gather に対応していないのでバッファは連続したメモリ空間になければなりません。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"155:1-155:52\"\u003e\n\u003cspan id=\"buffer-from-udmabuf-and-v4l2_memory_userptr\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#buffer-from-udmabuf-and-v4l2_memory_userptr\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e「Buffer from UDMABUF and V4L2_MEMORY_USERPTR」\u003c/h2\u003e\n\u003cp data-sourcepos=\"158:1-158:117\"\u003eもともと私がこの問題に興味を持ったのは、udmabuf に次の issue があげられたからです。\u003c/p\u003e\n\u003cul data-sourcepos=\"162:1-166:0\"\u003e\n\u003cli data-sourcepos=\"162:1-166:0\"\u003e「Buffer from UDMABUF and V4L2_MEMORY_USERPTR」\u003cbr\u003e\n\u003ca href=\"https://github.com/ikwzm/udmabuf/issues/38\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://github.com/ikwzm/udmabuf/issues/38\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"167:2-167:266\"\u003emmap によるデータ転送が遅いので、mmap の代わりに udmabuf で連続した物理メモリを確保してユーザー空間にマッピングして、それを V4L2 ドライバに渡せばデータ転送が速くしようという試みでした。\u003c/p\u003e\n\u003cp data-sourcepos=\"169:1-169:142\"\u003e最初は上手くいかなくて udmabuf に issue があげられたのですが、最終的にはこの試みは成功したようです。\u003c/p\u003e\n\u003cp data-sourcepos=\"173:1-173:165\"\u003e残念ながら、この方法は Linux Kernel 5.10 以降は使えません。詳細は次の記事の「注意(2024年4月3日追記)」参照してください。\u003c/p\u003e\n\u003cul data-sourcepos=\"175:1-179:0\"\u003e\n\u003cli data-sourcepos=\"175:1-179:0\"\u003e\u003ca href=\"https://qiita.com/ikwzm/items/c81e8b749576300fceee\"\u003e『Linuxでユーザー空間で動作するプログラムとハードウェアがメモリを共有するためのデバイスドライバ(V4L2_MEMORY_USERPTR編)』　@Qiita\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 data-sourcepos=\"180:1-180:76\"\u003e\n\u003cspan id=\"ultra96-で-julia-set-をぐりぐり動かせるやつを作った\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#ultra96-%E3%81%A7-julia-set-%E3%82%92%E3%81%90%E3%82%8A%E3%81%90%E3%82%8A%E5%8B%95%E3%81%8B%E3%81%9B%E3%82%8B%E3%82%84%E3%81%A4%E3%82%92%E4%BD%9C%E3%81%A3%E3%81%9F\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e「Ultra96 で Julia set をぐりぐり動かせるやつを作った」\u003c/h2\u003e\n\u003cp data-sourcepos=\"183:1-183:51\"\u003eこれは、ある方のブログの記事です。\u003c/p\u003e\n\u003cul data-sourcepos=\"187:1-191:0\"\u003e\n\u003cli data-sourcepos=\"187:1-191:0\"\u003e「Ultra96 で Julia set をぐりぐり動かせるやつを作った」\u003cbr\u003e\n\u003ca href=\"https://blog.myon.info/entry/2019/05/15/ultra96-julia-set-explorer/\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://blog.myon.info/entry/2019/05/15/ultra96-julia-set-explorer/\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp data-sourcepos=\"192:1-192:365\"\u003eこの記事では Vivado-HLS で記述したJulia set を  Ultra96 の FPGA に実装してディスプレイに表示しています。その際、FPGA との通信に Xilinx の VDMA を使っていて、最初は V4L2 の mmap を使っていたけど全然性能が出なくて、dma-buf というバッファを共有する方法にしたという話です。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"197:1-197:53\"\u003e\n\u003cspan id=\"imx6ull-v4l2-slow-memcpy-for-captured-memory\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#imx6ull-v4l2-slow-memcpy-for-captured-memory\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e「imx6ull v4l2 slow memcpy for captured memory」\u003c/h2\u003e\n\u003cp data-sourcepos=\"200:1-200:72\"\u003eNXP社のフォーラムでも同じような投稿がありました。\u003c/p\u003e\n\u003cul data-sourcepos=\"204:1-205:39\"\u003e\n\u003cli data-sourcepos=\"204:1-205:39\"\u003e「imx6ull v4l2 slow memcpy for captured memory」\u003cbr\u003e\n\u003ca href=\"https://community.nxp.com/thread/483135\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://community.nxp.com/thread/483135\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "body": "\n\n\n\n\n\n# はじめに\n\n\nV4L2 のストリーミングI/O(V4L2_MEMORY_MMAP) はV4L2 ストリーミング I/O の方式の一つで、V4L2 ドライバ内(カーネル内)で確保した V4L2 バッファを mmap 機構を使ってユーザー空間にマッピングすることで、ユーザープログラムが V4L2 バッファにアクセスできるようにします。V4L2バッファをユーザー空間から直接アクセス出来るため、この方式は比較的よく使われます。\n\n\n\nしかし、ある種の V4L2 ドライバでは、mmap でユーザー空間にマッピングする際にキャッシュがオフになってしまってメモリアクセスが非常に遅くなり性能が出ない問題がありました。\n\nこの問題を起こす V4L2 ドライバの一つとして、Xilinx の Video DMA があります。\n\n\n\nこの記事では、そのメカニズムを説明します。\n\n\n\nなお、この記事は、以前投稿した次記事の改訂版です。\n\n* [『V4L2 ストリーミングI/O(V4L2_MEMORY_MMAP) で性能が出ない問題)』 @Qiita](https://qiita.com/ikwzm/items/8f1c98efe6fd4ae2490a)\n\n\n\n\n\n\n# キャッシュがオフになるメカニズム\n\n\n\n## V4L2 バッファのメモリアロケータ−\n\n\nV4L2 バッファのメモリアロケータ−には次の３種類があります。\n\n  * vmalloc\t\t\t: DMA を伴わない V4L2 ドライバ用\n  * dma-sg\t\t\t: Scatter Gather に対応した DMA デバイス用\n  * dma-contig\t: Scatter Gather に対応していない DMA デバイス用\n\nこのうち、問題となるのは最後の dma-contig です。\n\n\n### vmalloc\n\n\nvmalloc はDMAを伴わない V4L2 ドライバのためのメモリアロケータ−です。例えば、USB Camera のV4L2 ドライバがこれにあたります。USB の場合は USB のデバイスドライバが USB デバイスとのデータ転送を行い、V4L2 ドライバ自体は直接 USB デバイスとのデータ転送を行いません。そのため、カーネルが普通に使っている vmalloc を使ってメモリを確保します。\n\n\n### dma-sg\n\n\ndma-sg は Scatter Gather に対応した DMA を持つデバイスのためのメモリアロケータ−です。Scatter Gather に対応しているため、バッファが物理メモリ空間上に連続していなくても DMA 転送が可能です。 dma-sg は Linux Kernel の DMA Mapping API を使ってメモリを確保します。しかし、mmap は DMA Mapping API を使っていません。\n\n\n### dma-contig\n\n\ndma-contig は Scatter Gather に対応していない DMA を持つデバイスのためのメモリアロケータ−です。Scatter Gather に対応していないため、バッファは物理メモリ空間上に連続していなければなりません。dma-contig は Linux  Kernel の DMA Mapping API を使ってメモリを確保します。そして mmap も DMA Mapping API を使います。実は Linux Kernel の DMA Mapping API が提供する mmap は、Cache Coherence Hardware を持っていないと CPU Cache が無効になります。\n\nしたがって、この dma-contig を採用した V4L2 ドライバでV4L2_MEMORY_MMAP を使うと、Cache Coherence Hardware を持っていない場合は CPU Cache が無効になります。\n\n\n## dma-contig の mmap\n\n\ndma-contig の mmap は次のようになっています。\n\n\n```C:https://elixir.bootlin.com/linux/v6.1.97/source/drivers/media/common/videobuf2/videobuf2-dma-contig.c\nstatic int vb2_dc_mmap(void *buf_priv, struct vm_area_struct *vma)\n{\n\tstruct vb2_dc_buf *buf = buf_priv;\n\tint ret;\n\tif (!buf) {\n\t\tprintk(KERN_ERR \"No buffer to map\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (buf-\u003enon_coherent_mem)\n\t\tret = dma_mmap_noncontiguous(buf-\u003edev, vma, buf-\u003esize,\n\t\t\t\t\t     buf-\u003edma_sgt);\n\telse\n\t\tret = dma_mmap_attrs(buf-\u003edev, vma, buf-\u003ecookie, buf-\u003edma_addr,\n\t\t\t\t     buf-\u003esize, buf-\u003eattrs);\n\tif (ret) {\n\t\tpr_err(\"Remapping memory failed, error: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\tvma-\u003evm_flags\t\t|= VM_DONTEXPAND | VM_DONTDUMP;\n\tvma-\u003evm_private_data\t= \u0026buf-\u003ehandler;\n\tvma-\u003evm_ops\t\t= \u0026vb2_common_vm_ops;\n\tvma-\u003evm_ops-\u003eopen(vma);\n\tpr_debug(\"%s: mapped dma addr 0x%08lx at 0x%08lx, size %lu\\n\",\n\t\t __func__, (unsigned long)buf-\u003edma_addr, vma-\u003evm_start,\n\t\t buf-\u003esize);\n\treturn 0;\n}\n```\n\n\n実際に mmap を行うのはLinux Kernel  の DMA Mapping API で提供される dma_mmap_attrs() です。なお、non_coherent_mem の場合は dma_mmap_noncontiguous() が呼ばれていますが、これは別の DMA Buffer をImport した時に使われるものなので、ここでは説明しません。\n\n\n\nLinux Kernel 5.0 以降の DMA Mapping API は dma-direct という実装が使われています。この dma-direct の説明と、CPU Cache が無効になるメカニズムについては、次の記事を参照してください。\n\n  * [『Linux で DMA Bufferを mmap した時に CPU Cacheが無効になる場合がある (Linux Kernel の Cache 問題の扱い)』 @Qiita](https://qiita.com/ikwzm/items/f32985890b2013ef5528)\n  * [『Linux では Cache Coherence Hardware を持っていないとDMA Buffer をmmap する際に CPU Cache が無効になる』 @Qiita](https://qiita.com/ikwzm/items/0f77072158ce842018fc)\n\n\n\n結論だけを述べると、dma-contig の mmap は、Cache Coherence Hardware を持っていないと CPU Cache が無効になります。\n\n\n# 対処方法\n\n\n~~こちらに udmabuf を使って対処する方法を投稿しました。~~(2024年改定 この記事の内容は古くて現在は使えません。現在、dma-heap を使った対処方法を執筆中)\n\n* [「Linuxでユーザー空間で動作するプログラムとハードウェアがメモリを共有するためのデバイスドライバ(V4L2_MEMORY_USERPTR編)」@Qiita](https://qiita.com/ikwzm/items/c81e8b749576300fceee)\n\n\n\n\n\n\n# 参考\n\n\n\n## 「V4l2 V4L2_MEMORY_USERPTR:contiguous mapping is too small 4096/1228800」\n\n\nXilinx のフォーラムに次のスレッドがありました。\n\n\n\n* 「V4l2 V4L2_MEMORY_USERPTR:contiguous mapping is too small 4096/1228800」   \nhttps://forums.xilinx.com/t5/Embedded-Linux/V4l2-V4L2-MEMORY-USERPTR-contiguous-mapping-is-too-small-4096/td-p/825067\n\n\n\nXilinx の VDMA を使ってキャプチャーしようとしたら mmap によるデータ転送が遅いので、mmap の代わりにユーザー空間にバッファを確保して、それをV4L2 ドライバに渡したら、今度は連続した空間が小さすぎてエラーになったという話です。\n\nこれは考えてみれば当たり前な話で、Xilinx の VDMA は Scatter Gather に対応していないのでバッファは連続したメモリ空間になければなりません。\n\n\n\n\n## 「Buffer from UDMABUF and V4L2_MEMORY_USERPTR」\n\n\nもともと私がこの問題に興味を持ったのは、udmabuf に次の issue があげられたからです。\n\n\n\n* 「Buffer from UDMABUF and V4L2_MEMORY_USERPTR」    \nhttps://github.com/ikwzm/udmabuf/issues/38\n\n\n\n mmap によるデータ転送が遅いので、mmap の代わりに udmabuf で連続した物理メモリを確保してユーザー空間にマッピングして、それを V4L2 ドライバに渡せばデータ転送が速くしようという試みでした。\n\n最初は上手くいかなくて udmabuf に issue があげられたのですが、最終的にはこの試みは成功したようです。\n\n\n\n残念ながら、この方法は Linux Kernel 5.10 以降は使えません。詳細は次の記事の「注意(2024年4月3日追記)」参照してください。\n\n* [『Linuxでユーザー空間で動作するプログラムとハードウェアがメモリを共有するためのデバイスドライバ(V4L2_MEMORY_USERPTR編)』　@Qiita](https://qiita.com/ikwzm/items/c81e8b749576300fceee)\n\n\n\n\n## 「Ultra96 で Julia set をぐりぐり動かせるやつを作った」\n\n\nこれは、ある方のブログの記事です。\n\n\n\n* 「Ultra96 で Julia set をぐりぐり動かせるやつを作った」   \nhttps://blog.myon.info/entry/2019/05/15/ultra96-julia-set-explorer/\n\n\n\nこの記事では Vivado-HLS で記述したJulia set を  Ultra96 の FPGA に実装してディスプレイに表示しています。その際、FPGA との通信に Xilinx の VDMA を使っていて、最初は V4L2 の mmap を使っていたけど全然性能が出なくて、dma-buf というバッファを共有する方法にしたという話です。\n\n\n\n\n## 「imx6ull v4l2 slow memcpy for captured memory」\n\n\nNXP社のフォーラムでも同じような投稿がありました。\n\n\n\n* 「imx6ull v4l2 slow memcpy for captured memory」   \nhttps://community.nxp.com/thread/483135\n",
    "coediting": false,
    "comments_count": 0,
    "created_at": "2024-08-22T12:09:23+09:00",
    "group": null,
    "id": "474ababbc99cb4812bc7",
    "likes_count": 0,
    "private": false,
    "reactions_count": 0,
    "stocks_count": 0,
    "tags": [
      {
        "name": "Linux",
        "versions": []
      },
      {
        "name": "RaspberryPi",
        "versions": []
      },
      {
        "name": "FPGA",
        "versions": []
      },
      {
        "name": "xilinx",
        "versions": []
      },
      {
        "name": "V4L2",
        "versions": []
      }
    ],
    "title": "V4L2 ストリーミングI/O(V4L2_MEMORY_MMAP) で性能が出ない問題 (2024年改訂版)",
    "updated_at": "2024-08-22T12:20:22+09:00",
    "url": "https://qiita.com/ikwzm/items/474ababbc99cb4812bc7",
    "user": {
      "description": "元へっぽこ電子回路エンジニア。現在隠居中。どちらかというとVHDL派。最近はFPGA+SoC でいろいろやってます。github でもいろいろと公開してます。\r\nhttps://github.com/ikwzm",
      "facebook_id": "",
      "followees_count": 50,
      "followers_count": 205,
      "github_login_name": "ikwzm",
      "id": "ikwzm",
      "items_count": 245,
      "linkedin_id": "",
      "location": "横浜",
      "name": "",
      "organization": "隠居",
      "permanent_id": 24981,
      "profile_image_url": "https://qiita-image-store.s3.amazonaws.com/0/24981/profile-images/1473684232",
      "team_only": false,
      "twitter_screen_name": "ikwzm",
      "website_url": ""
    },
    "page_views_count": null,
    "team_membership": null,
    "organization_url_name": null,
    "slide": false
  },
  {
    "rendered_body": "\u003cp data-sourcepos=\"1:1-1:40\"\u003eこんにちは。　code poet です。\u003c/p\u003e\n\u003ch1 data-sourcepos=\"3:1-3:14\"\u003e\n\u003cspan id=\"はじめに\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eはじめに\u003c/h1\u003e\n\u003cp data-sourcepos=\"4:1-4:362\"\u003eデータの効率的な管理とコストの最適化を意識する場面ってありますよね。BigQuery 追加されたプレビュー機能 INFORMATION_SCHEMA のビューは、プロジェクトのデータ処理を最適化するための方法を提供します。本記事では、この機能を紹介し、活用方法について解説します。\u003c/p\u003e\n\u003ch1 data-sourcepos=\"6:1-6:8\"\u003e\n\u003cspan id=\"目次\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E7%9B%AE%E6%AC%A1\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e目次\u003c/h1\u003e\n\u003col data-sourcepos=\"7:1-11:0\"\u003e\n\u003cli data-sourcepos=\"7:1-7:74\"\u003e\u003ca href=\"#INFORMATION_SCHEMA.INSIGHTS%E3%81%A8%E3%81%AF\"\u003eINFORMATION_SCHEMA.INSIGHTSとは\u003c/a\u003e\u003c/li\u003e\n\u003cli data-sourcepos=\"8:1-8:118\"\u003e\u003ca href=\"#INFORMATION_SCHEMA.RECOMMENDATIONS%E3%81%A8%E3%81%9D%E3%81%AE%E6%B4%BB%E7%94%A8%E6%96%B9%E6%B3%95\"\u003eINFORMATION_SCHEMA.RECOMMENDATIONSとその活用方法\u003c/a\u003e\u003c/li\u003e\n\u003cli data-sourcepos=\"9:1-9:150\"\u003e\u003ca href=\"#INFORMATION_SCHEMA.RECOMMENDATIONS_BY_ORGANIZATION%E3%81%A8%E3%81%9D%E3%81%AE%E6%B4%BB%E7%94%A8%E6%96%B9%E6%B3%95\"\u003eINFORMATION_SCHEMA.RECOMMENDATIONS_BY_ORGANIZATIONとその活用方法\u003c/a\u003e\u003c/li\u003e\n\u003cli data-sourcepos=\"10:1-11:0\"\u003e\u003ca href=\"#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE\"\u003e参考文献\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp data-sourcepos=\"12:1-12:22\"\u003e\u003ca id=\"\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1 data-sourcepos=\"13:1-13:35\"\u003e\n\u003cspan id=\"information_schemainsightsとは\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#information_schemainsights%E3%81%A8%E3%81%AF\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eINFORMATION_SCHEMA.INSIGHTSとは\u003c/h1\u003e\n\u003cp data-sourcepos=\"15:1-15:455\"\u003e\u003cstrong\u003eINFORMATION_SCHEMA.INSIGHTS\u003c/strong\u003eは、現在のプロジェクト内で利用可能なBigQueryの推奨事項に関する洞察を提供するビューです。このビューは、Recommendation Hubから取得されたすべてのBigQueryインサイトを一元的に表示し、データセットのパーティショニングやクラスタリング、マテリアライズドビューの推奨事項、ロール推奨などの情報を含みます。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"17:1-17:72\"\u003e\n\u003cspan id=\"コスト削減のためのアクティブなインサイトの確認\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%82%B3%E3%82%B9%E3%83%88%E5%89%8A%E6%B8%9B%E3%81%AE%E3%81%9F%E3%82%81%E3%81%AE%E3%82%A2%E3%82%AF%E3%83%86%E3%82%A3%E3%83%96%E3%81%AA%E3%82%A4%E3%83%B3%E3%82%B5%E3%82%A4%E3%83%88%E3%81%AE%E7%A2%BA%E8%AA%8D\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eコスト削減のためのアクティブなインサイトの確認\u003c/h2\u003e\n\u003cp data-sourcepos=\"19:1-19:291\"\u003e以下のクエリ例は、コストカテゴリでアクティブなインサイトに関連する推奨事項を3つ表示します。推奨事項に基づく月次の節約見込み（GB単位のストレージ削減量、スロット時間の削減量）も一緒に表示されます。\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"sql\" data-sourcepos=\"21:1-65:5\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"k\"\u003eWITH\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e-- BigQueryの推奨事項に関するインサイトを取得するクエリ\u003c/span\u003e\n  \u003cspan class=\"n\"\u003einsights\u003c/span\u003e \u003cspan class=\"k\"\u003eAS\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\n  \u003cspan class=\"k\"\u003eSELECT\u003c/span\u003e\n    \u003cspan class=\"o\"\u003e*\u003c/span\u003e\n  \u003cspan class=\"k\"\u003eFROM\u003c/span\u003e\n    \u003cspan class=\"nv\"\u003e`region-us`\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eINFORMATION_SCHEMA\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eINSIGHTS\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e-- BigQueryの推奨事項そのものに関するデータを取得するクエリ\u003c/span\u003e\n  \u003cspan class=\"n\"\u003erecs\u003c/span\u003e \u003cspan class=\"k\"\u003eAS\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\n  \u003cspan class=\"k\"\u003eSELECT\u003c/span\u003e\n    \u003cspan class=\"n\"\u003erecommender\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"n\"\u003erecommendation_id\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eadditional_details\u003c/span\u003e\n  \u003cspan class=\"k\"\u003eFROM\u003c/span\u003e\n    \u003cspan class=\"nv\"\u003e`region-us`\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eINFORMATION_SCHEMA\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eRECOMMENDATIONS\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"k\"\u003eSELECT\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e-- 推奨を行ったRecommenderのタイプ\u003c/span\u003e\n  \u003cspan class=\"n\"\u003erecommender\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e-- 対象リソースの名前\u003c/span\u003e\n  \u003cspan class=\"n\"\u003etarget_resources\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e-- 毎月推定される保存データ量（GB単位）を計算\u003c/span\u003e\n  \u003cspan class=\"n\"\u003eLAX_INT64\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003erecs\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eadditional_details\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eoverview\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebytesSavedMonthly\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"n\"\u003ePOW\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1024\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eAS\u003c/span\u003e \u003cspan class=\"n\"\u003eest_gb_saved_monthly\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e-- 毎月推定されるスロット時間の節約量を計算（時間単位）\u003c/span\u003e\n  \u003cspan class=\"n\"\u003eLAX_INT64\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003erecs\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eadditional_details\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eoverview\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eslotMsSavedMonthly\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1000\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"mi\"\u003e3600\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eAS\u003c/span\u003e \u003cspan class=\"n\"\u003eslot_hours_saved_monthly\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e-- インサイトが観察された期間を日数に変換\u003c/span\u003e\n  \u003cspan class=\"n\"\u003einsights\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eadditional_details\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eobservation_period_seconds\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"mi\"\u003e86400\u003c/span\u003e \u003cspan class=\"k\"\u003eAS\u003c/span\u003e \u003cspan class=\"n\"\u003eobservation_period_days\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e-- インサイトが最後に更新された日時\u003c/span\u003e\n  \u003cspan class=\"n\"\u003elast_updated_time\u003c/span\u003e\n\u003cspan class=\"k\"\u003eFROM\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e-- インサイトと推奨事項を結合\u003c/span\u003e\n  \u003cspan class=\"n\"\u003einsights\u003c/span\u003e\n\u003cspan class=\"k\"\u003eJOIN\u003c/span\u003e\n  \u003cspan class=\"n\"\u003erecs\u003c/span\u003e\n\u003cspan class=\"k\"\u003eON\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e-- 推奨事項IDがインサイトに含まれていることを確認\u003c/span\u003e\n  \u003cspan class=\"n\"\u003erecommendation_id\u003c/span\u003e \u003cspan class=\"k\"\u003eIN\u003c/span\u003e \u003cspan class=\"k\"\u003eUNNEST\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eassociated_recommendation_ids\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"k\"\u003eWHERE\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e-- 状態が「ACTIVE」であることを確認\u003c/span\u003e\n  \u003cspan class=\"k\"\u003estate\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s1\"\u003e'ACTIVE'\u003c/span\u003e \u003cspan class=\"k\"\u003eAND\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e-- カテゴリが「COST」であることを確認\u003c/span\u003e\n  \u003cspan class=\"n\"\u003ecategory\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s1\"\u003e'COST'\u003c/span\u003e\n\u003cspan class=\"k\"\u003eLIMIT\u003c/span\u003e\n  \u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"67:1-67:156\"\u003eこのクエリにより、推奨事項に関連するリソースと、観察期間中にどれだけのコスト削減が可能かが確認できます。\u003c/p\u003e\n\u003cp data-sourcepos=\"69:1-69:22\"\u003e\u003ca id=\"\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1 data-sourcepos=\"70:1-70:57\"\u003e\n\u003cspan id=\"information_schemarecommendationsとその活用方法\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#information_schemarecommendations%E3%81%A8%E3%81%9D%E3%81%AE%E6%B4%BB%E7%94%A8%E6%96%B9%E6%B3%95\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eINFORMATION_SCHEMA.RECOMMENDATIONSとその活用方法\u003c/h1\u003e\n\u003cp data-sourcepos=\"72:1-72:367\"\u003e\u003cstrong\u003eINFORMATION_SCHEMA.RECOMMENDATIONS\u003c/strong\u003eビューは、現在のプロジェクト内のすべてのBigQuery推奨事項に関するデータを提供します。このビューを使用することで、データセットのパーティショニングやクラスタリング、マテリアライズドビューの作成に関する推奨事項を確認できます。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"74:1-74:57\"\u003e\n\u003cspan id=\"プロジェクト全体のコスト削減推奨事項\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%83%97%E3%83%AD%E3%82%B8%E3%82%A7%E3%82%AF%E3%83%88%E5%85%A8%E4%BD%93%E3%81%AE%E3%82%B3%E3%82%B9%E3%83%88%E5%89%8A%E6%B8%9B%E6%8E%A8%E5%A5%A8%E4%BA%8B%E9%A0%85\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eプロジェクト全体のコスト削減推奨事項\u003c/h2\u003e\n\u003cp data-sourcepos=\"76:1-76:174\"\u003e以下のクエリ例は、プロジェクト全体でスロット時間を節約する推奨事項を表示し、推奨事項の優先度に基づいてソートします。\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"sql\" data-sourcepos=\"78:1-99:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e  \u003cspan class=\"c1\"\u003e-- 推奨の種類、対象リソース、推定の月間保存容量（GB）、推定の月間保存スロット時間、および最終更新時間を選択\u003c/span\u003e\n\u003cspan class=\"k\"\u003eSELECT\u003c/span\u003e\n  \u003cspan class=\"n\"\u003erecommender\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e-- 推奨の種類（例: パーティションとクラスタリング、マテリアライズドビューなど）\u003c/span\u003e\n  \u003cspan class=\"n\"\u003etarget_resources\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e-- 推奨の対象となるリソース（例: テーブル名、プロジェクト名など）\u003c/span\u003e\n  \u003cspan class=\"n\"\u003eLAX_INT64\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eadditional_details\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eoverview\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebytesSavedMonthly\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"n\"\u003ePOW\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1024\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eAS\u003c/span\u003e \u003cspan class=\"n\"\u003eest_gb_saved_monthly\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e-- 推定の月間保存容量（GB）\u003c/span\u003e\n  \u003cspan class=\"n\"\u003eLAX_INT64\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eadditional_details\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eoverview\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eslotMsSavedMonthly\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1000\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"mi\"\u003e3600\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eAS\u003c/span\u003e \u003cspan class=\"n\"\u003eslot_hours_saved_monthly\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e-- 推定の月間保存スロット時間（時間単位）\u003c/span\u003e\n  \u003cspan class=\"n\"\u003elast_updated_time\u003c/span\u003e  \u003cspan class=\"c1\"\u003e-- 推奨が最後に更新された日時\u003c/span\u003e\n\u003cspan class=\"k\"\u003eFROM\u003c/span\u003e\n  \u003cspan class=\"nv\"\u003e`region-us`\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eINFORMATION_SCHEMA\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eRECOMMENDATIONS_BY_PROJECT\u003c/span\u003e  \u003cspan class=\"c1\"\u003e-- プロジェクト単位での推奨を格納しているビューを指定\u003c/span\u003e\n\u003cspan class=\"k\"\u003eWHERE\u003c/span\u003e\n  \u003cspan class=\"n\"\u003eprimary_impact\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecategory\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s1\"\u003e'COST'\u003c/span\u003e  \u003cspan class=\"c1\"\u003e-- コスト削減に関連する推奨のみをフィルタリング\u003c/span\u003e\n  \u003cspan class=\"k\"\u003eAND\u003c/span\u003e \u003cspan class=\"k\"\u003estate\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s1\"\u003e'ACTIVE'\u003c/span\u003e  \u003cspan class=\"c1\"\u003e-- 現在アクティブな状態の推奨のみをフィルタリング\u003c/span\u003e\n\u003cspan class=\"k\"\u003eORDER\u003c/span\u003e \u003cspan class=\"k\"\u003eBY\u003c/span\u003e\n  \u003cspan class=\"n\"\u003eslot_hours_saved_monthly\u003c/span\u003e \u003cspan class=\"k\"\u003eDESC\u003c/span\u003e  \u003cspan class=\"c1\"\u003e-- 月間保存スロット時間が多い順に並び替え\u003c/span\u003e\n\u003cspan class=\"k\"\u003eLIMIT\u003c/span\u003e\n  \u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e  \u003cspan class=\"c1\"\u003e-- 結果を上位3件に制限\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"101:1-101:174\"\u003eこのクエリにより、最も効果的なコスト削減が見込まれる推奨事項を簡単に確認でき、リソースの最適化が行いやすくなります。\u003c/p\u003e\n\u003cp data-sourcepos=\"103:1-103:22\"\u003e\u003ca id=\"\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1 data-sourcepos=\"104:1-104:73\"\u003e\n\u003cspan id=\"information_schemarecommendations_by_organizationとその活用方法\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#information_schemarecommendations_by_organization%E3%81%A8%E3%81%9D%E3%81%AE%E6%B4%BB%E7%94%A8%E6%96%B9%E6%B3%95\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eINFORMATION_SCHEMA.RECOMMENDATIONS_BY_ORGANIZATIONとその活用方法\u003c/h1\u003e\n\u003cp data-sourcepos=\"105:1-105:298\"\u003eINFORMATION_SCHEMA.RECOMMENDATIONS_BY_ORGANIZATIONビューは、組織内のすべてのプロジェクトに対するBigQuery推奨事項に関するデータを提供します。このビューを活用することで、組織全体のデータ最適化を検討する際に活用できます。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"107:1-107:45\"\u003e\n\u003cspan id=\"組織全体のコスト削減推奨事項\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E7%B5%84%E7%B9%94%E5%85%A8%E4%BD%93%E3%81%AE%E3%82%B3%E3%82%B9%E3%83%88%E5%89%8A%E6%B8%9B%E6%8E%A8%E5%A5%A8%E4%BA%8B%E9%A0%85\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e組織全体のコスト削減推奨事項\u003c/h2\u003e\n\u003cp data-sourcepos=\"109:1-109:93\"\u003e以下のクエリ例では、組織全体のコスト削減推奨事項を表示します。\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"sql\" data-sourcepos=\"111:1-130:3\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e\u003cspan class=\"k\"\u003eSELECT\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e-- プロジェクトIDを選択します\u003c/span\u003e\n  \u003cspan class=\"n\"\u003eproject_id\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e-- 月ごとのバイト削減量をギガバイト単位に変換して、推定値を表示します\u003c/span\u003e\n  \u003cspan class=\"n\"\u003eLAX_INT64\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eadditional_details\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eoverview\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebytesSavedMonthly\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"n\"\u003ePOW\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1024\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eAS\u003c/span\u003e \u003cspan class=\"n\"\u003eest_gb_saved_monthly\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e-- 月ごとのスロット時間削減量を時間単位に変換して、推定値を表示します\u003c/span\u003e\n  \u003cspan class=\"n\"\u003eLAX_INT64\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eadditional_details\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eoverview\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eslotMsSavedMonthly\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1000\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"mi\"\u003e3600\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eAS\u003c/span\u003e \u003cspan class=\"n\"\u003eslot_hours_saved_monthly\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e-- 推奨事項が最後に更新された時間を表示します\u003c/span\u003e\n  \u003cspan class=\"n\"\u003elast_updated_time\u003c/span\u003e\n\u003cspan class=\"k\"\u003eFROM\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e-- データを取得するテーブルを指定します。region-usリージョンのRECOMMENDATIONS_BY_ORGANIZATIONビューからデータを取得します\u003c/span\u003e\n  \u003cspan class=\"nv\"\u003e`region-us`\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eINFORMATION_SCHEMA\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eRECOMMENDATIONS_BY_ORGANIZATION\u003c/span\u003e\n\u003cspan class=\"k\"\u003eWHERE\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e-- 'google.bigquery.materializedview.Recommender'リコメンダーによる推奨事項のみをフィルタリングします\u003c/span\u003e\n  \u003cspan class=\"n\"\u003erecommender\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s1\"\u003e'google.bigquery.materializedview.Recommender'\u003c/span\u003e\n  \u003cspan class=\"c1\"\u003e-- 結果の上位3件のみを取得します\u003c/span\u003e\n\u003cspan class=\"k\"\u003eLIMIT\u003c/span\u003e\n  \u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"132:1-132:138\"\u003eこのクエリにより、組織全体でどのプロジェクトが最も効果的にデータを削減できるかが分かります。\u003c/p\u003e\n\u003ch2 data-sourcepos=\"134:1-134:12\"\u003e\n\u003cspan id=\"まとめ\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%81%BE%E3%81%A8%E3%82%81\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eまとめ\u003c/h2\u003e\n\u003cp data-sourcepos=\"135:1-135:349\"\u003e今回は、最新のリリースに関する3つのビュー「INFORMATION_SCHEMA.INSIGHTS」「INFORMATION_SCHEMA.RECOMMENDATIONS」「INFORMATION_SCHEMA.RECOMMENDATIONS_BY_ORGANIZATION」について解説しました。これらのビューを活用することで、データの最適化やコスト削減に役立てていきたいですね。\u003c/p\u003e\n\u003cp data-sourcepos=\"137:1-137:23\"\u003e\u003ca id=\"\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1 data-sourcepos=\"138:1-138:14\"\u003e\n\u003cspan id=\"参考文献\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e参考文献\u003c/h1\u003e\n\u003cul data-sourcepos=\"139:1-141:159\"\u003e\n\u003cli data-sourcepos=\"139:1-139:122\"\u003e\u003ca href=\"https://cloud.google.com/bigquery/docs/information-schema-insights\" rel=\"nofollow noopener\" target=\"_blank\"\u003eBigQuery INFORMATION_SCHEMA Insights Documentation\u003c/a\u003e\u003c/li\u003e\n\u003cli data-sourcepos=\"140:1-140:136\"\u003e\u003ca href=\"https://cloud.google.com/bigquery/docs/information-schema-recommendations\" rel=\"nofollow noopener\" target=\"_blank\"\u003eBigQuery INFORMATION_SCHEMA Recommendations Documentation\u003c/a\u003e\u003c/li\u003e\n\u003cli data-sourcepos=\"141:1-141:159\"\u003e\u003ca href=\"https://cloud.google.com/bigquery/docs/information-schema-recommendations-by-org\" rel=\"nofollow noopener\" target=\"_blank\"\u003eBigQuery INFORMATION_SCHEMA Recommendations by Organization Documentation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n",
    "body": "こんにちは。　code poet です。\n\n# はじめに\nデータの効率的な管理とコストの最適化を意識する場面ってありますよね。BigQuery 追加されたプレビュー機能 INFORMATION_SCHEMA のビューは、プロジェクトのデータ処理を最適化するための方法を提供します。本記事では、この機能を紹介し、活用方法について解説します。\n\n# 目次\n1. [INFORMATION_SCHEMA.INSIGHTSとは](#INFORMATION_SCHEMA.INSIGHTSとは)\n2. [INFORMATION_SCHEMA.RECOMMENDATIONSとその活用方法](#INFORMATION_SCHEMA.RECOMMENDATIONSとその活用方法)\n3. [INFORMATION_SCHEMA.RECOMMENDATIONS_BY_ORGANIZATIONとその活用方法](#INFORMATION_SCHEMA.RECOMMENDATIONS_BY_ORGANIZATIONとその活用方法)\n4. [参考文献](#参考文献)\n\n\u003ca id=\"#Chapter1\"\u003e\u003c/a\u003e\n# INFORMATION_SCHEMA.INSIGHTSとは\n\n**INFORMATION_SCHEMA.INSIGHTS**は、現在のプロジェクト内で利用可能なBigQueryの推奨事項に関する洞察を提供するビューです。このビューは、Recommendation Hubから取得されたすべてのBigQueryインサイトを一元的に表示し、データセットのパーティショニングやクラスタリング、マテリアライズドビューの推奨事項、ロール推奨などの情報を含みます。\n\n## コスト削減のためのアクティブなインサイトの確認\n\n以下のクエリ例は、コストカテゴリでアクティブなインサイトに関連する推奨事項を3つ表示します。推奨事項に基づく月次の節約見込み（GB単位のストレージ削減量、スロット時間の削減量）も一緒に表示されます。\n\n```sql\nWITH\n  -- BigQueryの推奨事項に関するインサイトを取得するクエリ\n  insights AS (\n  SELECT\n    *\n  FROM\n    `region-us`.INFORMATION_SCHEMA.INSIGHTS),\n  -- BigQueryの推奨事項そのものに関するデータを取得するクエリ\n  recs AS (\n  SELECT\n    recommender,\n    recommendation_id,\n    additional_details\n  FROM\n    `region-us`.INFORMATION_SCHEMA.RECOMMENDATIONS)\nSELECT\n  -- 推奨を行ったRecommenderのタイプ\n  recommender,\n  -- 対象リソースの名前\n  target_resources,\n  -- 毎月推定される保存データ量（GB単位）を計算\n  LAX_INT64(recs.additional_details.overview.bytesSavedMonthly) / POW(1024, 3) AS est_gb_saved_monthly,\n  -- 毎月推定されるスロット時間の節約量を計算（時間単位）\n  LAX_INT64(recs.additional_details.overview.slotMsSavedMonthly) / (1000 * 3600) AS slot_hours_saved_monthly,\n  -- インサイトが観察された期間を日数に変換\n  insights.additional_details.observation_period_seconds / 86400 AS observation_period_days,\n  -- インサイトが最後に更新された日時\n  last_updated_time\nFROM\n  -- インサイトと推奨事項を結合\n  insights\nJOIN\n  recs\nON\n  -- 推奨事項IDがインサイトに含まれていることを確認\n  recommendation_id IN UNNEST(associated_recommendation_ids)\nWHERE\n  -- 状態が「ACTIVE」であることを確認\n  state = 'ACTIVE' AND\n  -- カテゴリが「COST」であることを確認\n  category = 'COST'\nLIMIT\n  3;\n  ```\n\nこのクエリにより、推奨事項に関連するリソースと、観察期間中にどれだけのコスト削減が可能かが確認できます。\n\n\u003ca id=\"#Chapter2\"\u003e\u003c/a\u003e\n# INFORMATION_SCHEMA.RECOMMENDATIONSとその活用方法\n\n**INFORMATION_SCHEMA.RECOMMENDATIONS**ビューは、現在のプロジェクト内のすべてのBigQuery推奨事項に関するデータを提供します。このビューを使用することで、データセットのパーティショニングやクラスタリング、マテリアライズドビューの作成に関する推奨事項を確認できます。\n\n## プロジェクト全体のコスト削減推奨事項\n\n以下のクエリ例は、プロジェクト全体でスロット時間を節約する推奨事項を表示し、推奨事項の優先度に基づいてソートします。\n\n```sql\n  -- 推奨の種類、対象リソース、推定の月間保存容量（GB）、推定の月間保存スロット時間、および最終更新時間を選択\nSELECT\n  recommender,\n  -- 推奨の種類（例: パーティションとクラスタリング、マテリアライズドビューなど）\n  target_resources,\n  -- 推奨の対象となるリソース（例: テーブル名、プロジェクト名など）\n  LAX_INT64(additional_details.overview.bytesSavedMonthly) / POW(1024, 3) AS est_gb_saved_monthly,\n  -- 推定の月間保存容量（GB）\n  LAX_INT64(additional_details.overview.slotMsSavedMonthly) / (1000 * 3600) AS slot_hours_saved_monthly,\n  -- 推定の月間保存スロット時間（時間単位）\n  last_updated_time  -- 推奨が最後に更新された日時\nFROM\n  `region-us`.INFORMATION_SCHEMA.RECOMMENDATIONS_BY_PROJECT  -- プロジェクト単位での推奨を格納しているビューを指定\nWHERE\n  primary_impact.category = 'COST'  -- コスト削減に関連する推奨のみをフィルタリング\n  AND state = 'ACTIVE'  -- 現在アクティブな状態の推奨のみをフィルタリング\nORDER BY\n  slot_hours_saved_monthly DESC  -- 月間保存スロット時間が多い順に並び替え\nLIMIT\n  3;  -- 結果を上位3件に制限\n```\n\nこのクエリにより、最も効果的なコスト削減が見込まれる推奨事項を簡単に確認でき、リソースの最適化が行いやすくなります。\n\n\u003ca id=\"#Chapter3\"\u003e\u003c/a\u003e\n# INFORMATION_SCHEMA.RECOMMENDATIONS_BY_ORGANIZATIONとその活用方法\nINFORMATION_SCHEMA.RECOMMENDATIONS_BY_ORGANIZATIONビューは、組織内のすべてのプロジェクトに対するBigQuery推奨事項に関するデータを提供します。このビューを活用することで、組織全体のデータ最適化を検討する際に活用できます。\n\n## 組織全体のコスト削減推奨事項\n\n以下のクエリ例では、組織全体のコスト削減推奨事項を表示します。\n\n```sql\nSELECT\n  -- プロジェクトIDを選択します\n  project_id,\n  -- 月ごとのバイト削減量をギガバイト単位に変換して、推定値を表示します\n  LAX_INT64(additional_details.overview.bytesSavedMonthly) / POW(1024, 3) AS est_gb_saved_monthly,\n  -- 月ごとのスロット時間削減量を時間単位に変換して、推定値を表示します\n  LAX_INT64(additional_details.overview.slotMsSavedMonthly) / (1000 * 3600) AS slot_hours_saved_monthly,\n  -- 推奨事項が最後に更新された時間を表示します\n  last_updated_time\nFROM\n  -- データを取得するテーブルを指定します。region-usリージョンのRECOMMENDATIONS_BY_ORGANIZATIONビューからデータを取得します\n  `region-us`.INFORMATION_SCHEMA.RECOMMENDATIONS_BY_ORGANIZATION\nWHERE\n  -- 'google.bigquery.materializedview.Recommender'リコメンダーによる推奨事項のみをフィルタリングします\n  recommender = 'google.bigquery.materializedview.Recommender'\n  -- 結果の上位3件のみを取得します\nLIMIT\n  3;\n```\n\nこのクエリにより、組織全体でどのプロジェクトが最も効果的にデータを削減できるかが分かります。\n\n## まとめ\n今回は、最新のリリースに関する3つのビュー「INFORMATION_SCHEMA.INSIGHTS」「INFORMATION_SCHEMA.RECOMMENDATIONS」「INFORMATION_SCHEMA.RECOMMENDATIONS_BY_ORGANIZATION」について解説しました。これらのビューを活用することで、データの最適化やコスト削減に役立てていきたいですね。\n\n\u003ca id=\"#reference\"\u003e\u003c/a\u003e\n# 参考文献\n- [BigQuery INFORMATION_SCHEMA Insights Documentation](https://cloud.google.com/bigquery/docs/information-schema-insights)\n- [BigQuery INFORMATION_SCHEMA Recommendations Documentation](https://cloud.google.com/bigquery/docs/information-schema-recommendations)\n- [BigQuery INFORMATION_SCHEMA Recommendations by Organization Documentation](https://cloud.google.com/bigquery/docs/information-schema-recommendations-by-org)\n",
    "coediting": false,
    "comments_count": 0,
    "created_at": "2024-08-22T12:07:21+09:00",
    "group": null,
    "id": "1ab8161a5d9fe9cdf0d8",
    "likes_count": 0,
    "private": false,
    "reactions_count": 0,
    "stocks_count": 0,
    "tags": [
      {
        "name": "SQL",
        "versions": []
      },
      {
        "name": "BigQuery",
        "versions": []
      },
      {
        "name": "GoogleCloud",
        "versions": []
      }
    ],
    "title": "機能紹介：BigQueryの「INFORMATION_SCHEMA」ビューで、コスト削減と最適化を実現しよう！",
    "updated_at": "2024-08-22T12:07:21+09:00",
    "url": "https://qiita.com/codepoet/items/1ab8161a5d9fe9cdf0d8",
    "user": {
      "description": "書く練習",
      "facebook_id": "",
      "followees_count": 0,
      "followers_count": 0,
      "github_login_name": null,
      "id": "codepoet",
      "items_count": 6,
      "linkedin_id": "",
      "location": "",
      "name": "- -",
      "organization": "",
      "permanent_id": 665456,
      "profile_image_url": "https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/665456/profile-images/1721884722",
      "team_only": false,
      "twitter_screen_name": null,
      "website_url": ""
    },
    "page_views_count": null,
    "team_membership": null,
    "organization_url_name": null,
    "slide": false
  },
  {
    "rendered_body": "\u003cp data-sourcepos=\"1:1-2:43\"\u003e\u003ccode\u003etmux new -s \u0026lt;name\u0026gt;\u003c/code\u003e\u003cbr\u003e\n\u003cstrong\u003e新しいセッションを作成する\u003c/strong\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"4:1-4:66\"\u003e\u003cstrong\u003e実行したいコマンドを実行する\u003c/strong\u003e (sh run.shなど..)\u003c/p\u003e\n\u003cp data-sourcepos=\"6:1-6:52\"\u003e実行したら \u003ccode\u003eCtrl+B\u003c/code\u003e の後に \u003ccode\u003eD\u003c/code\u003e を押す\u003c/p\u003e\n\u003cp data-sourcepos=\"8:1-9:20\"\u003e\u003cstrong\u003e再度アタッチする場合\u003c/strong\u003e\u003cbr\u003e\n\u003ccode\u003etmux a -t \u0026lt;name\u0026gt;\u003c/code\u003e\u003c/p\u003e\n\u003cp data-sourcepos=\"11:1-12:11\"\u003e\u003cstrong\u003e現在アクティブなセッションリストを見る\u003c/strong\u003e\u003cbr\u003e\n\u003ccode\u003etmux ls\u003c/code\u003e\u003c/p\u003e\n",
    "body": "``tmux new -s \u003cname\u003e``\n**新しいセッションを作成する**\n\n**実行したいコマンドを実行する** (sh run.shなど..)\n\n実行したら ``Ctrl+B`` の後に ``D`` を押す\n\n**再度アタッチする場合**\n``tmux a -t \u003cname\u003e``\n\n**現在アクティブなセッションリストを見る**\n``tmux ls``\n",
    "coediting": false,
    "comments_count": 0,
    "created_at": "2024-08-22T12:04:57+09:00",
    "group": null,
    "id": "b67b4b60df3e124a8c4d",
    "likes_count": 0,
    "private": false,
    "reactions_count": 0,
    "stocks_count": 0,
    "tags": [
      {
        "name": "tmux",
        "versions": []
      }
    ],
    "title": "tmux の使い方 ( 自分用でもある )",
    "updated_at": "2024-08-22T12:04:57+09:00",
    "url": "https://qiita.com/m1nto/items/b67b4b60df3e124a8c4d",
    "user": {
      "description": "𝟣𝟦𝗒 / 𝖶𝖾𝖻 𝖤𝗇𝗀𝗂𝗇𝖾𝖾𝗋",
      "facebook_id": "",
      "followees_count": 1,
      "followers_count": 1,
      "github_login_name": "m1ntooo",
      "id": "m1nto",
      "items_count": 4,
      "linkedin_id": "",
      "location": "JPN",
      "name": "",
      "organization": "",
      "permanent_id": 3529959,
      "profile_image_url": "https://s3-ap-northeast-1.amazonaws.com/qiita-image-store/0/3529959/8b25db055f8a523f57bd9d8478e4b16a3da8b0a9/x_large.png?1707905305",
      "team_only": false,
      "twitter_screen_name": "m1ntooo_",
      "website_url": "https://m1ntooo.com"
    },
    "page_views_count": null,
    "team_membership": null,
    "organization_url_name": null,
    "slide": false
  },
  {
    "rendered_body": "\u003ch2 data-sourcepos=\"1:1-1:83\"\u003e\n\u003cspan id=\"excelでカーソルキーでセル移動ができなくなったときの対処\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#excel%E3%81%A7%E3%82%AB%E3%83%BC%E3%82%BD%E3%83%AB%E3%82%AD%E3%83%BC%E3%81%A7%E3%82%BB%E3%83%AB%E7%A7%BB%E5%8B%95%E3%81%8C%E3%81%A7%E3%81%8D%E3%81%AA%E3%81%8F%E3%81%AA%E3%81%A3%E3%81%9F%E3%81%A8%E3%81%8D%E3%81%AE%E5%AF%BE%E5%87%A6\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eExcelでカーソルキーでセル移動ができなくなったときの対処\u003c/h2\u003e\n\u003cp data-sourcepos=\"3:1-4:37\"\u003eExcel使用していたらカーソルキーでセル移動ができずスクリーンが移動してしまう場合は\u003cbr\u003e\nScrollLock状態になってます。\u003c/p\u003e\n\u003cp data-sourcepos=\"6:1-6:100\"\u003eスクロールロック状態を解除するには、「Scroll Lock」キーを押せばOKです。\u003c/p\u003e\n\u003cp data-sourcepos=\"8:1-8:98\"\u003eキーボード上には、「Scroll Lock」や「Scr Lk」という表記になっています。\u003c/p\u003e\n",
    "body": "## Excelでカーソルキーでセル移動ができなくなったときの対処\n\nExcel使用していたらカーソルキーでセル移動ができずスクリーンが移動してしまう場合は\nScrollLock状態になってます。\n\nスクロールロック状態を解除するには、「Scroll Lock」キーを押せばOKです。\n\nキーボード上には、「Scroll Lock」や「Scr Lk」という表記になっています。\n",
    "coediting": false,
    "comments_count": 0,
    "created_at": "2024-08-22T12:04:27+09:00",
    "group": null,
    "id": "62b6b8d09f49070eda44",
    "likes_count": 0,
    "private": false,
    "reactions_count": 0,
    "stocks_count": 0,
    "tags": [
      {
        "name": "Excel",
        "versions": []
      }
    ],
    "title": "Excelでカーソルキーでセル移動ができなくなったときの対処法",
    "updated_at": "2024-08-22T12:04:27+09:00",
    "url": "https://qiita.com/snowmag/items/62b6b8d09f49070eda44",
    "user": {
      "description": "AWSのインフラエンジニアしてます。メモ程度に記事残してます。誤りもあるかもしれませんがその際は指摘していただけますと幸いです。\r\n基本的に下書きで残しているのを気まぐれで投稿してます。",
      "facebook_id": "",
      "followees_count": 2,
      "followers_count": 0,
      "github_login_name": null,
      "id": "snowmag",
      "items_count": 10,
      "linkedin_id": "",
      "location": "",
      "name": "",
      "organization": "",
      "permanent_id": 1737452,
      "profile_image_url": "https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1737452/profile-images/1705417261",
      "team_only": false,
      "twitter_screen_name": null,
      "website_url": ""
    },
    "page_views_count": null,
    "team_membership": null,
    "organization_url_name": null,
    "slide": false
  },
  {
    "rendered_body": "\u003cp data-sourcepos=\"1:1-1:110\"\u003eDockerでMySQLが立ち上がらなかった、ハマってしまったのでメモで残しておきます。\u003c/p\u003e\n\u003cp data-sourcepos=\"3:1-3:68\"\u003eWordPress用にmysqlを作成したく以下のコマンドを入力\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"6:5-8:0\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003edocker run --name mysql -dit --net=wordpress -e MYSQL_ROOT_PASSWORD=Password -e MYSQL_DATABASE=wordpressdb -e MYSQL_USER=mysql -e MYSQL_PASSWORD=Password mysql --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci --default-authentication-plugin=mysql_native_password\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"9:1-9:54\"\u003eオプションを外して以下コマンドを入力\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"11:5-12:0\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e docker run --name mysql000 -dit --net=wordpress -e MYSQL_ROOT_PASSWORD=Password -e MYSQL_DATABASE=wordpressdb -e MYSQL_USER=mysql -e MYSQL_PASSWORD=Password mysql --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003ch3 data-sourcepos=\"13:1-13:22\"\u003e\n\u003cspan id=\"成功時の出力\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E6%88%90%E5%8A%9F%E6%99%82%E3%81%AE%E5%87%BA%E5%8A%9B\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e成功時の出力\u003c/h3\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"15:5-25:0\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003eUnable to find image 'mysql:latest' locally\nlatest: Pulling from library/mysql\nc72f53f7235b: Pull complete \nc7e4ed755af2: Pull complete \n6c8c802f90bc: Pull complete \neecc55f854cd: Pull complete \ncc8dabc09813: Pull complete \ne376e56dbcf9: Pull complete \nbd552de0e856: Pull complete \n78c8170ad3df: Pull complete \n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"26:1-26:89\"\u003eその後プロセス状況を確認し、mysqlが立ち上がっていることを確認\u003c/p\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"29:5-34:0\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003edocker ps \nCONTAINER ID   IMAGE       COMMAND                   CREATED          STATUS          PORTS                  NAMES\nac77908fb050   mysql       \"docker-entrypoint.s…\"   10 seconds ago   Up 9 seconds    3306/tcp, 33060/tcp    mysql\n13232cbf8399   wordpress   \"docker-entrypoint.s…\"   22 minutes ago   Up 22 minutes   0.0.0.0:8085-\u0026gt;80/tcp   wordpress\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003ch3 data-sourcepos=\"35:1-35:67\"\u003e\n\u003cspan id=\"初めに実行したコマンドで問題が発生した原因\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E5%88%9D%E3%82%81%E3%81%AB%E5%AE%9F%E8%A1%8C%E3%81%97%E3%81%9F%E3%82%B3%E3%83%9E%E3%83%B3%E3%83%89%E3%81%A7%E5%95%8F%E9%A1%8C%E3%81%8C%E7%99%BA%E7%94%9F%E3%81%97%E3%81%9F%E5%8E%9F%E5%9B%A0\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e初めに実行したコマンドで問題が発生した原因\u003c/h3\u003e\n\u003cdiv class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"37:5-37:58\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre\u003e\u003ccode\u003e--default-authentication-plugin=mysql_native_password \n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp data-sourcepos=\"38:1-38:397\"\u003eMySQLオプションオプションを指定した際に、MySQLバージョンやイメージの互換性に問題がある場合、コンテナが正常に起動しないことがあります。このオプションは、MySQL 8.0以降で必要な場合に使用しますが、場合によっては不要です、勉強用で環境構築くらいであれば、不要になると思います。\u003c/p\u003e\n\u003ch3 data-sourcepos=\"40:1-40:14\"\u003e\n\u003cspan id=\"まとめ\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%81%BE%E3%81%A8%E3%82%81\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eまとめ\u003c/h3\u003e\n\u003cp data-sourcepos=\"42:1-42:260\"\u003e今回の問題は、MySQLコンテナを立ち上げる際のコマンドオプションに問題があったことが原因でした。オプションの見直しと修正を行うことで、正常にコンテナを立ち上げることができました。\u003c/p\u003e\n",
    "body": "DockerでMySQLが立ち上がらなかった、ハマってしまったのでメモで残しておきます。\n\nWordPress用にmysqlを作成したく以下のコマンドを入力\n\n\n    docker run --name mysql -dit --net=wordpress -e MYSQL_ROOT_PASSWORD=Password -e MYSQL_DATABASE=wordpressdb -e MYSQL_USER=mysql -e MYSQL_PASSWORD=Password mysql --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci --default-authentication-plugin=mysql_native_password\n\n\nオプションを外して以下コマンドを入力\n\n     docker run --name mysql000 -dit --net=wordpress -e MYSQL_ROOT_PASSWORD=Password -e MYSQL_DATABASE=wordpressdb -e MYSQL_USER=mysql -e MYSQL_PASSWORD=Password mysql --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci\n\n### 成功時の出力\n\n    Unable to find image 'mysql:latest' locally\n    latest: Pulling from library/mysql\n    c72f53f7235b: Pull complete \n    c7e4ed755af2: Pull complete \n    6c8c802f90bc: Pull complete \n    eecc55f854cd: Pull complete \n    cc8dabc09813: Pull complete \n    e376e56dbcf9: Pull complete \n    bd552de0e856: Pull complete \n    78c8170ad3df: Pull complete \n\nその後プロセス状況を確認し、mysqlが立ち上がっていることを確認\n\n\n    docker ps \n    CONTAINER ID   IMAGE       COMMAND                   CREATED          STATUS          PORTS                  NAMES\n    ac77908fb050   mysql       \"docker-entrypoint.s…\"   10 seconds ago   Up 9 seconds    3306/tcp, 33060/tcp    mysql\n    13232cbf8399   wordpress   \"docker-entrypoint.s…\"   22 minutes ago   Up 22 minutes   0.0.0.0:8085-\u003e80/tcp   wordpress\n\n\n### 初めに実行したコマンドで問題が発生した原因\n\n    --default-authentication-plugin=mysql_native_password \nMySQLオプションオプションを指定した際に、MySQLバージョンやイメージの互換性に問題がある場合、コンテナが正常に起動しないことがあります。このオプションは、MySQL 8.0以降で必要な場合に使用しますが、場合によっては不要です、勉強用で環境構築くらいであれば、不要になると思います。\n\n###  まとめ\n\n今回の問題は、MySQLコンテナを立ち上げる際のコマンドオプションに問題があったことが原因でした。オプションの見直しと修正を行うことで、正常にコンテナを立ち上げることができました。\n",
    "coediting": false,
    "comments_count": 0,
    "created_at": "2024-08-22T11:59:43+09:00",
    "group": null,
    "id": "f1d06e6f6abcb3e6d11b",
    "likes_count": 0,
    "private": false,
    "reactions_count": 0,
    "stocks_count": 0,
    "tags": [
      {
        "name": "MySQL",
        "versions": []
      },
      {
        "name": "Docker",
        "versions": []
      }
    ],
    "title": "DockerでMySQLが立ち上がらなかった",
    "updated_at": "2024-08-22T12:03:52+09:00",
    "url": "https://qiita.com/snowmag/items/f1d06e6f6abcb3e6d11b",
    "user": {
      "description": "AWSのインフラエンジニアしてます。メモ程度に記事残してます。誤りもあるかもしれませんがその際は指摘していただけますと幸いです。\r\n基本的に下書きで残しているのを気まぐれで投稿してます。",
      "facebook_id": "",
      "followees_count": 2,
      "followers_count": 0,
      "github_login_name": null,
      "id": "snowmag",
      "items_count": 10,
      "linkedin_id": "",
      "location": "",
      "name": "",
      "organization": "",
      "permanent_id": 1737452,
      "profile_image_url": "https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/1737452/profile-images/1705417261",
      "team_only": false,
      "twitter_screen_name": null,
      "website_url": ""
    },
    "page_views_count": null,
    "team_membership": null,
    "organization_url_name": null,
    "slide": false
  }
]